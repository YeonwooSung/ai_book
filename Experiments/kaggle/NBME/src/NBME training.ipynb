{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1juyGzghsMxC"},"outputs":[],"source":["# !pip install kaggle\n","# from google.colab import files\n","# files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32037,"status":"ok","timestamp":1651385024765,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"2qqloURAtVXk","outputId":"fa2d8bc7-7b78-4081-f0f6-9ae0539e9bb5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bQ42V6jOttQy"},"outputs":[],"source":["# !ls -1ha kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10465,"status":"ok","timestamp":1651385035226,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"5fQBEA6xxpMw","outputId":"e49fe9d8-4621-4f25-874f-9cf21d711110"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 14.6 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 7.4 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 74.5 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 84.0 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 83.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 14.5 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n"]}],"source":["!pip install transformers\n","!pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6107,"status":"ok","timestamp":1651385041316,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"ayN821m8MeWX","outputId":"1b752617-1039-402d-92c4-815d8b327734"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting wandb\n","  Downloading wandb-0.12.15-py2.py3-none-any.whl (1.8 MB)\n","\u001b[?25l\r\u001b[K     |▏                               | 10 kB 37.0 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 40.9 MB/s eta 0:00:01\r\u001b[K     |▌                               | 30 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |▊                               | 40 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |█                               | 61 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 71 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 92 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 102 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 112 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 122 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 133 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 143 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 153 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 163 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 174 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 184 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 194 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 204 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 215 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████                            | 225 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 235 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 245 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 256 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 266 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████                           | 276 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 286 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 296 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 307 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 317 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 327 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 337 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 348 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 358 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 368 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 378 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 389 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 399 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 409 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 419 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 430 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 440 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 450 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 460 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 471 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 481 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 491 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 501 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 512 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 522 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 532 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 542 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 552 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 563 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 573 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 583 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 593 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 604 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 614 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 624 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 634 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 645 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 655 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 665 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 675 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 686 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 696 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 706 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 716 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 727 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 737 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 747 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 757 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 768 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 778 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 788 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 798 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 808 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 819 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 829 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 839 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 849 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 860 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 870 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 880 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 890 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 901 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 911 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 921 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 931 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 942 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 952 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 962 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 972 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 983 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 993 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.0 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.0 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.0 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.0 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.0 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.1 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.1 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.1 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.1 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.1 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.1 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.1 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.1 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.1 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.2 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.2 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.2 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.2 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.2 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.2 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.2 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.2 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.3 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.3 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.3 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.3 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.3 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.3 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.3 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.3 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.3 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.4 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.4 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.4 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.4 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.4 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.4 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.4 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.4 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.4 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.5 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.5 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.5 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.5 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.5 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.5 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.5 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.5 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.5 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.6 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.6 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.6 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.6 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.6 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.6 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.6 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.6 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.6 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.7 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.7 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.7 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.7 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.7 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.7 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.7 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.7 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.7 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.8 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.8 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8 MB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8 MB 14.0 MB/s \n","\u001b[?25hCollecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 57.0 MB/s \n","\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Collecting setproctitle\n","  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.5.10-py2.py3-none-any.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 75.1 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n","Collecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Building wheels for collected packages: pathtools\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=1dbcc5f0ee23967c0ed04e1559c43e8c42edbbb49e6d704db7df156dfd5a7a70\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built pathtools\n","Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n","Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.10 setproctitle-1.2.3 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.15\n"]}],"source":["!pip3 install wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bhO7fjARwhAB"},"outputs":[],"source":["# !pip3 install tokenizers wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dXxUaSKftuvv"},"outputs":[],"source":["# !mkdir -p ~/.kaggle\n","# !cp kaggle.json ~/.kaggle/\n","# # Permission Warning 이 일어나지 않도록 \n","# !chmod 600 ~/.kaggle/kaggle.json\n","# # 본인이 참가한 모든 대회 보기 \n","# !kaggle competitions list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V7QsENfyuT0y"},"outputs":[],"source":["import os\n","os.chdir(\"drive/\")\n","os.chdir('My Drive')\n","os.chdir('Kaggle')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hUeBd2eGt1bW"},"outputs":[],"source":["# !kaggle competitions download -c nbme-score-clinical-patient-notes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SvSSwcRstln8"},"outputs":[],"source":["# !unzip nbme-score-clinical-patient-notes.zip\n","\n","# !ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1794,"status":"ok","timestamp":1651385043105,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"EFqXCIzwoyzU","outputId":"5da26039-0473-4be3-e06e-6d2bb95f1274"},"outputs":[{"name":"stdout","output_type":"stream","text":["env: TOKENIZERS_PARALLELISM=true\n"]}],"source":["debert_v3_tokenizer_path = 'deberta-v2-v3-fast-tokenizer'\n","%env TOKENIZERS_PARALLELISM=true\n","\n","import shutil\n","from pathlib import Path\n","\n","transformers_path = Path('/usr/local/lib/python3.7/dist-packages/transformers')\n","input_dir = Path('./deberta-v2-v3-fast-tokenizer')\n","\n","convert_file = input_dir / \"convert_slow_tokenizer.py\"\n","conversion_path = transformers_path / convert_file.name\n","\n","if conversion_path.exists():\n","    conversion_path.unlink()\n","\n","shutil.copy(convert_file, transformers_path)\n","deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n","\n","for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n","    filepath = deberta_v2_path/filename\n","    \n","    if filepath.exists():\n","        filepath.unlink()\n","    shutil.copy(input_dir/filename, filepath)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fl4G_mNCu1ed"},"outputs":[],"source":["OUTPUT_DIR = './nbme-deberta-v3-outputs/'\n","if not os.path.exists(OUTPUT_DIR):\n","    os.makedirs(OUTPUT_DIR)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1651385043106,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"Tf8VjiPVwSzF","outputId":"376df6fc-44a3-4e2a-8f75-fa38718a3892"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sun May  1 06:04:02 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"markdown","metadata":{"id":"xOMdcujXv9Z_"},"source":["## CFG"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z9LW9quRvtoL"},"outputs":[],"source":["# ====================================================\n","# CFG\n","# ====================================================\n","class CFG:\n","    wandb=True\n","    competition='NBME'\n","    _wandb_kernel='nakama'\n","    debug=False\n","    apex=True\n","    print_freq=100\n","    num_workers=4\n","    model=\"microsoft/deberta-v3-large\"  # [\"microsoft/deberta-base\", \"microsoft/deberta-large\", \"microsoft/deberta-v3-large\"]\n","    scheduler='cosine' # ['linear', 'cosine']\n","    batch_scheduler=True\n","    num_cycles=0.5\n","    num_warmup_steps=0\n","    epochs=5\n","    encoder_lr= 2e-5\n","    decoder_lr= 2e-5\n","    min_lr=1e-6\n","    eps=1e-6\n","    betas=(0.9, 0.999)\n","    batch_size= 6 #12  # 24 in inference\n","    fc_dropout= 0.2\n","    max_len=512\n","    weight_decay= 0.01\n","    gradient_accumulation_steps=1\n","    max_grad_norm=1000\n","    seed=42\n","    n_fold=15\n","    trn_fold=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n","    train=True\n","    \n","if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.trn_fold = [0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"executionInfo":{"elapsed":9979,"status":"ok","timestamp":1651385053081,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"XA6QLrMQv-mc","outputId":"fb942ef7-4547-4790-e8d6-aa0d17a9d321"},"outputs":[{"name":"stdout","output_type":"stream","text":["login to wandb\n"]},{"data":{"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbluehills\u001b[0m (use `wandb login --relogin` to force relogin)\n"]},{"data":{"text/html":["Tracking run with wandb version 0.12.15"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/Kaggle/wandb/run-20220501_060408-2i25zy57</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/bluehills/NBME-Public/runs/2i25zy57\" target=\"_blank\">microsoft/deberta-v3-large</a></strong> to <a href=\"https://wandb.ai/bluehills/NBME-Public\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# ====================================================\n","# wandb\n","# ====================================================\n","if CFG.wandb:\n","    import wandb\n","    try:\n","        # from kaggle_secrets import UserSecretsClient\n","        # user_secrets = UserSecretsClient()\n","        # secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n","        # wandb.login(key=secret_value_0)\n","        print('login to wandb')\n","        wandb.login()\n","        anony = None\n","    except:\n","        anony = \"must\"\n","        print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n","\n","\n","    def class2dict(f):\n","        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n","\n","    run = wandb.init(project='NBME-Public', \n","                     name=CFG.model,\n","                     config=class2dict(CFG),\n","                     group=CFG.model,\n","                     job_type=\"train\",\n","                     anonymous=anony)"]},{"cell_type":"markdown","metadata":{"id":"qR8EYRj4wX8U"},"source":["## Library"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5550,"status":"ok","timestamp":1651385058628,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"a5048-HowFGB","outputId":"9f2ff4f9-b069-4330-9bbb-b819b5dee900"},"outputs":[{"name":"stdout","output_type":"stream","text":["tokenizers.__version__: 0.12.1\n","transformers.__version__: 4.18.0\n","env: TOKENIZERS_PARALLELISM=true\n"]}],"source":["# ====================================================\n","# Library\n","# ====================================================\n","import os\n","import gc\n","import re\n","import ast\n","import sys\n","import copy\n","import json\n","import time\n","import math\n","import string\n","import pickle\n","import random\n","import joblib\n","import itertools\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import scipy as sp\n","import numpy as np\n","import pandas as pd\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","from tqdm.auto import tqdm\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import Parameter\n","import torch.nn.functional as F\n","from torch.optim import Adam, SGD, AdamW\n","from torch.utils.data import DataLoader, Dataset\n","\n","# os.system('pip uninstall -y transformers')\n","# os.system('python -m pip install --no-index --find-links=../input/nbme-pip-wheels transformers')\n","import tokenizers\n","import transformers\n","print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n","print(f\"transformers.__version__: {transformers.__version__}\")\n","from transformers import AutoTokenizer, AutoModel, AutoConfig\n","from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n","%env TOKENIZERS_PARALLELISM=true\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"Sw2QNpKyzBWS"},"source":["## Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZS5qilDjwZHT"},"outputs":[],"source":["# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\n","\n","def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    return micro_f1(bin_preds, bin_truths)\n","def create_labels_for_scoring(df):\n","    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n","    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, 'location']\n","        if lst:\n","            new_lst = ';'.join(lst)\n","            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n","    # create labels\n","    truths = []\n","    for location_list in df['location_for_create_labels'].values:\n","        truth = []\n","        if len(location_list) > 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(';')]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","    return truths\n","\n","\n","def get_char_probs(texts, predictions, tokenizer):\n","    results = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n","        encoded = tokenizer(text, \n","                            add_special_tokens=True,\n","                            return_offsets_mapping=True)\n","        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n","            start = offset_mapping[0]\n","            end = offset_mapping[1]\n","            results[i][start:end] = pred\n","    return results\n","\n","\n","def get_results(char_probs, th=0.5):\n","    results = []\n","    for char_prob in char_probs:\n","        result = np.where(char_prob >= th)[0] + 1\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    return results\n","\n","\n","def get_predictions(results):\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(';')]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    return predictions"]},{"cell_type":"markdown","metadata":{"id":"8hAL46O9zDaV"},"source":["## Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lWEcVEvNy6iD"},"outputs":[],"source":["# ====================================================\n","# Utils\n","# ====================================================\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","    return score\n","\n","\n","def get_logger(filename='inference'):\n","    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n","    logger = getLogger(__name__)\n","    logger.setLevel(INFO)\n","    handler1 = StreamHandler()\n","    handler1.setFormatter(Formatter(\"%(message)s\"))\n","    handler2 = FileHandler(filename=f\"{filename}.log\")\n","    handler2.setFormatter(Formatter(\"%(message)s\"))\n","    logger.addHandler(handler1)\n","    logger.addHandler(handler2)\n","    return logger\n","\n","LOGGER = get_logger()\n","\n","def seed_everything(seed=42):\n","    '''\n","    Sets the seed of the entire notebook so results are the same every time we run.\n","    This is for REPRODUCIBILITY.\n","    '''\n","    random.seed(seed)\n","    # Set a fixed value for the hash seed\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    \n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        # When running on the CuDNN backend, two further options must be set\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","    \n","seed_everything(seed=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JCxSZGjlKkGu"},"outputs":[],"source":["best_th = 0.5\n","best_score = 0."]},{"cell_type":"markdown","metadata":{"id":"RoQICqwdmuat"},"source":["## Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":637},"executionInfo":{"elapsed":1903,"status":"ok","timestamp":1651385060524,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"3J4wnJXszIx4","outputId":"67f04305-352c-4a11-b8b9-9ae3af3a36f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["train.shape: (14300, 6)\n"]},{"data":{"text/html":["\n","  <div id=\"df-d2af1984-0395-4851-b40d-45ed2528eef7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>case_num</th>\n","      <th>pn_num</th>\n","      <th>feature_num</th>\n","      <th>annotation</th>\n","      <th>location</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00016_000</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>0</td>\n","      <td>[dad with recent heart attcak]</td>\n","      <td>[696 724]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>00016_001</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>1</td>\n","      <td>[mom with \"thyroid disease]</td>\n","      <td>[668 693]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00016_002</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>2</td>\n","      <td>[chest pressure]</td>\n","      <td>[203 217]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>00016_003</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>3</td>\n","      <td>[intermittent episodes, episode]</td>\n","      <td>[70 91, 176 183]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>00016_004</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>4</td>\n","      <td>[felt as if he were going to pass out]</td>\n","      <td>[222 258]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d2af1984-0395-4851-b40d-45ed2528eef7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d2af1984-0395-4851-b40d-45ed2528eef7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d2af1984-0395-4851-b40d-45ed2528eef7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["          id  case_num  pn_num  feature_num                              annotation          location\n","0  00016_000         0      16            0          [dad with recent heart attcak]         [696 724]\n","1  00016_001         0      16            1             [mom with \"thyroid disease]         [668 693]\n","2  00016_002         0      16            2                        [chest pressure]         [203 217]\n","3  00016_003         0      16            3        [intermittent episodes, episode]  [70 91, 176 183]\n","4  00016_004         0      16            4  [felt as if he were going to pass out]         [222 258]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["features.shape: (143, 3)\n"]},{"data":{"text/html":["\n","  <div id=\"df-bb89e26e-fec1-4064-a5e3-09b76e817da7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>feature_num</th>\n","      <th>case_num</th>\n","      <th>feature_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>Family-history-of-thyroid-disorder</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>Chest-pressure</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>Intermittent-symptoms</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>Lightheaded</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb89e26e-fec1-4064-a5e3-09b76e817da7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-bb89e26e-fec1-4064-a5e3-09b76e817da7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-bb89e26e-fec1-4064-a5e3-09b76e817da7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   feature_num  case_num                                       feature_text\n","0            0         0  Family-history-of-MI-OR-Family-history-of-myoc...\n","1            1         0                 Family-history-of-thyroid-disorder\n","2            2         0                                     Chest-pressure\n","3            3         0                              Intermittent-symptoms\n","4            4         0                                        Lightheaded"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["patient_notes.shape: (42146, 3)\n"]},{"data":{"text/html":["\n","  <div id=\"df-2475fa99-a28b-4c2b-ab64-be59b7541bfa\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pn_num</th>\n","      <th>case_num</th>\n","      <th>pn_history</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>17-year-old male, has come to the student heal...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>17 yo male with recurrent palpitations for the...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>Dillon Cleveland is a 17 y.o. male patient wit...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>a 17 yo m c/o palpitation started 3 mos ago; \\...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>17yo male with no pmh here for evaluation of p...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2475fa99-a28b-4c2b-ab64-be59b7541bfa')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2475fa99-a28b-4c2b-ab64-be59b7541bfa button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2475fa99-a28b-4c2b-ab64-be59b7541bfa');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   pn_num  case_num                                         pn_history\n","0       0         0  17-year-old male, has come to the student heal...\n","1       1         0  17 yo male with recurrent palpitations for the...\n","2       2         0  Dillon Cleveland is a 17 y.o. male patient wit...\n","3       3         0  a 17 yo m c/o palpitation started 3 mos ago; \\...\n","4       4         0  17yo male with no pmh here for evaluation of p..."]},"metadata":{},"output_type":"display_data"}],"source":["# ====================================================\n","# Data Loading\n","# ====================================================\n","train = pd.read_csv('./train.csv')\n","train['annotation'] = train['annotation'].apply(ast.literal_eval)\n","train['location'] = train['location'].apply(ast.literal_eval)\n","features = pd.read_csv('./features.csv')\n","def preprocess_features(features):\n","    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n","    return features\n","features = preprocess_features(features)\n","patient_notes = pd.read_csv('./patient_notes.csv')\n","\n","\n","print(f\"train.shape: {train.shape}\")\n","display(train.head())\n","print(f\"features.shape: {features.shape}\")\n","display(features.head())\n","print(f\"patient_notes.shape: {patient_notes.shape}\")\n","display(patient_notes.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1651385060524,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"owVK4NapzKtB","outputId":"8773f237-4a75-4894-9c6e-8b3b5d59cf01"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-c89db1a8-f93b-41ed-9df7-168aca8eeece\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>case_num</th>\n","      <th>pn_num</th>\n","      <th>feature_num</th>\n","      <th>annotation</th>\n","      <th>location</th>\n","      <th>feature_text</th>\n","      <th>pn_history</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00016_000</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>0</td>\n","      <td>[dad with recent heart attcak]</td>\n","      <td>[696 724]</td>\n","      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n","      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>00016_001</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>1</td>\n","      <td>[mom with \"thyroid disease]</td>\n","      <td>[668 693]</td>\n","      <td>Family-history-of-thyroid-disorder</td>\n","      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00016_002</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>2</td>\n","      <td>[chest pressure]</td>\n","      <td>[203 217]</td>\n","      <td>Chest-pressure</td>\n","      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>00016_003</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>3</td>\n","      <td>[intermittent episodes, episode]</td>\n","      <td>[70 91, 176 183]</td>\n","      <td>Intermittent-symptoms</td>\n","      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>00016_004</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>4</td>\n","      <td>[felt as if he were going to pass out]</td>\n","      <td>[222 258]</td>\n","      <td>Lightheaded</td>\n","      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c89db1a8-f93b-41ed-9df7-168aca8eeece')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c89db1a8-f93b-41ed-9df7-168aca8eeece button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c89db1a8-f93b-41ed-9df7-168aca8eeece');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["          id  case_num  pn_num  feature_num                              annotation          location                                       feature_text                                         pn_history\n","0  00016_000         0      16            0          [dad with recent heart attcak]         [696 724]  Family-history-of-MI-OR-Family-history-of-myoc...  HPI: 17yo M presents with palpitations. Patien...\n","1  00016_001         0      16            1             [mom with \"thyroid disease]         [668 693]                 Family-history-of-thyroid-disorder  HPI: 17yo M presents with palpitations. Patien...\n","2  00016_002         0      16            2                        [chest pressure]         [203 217]                                     Chest-pressure  HPI: 17yo M presents with palpitations. Patien...\n","3  00016_003         0      16            3        [intermittent episodes, episode]  [70 91, 176 183]                              Intermittent-symptoms  HPI: 17yo M presents with palpitations. Patien...\n","4  00016_004         0      16            4  [felt as if he were going to pass out]         [222 258]                                        Lightheaded  HPI: 17yo M presents with palpitations. Patien..."]},"metadata":{},"output_type":"display_data"}],"source":["train = train.merge(features, on=['feature_num', 'case_num'], how='left')\n","train = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n","display(train.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rk9JwCZgzcFB"},"outputs":[],"source":["# incorrect annotation\n","train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n","train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n","\n","train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n","train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n","\n","train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n","train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n","\n","train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n","train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n","\n","train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n","train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n","\n","train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n","train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n","\n","train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n","train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n","\n","train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n","train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n","\n","train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n","train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n","\n","train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n","train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n","\n","train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n","train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n","\n","train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n","train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n","\n","train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n","train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n","\n","train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n","train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n","\n","train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n","train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n","\n","train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n","train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n","\n","train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n","train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n","\n","train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n","train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n","\n","train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n","train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n","\n","train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n","train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n","\n","train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n","train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n","\n","train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n","train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n","\n","train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n","train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n","\n","train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n","train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n","\n","train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n","train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n","\n","train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n","train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n","\n","train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n","train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n","\n","train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n","train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n","\n","train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n","train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n","\n","train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n","train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n","\n","train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n","train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n","\n","train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n","train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n","\n","train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n","train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n","\n","train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n","train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n","\n","train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n","train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n","\n","train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n","train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n","\n","train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n","train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n","\n","train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n","train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n","\n","train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n","train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n","\n","train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n","train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n","\n","train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n","train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n","\n","train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n","train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1651385060525,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"6OKNeveOzfeG","outputId":"c65097fb-2d07-405a-f258-f8684428ceb0"},"outputs":[{"data":{"text/plain":["1    8185\n","0    4399\n","2    1292\n","3     287\n","4      99\n","5      27\n","6       9\n","7       1\n","8       1\n","Name: annotation_length, dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["train['annotation_length'] = train['annotation'].apply(len)\n","display(train['annotation_length'].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Y-R6wovSAHe"},"outputs":[],"source":["# pl_df = pd.read_csv('./pl_train.csv')\n","# print(pl_df.shape)\n","# frames = [train, pl_df]\n","# train = pd.concat(frames)\n","# print(train.shape)\n","# train.head()"]},{"cell_type":"markdown","metadata":{"id":"QM2WfrOuzkGG"},"source":["## CV split"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"executionInfo":{"elapsed":501,"status":"ok","timestamp":1651385061020,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"qLsEGvn6ziAl","outputId":"358647eb-adf5-440e-c82f-fbfb02c5bf37"},"outputs":[{"data":{"text/plain":["fold\n","0     957\n","1     957\n","2     957\n","3     957\n","4     957\n","5     948\n","6     948\n","7     948\n","8     948\n","9     948\n","10    955\n","11    955\n","12    955\n","13    955\n","14    955\n","dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["# ====================================================\n","# CV split\n","# ====================================================\n","Fold = GroupKFold(n_splits=CFG.n_fold)\n","groups = train['pn_num'].values\n","\n","train['fold'] = train['case_num']\n","train_fold = train['fold']\n","\n","for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n","    # pandas select rows by index\n","    # <https://www.statology.org/pandas-select-rows-by-index/>\n","    #train_fold.iloc[val_index] = int(n)\n","    train.loc[val_index, 'fold'] = int(n)\n","train['fold'] = train_fold\n","train['fold'] = train['fold'].astype(int)\n","display(train.groupby('fold').size())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D7Uwy_TIzm46"},"outputs":[],"source":["if CFG.debug:\n","    display(train.groupby('fold').size())\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    display(train.groupby('fold').size())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4sFoI1uT1Vtr"},"outputs":[],"source":["# train.to_csv('./mytest.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"l3QrVKDBzrNj"},"source":["## Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Is3kfHJVnneZ"},"outputs":[],"source":["from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n","from transformers.models.deberta_v2.tokenization_deberta_v2 import DebertaV2Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uSWhHwRuzpM1"},"outputs":[],"source":["# ====================================================\n","# tokenizer\n","# ====================================================\n","if CFG.model == 'microsoft/deberta-v3-large' or CFG.model == 'microsoft/deberta-v3-xlarge' or CFG.model == 'microsoft/deberta-v2-xlarge':\n","    tokenizer = DebertaV2TokenizerFast.from_pretrained('./pretrained_deberta_v2_v3_tokenizer/')\n","    # tokenizer = DebertaV2Tokenizer.from_pretrained('./pretrained_deberta_v2_v3_tokenizer/')\n","    CFG.tokenizer = tokenizer\n","else:\n","    tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n","    tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n","    CFG.tokenizer = tokenizer"]},{"cell_type":"markdown","metadata":{"id":"TdPB7Q2dzwlB"},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":133,"referenced_widgets":["006aaa62f7394e4aa4c92d1ff3880b6c","ba422c37787c4677a6a0c39f8c0a8e2d","3193635daa9b4193a18f99934a109dc7","f4537c3202f74bf38a6b6aaf55628a7e","84d6f88223c548faac308f07f25eac93","870dc3300e73417eab6f04a7f4d7998b","20c0cd7e5aa84ff586106d298e4a9973","ca09bdced93c444185d08496d55d27d9","bc11d959533e4a559a1fc5c62a8777d2","cb1063c1ca2e47278516010d79d421d3","a5f4b80673bd4e3ab2eb1a8a822a7286","9d3d377dd68546409792f1b62ef0a915","8121a961c716442f9cae45e0032a827a","4be9ceaa5df443d190178ec17f39ce3c","f404c24e5b5749b2b43febeec330645a","2f72841178af424e9ef577d29ccdc4fe","de433da4e9dd4ceab10d738560bb0388","cc014a6e03ee479d8dbd838b6288f03f","9c6246008ea8487bb3025f44926318e5","c129096d2da247ed8648f342a80dbdec","be0937b1a0b1422295a7774b3a8baec7","d1292607f5ef4284bc751aa52bf002b7"]},"executionInfo":{"elapsed":21706,"status":"ok","timestamp":1651385084656,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"VyjM4MdWzuTa","outputId":"428279e7-65bf-436e-dd0d-5912968ef9a1"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"006aaa62f7394e4aa4c92d1ff3880b6c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/42146 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["pn_history max(lengths): 323\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d3d377dd68546409792f1b62ef0a915","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/143 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["feature_text max(lengths): 28\n","max_len: 354\n"]}],"source":["# ====================================================\n","# Define max_len\n","# ====================================================\n","\n","def define_max_len_(text_col, df):\n","    max_lenghts = []\n","    tk0 = tqdm(df[text_col].fillna(\"\").values, total=len(df))\n","    for text in tk0:\n","        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n","        max_lenghts.append(length)\n","    LOGGER.info(f'{text_col} max(lengths): {max(max_lenghts)}')\n","    \n","    return max_lenghts\n","\n","\n","\n","pn_history_lengths = define_max_len_('pn_history', patient_notes)\n","features_lengths = define_max_len_('feature_text', features)\n","CFG.max_len = max(pn_history_lengths) + max(features_lengths) + 3 # cls & sep & sep\n","# CFG.max_len = 512\n","LOGGER.info(f\"max_len: {CFG.max_len}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1ol14Phzxd6"},"outputs":[],"source":["# ====================================================\n","# Dataset\n","# ====================================================\n","def prepare_input(cfg, text, feature_text):\n","    inputs = cfg.tokenizer(text, feature_text, \n","                           add_special_tokens=True,\n","                           max_length=CFG.max_len,\n","                           padding=\"max_length\",\n","                           return_offsets_mapping=False)\n","    for k, v in inputs.items():\n","        inputs[k] = torch.tensor(v, dtype=torch.long)\n","    return inputs\n","\n","\n","def create_label(cfg, text, annotation_length, location_list):\n","    encoded = cfg.tokenizer(text,\n","                            add_special_tokens=True,\n","                            max_length=CFG.max_len,\n","                            padding=\"max_length\",\n","                            return_offsets_mapping=True)\n","    offset_mapping = encoded['offset_mapping']\n","    ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n","    label = np.zeros(len(offset_mapping))\n","    label[ignore_idxes] = -1\n","    if annotation_length != 0:\n","        for location in location_list:\n","            for loc in [s.split() for s in location.split(';')]:\n","                start_idx = -1\n","                end_idx = -1\n","                start, end = int(loc[0]), int(loc[1])\n","                for idx in range(len(offset_mapping)):\n","                    if (start_idx == -1) & (start < offset_mapping[idx][0]):\n","                        start_idx = idx - 1\n","                    if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n","                        end_idx = idx + 1\n","                if start_idx == -1:\n","                    start_idx = end_idx\n","                if (start_idx != -1) & (end_idx != -1):\n","                    label[start_idx:end_idx] = 1\n","    return torch.tensor(label, dtype=torch.float)\n","\n","\n","class TrainDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.feature_texts = df['feature_text'].values\n","        self.pn_historys = df['pn_history'].values\n","        self.annotation_lengths = df['annotation_length'].values\n","        self.locations = df['location'].values\n","\n","    def __len__(self):\n","        return len(self.feature_texts)\n","\n","    def __getitem__(self, item):\n","        inputs = prepare_input(self.cfg, \n","                               self.pn_historys[item], \n","                               self.feature_texts[item])\n","        label = create_label(self.cfg, \n","                             self.pn_historys[item], \n","                             self.annotation_lengths[item], \n","                             self.locations[item])\n","        return inputs, label"]},{"cell_type":"markdown","metadata":{"id":"4GtpqO_rz6FD"},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OmbliWj6z1-v"},"outputs":[],"source":["# ====================================================\n","# Model\n","# ====================================================\n","class CustomModel(nn.Module):\n","    def __init__(self, cfg, config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","        if config_path is None:\n","            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n","        else:\n","            self.config = torch.load(config_path)\n","        if pretrained:\n","            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n","        else:\n","            self.model = AutoModel(self.config)\n","        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n","        self.fc = nn.Linear(self.config.hidden_size, 1)\n","        self._init_weights(self.fc)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","        \n","    def feature(self, inputs):\n","        outputs = self.model(**inputs)\n","        last_hidden_states = outputs[0]\n","        return last_hidden_states\n","\n","    def forward(self, inputs):\n","        feature = self.feature(inputs)\n","        output = self.fc(self.fc_dropout(feature))\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"OQ0GNu8R0BHC"},"source":["## Helper function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Au791lcz-Tp"},"outputs":[],"source":["# ====================================================\n","# Helper functions\n","# ====================================================\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n","\n","\n","def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = end = time.time()\n","    global_step = 0\n","    for step, (inputs, labels) in enumerate(train_loader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            y_preds = model(inputs)\n","        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","            global_step += 1\n","            if CFG.batch_scheduler:\n","                scheduler.step()\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n","            print('Epoch: [{0}][{1}/{2}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  'Grad: {grad_norm:.4f}  '\n","                  'LR: {lr:.8f}  '\n","                  .format(epoch+1, step, len(train_loader), \n","                          remain=timeSince(start, float(step+1)/len(train_loader)),\n","                          loss=losses,\n","                          grad_norm=grad_norm,\n","                          lr=scheduler.get_lr()[0]))\n","        if CFG.wandb:\n","            wandb.log({f\"[fold{fold}] loss\": losses.val,\n","                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n","    return losses.avg\n","\n","\n","def valid_fn(valid_loader, model, criterion, device):\n","    losses = AverageMeter()\n","    model.eval()\n","    preds = []\n","    start = end = time.time()\n","    for step, (inputs, labels) in enumerate(valid_loader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.no_grad():\n","            y_preds = model(inputs)\n","        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n","        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(y_preds.sigmoid().to('cpu').numpy())\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n","            print('EVAL: [{0}/{1}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  .format(step, len(valid_loader),\n","                          loss=losses,\n","                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n","    predictions = np.concatenate(preds)\n","    return losses.avg, predictions\n","\n","\n","def inference_fn(test_loader, model, device):\n","    preds = []\n","    model.eval()\n","    model.to(device)\n","    tk0 = tqdm(test_loader, total=len(test_loader))\n","    for inputs in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        with torch.no_grad():\n","            y_preds = model(inputs)\n","        preds.append(y_preds.sigmoid().to('cpu').numpy())\n","    predictions = np.concatenate(preds)\n","    return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rbRozESi0C3r"},"outputs":[],"source":["# ====================================================\n","# train loop\n","# ====================================================\n","def train_loop(folds, fold):\n","    \n","    LOGGER.info(f\"========== fold: {fold} training ==========\")\n","\n","    # ====================================================\n","    # loader\n","    # ====================================================\n","    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n","    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n","    valid_texts = valid_folds['pn_history'].values\n","    valid_labels = create_labels_for_scoring(valid_folds)\n","    \n","    train_dataset = TrainDataset(CFG, train_folds)\n","    valid_dataset = TrainDataset(CFG, valid_folds)\n","\n","    train_loader = DataLoader(train_dataset,\n","                              batch_size=CFG.batch_size,\n","                              shuffle=True,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n","    valid_loader = DataLoader(valid_dataset,\n","                              batch_size=CFG.batch_size,\n","                              shuffle=False,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","\n","    # ====================================================\n","    # model & optimizer\n","    # ====================================================\n","    model = CustomModel(CFG, config_path=None, pretrained=True)\n","    torch.save(model.config, OUTPUT_DIR+'config.pth')\n","    model.to(device)\n","    \n","    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n","        param_optimizer = list(model.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","        optimizer_parameters = [\n","            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","             'lr': encoder_lr, 'weight_decay': weight_decay},\n","            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n","             'lr': encoder_lr, 'weight_decay': 0.0},\n","            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n","             'lr': decoder_lr, 'weight_decay': 0.0}\n","        ]\n","        return optimizer_parameters\n","\n","    optimizer_parameters = get_optimizer_params(model,\n","                                                encoder_lr=CFG.encoder_lr, \n","                                                decoder_lr=CFG.decoder_lr,\n","                                                weight_decay=CFG.weight_decay)\n","    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n","    \n","    # ====================================================\n","    # scheduler\n","    # ====================================================\n","    def get_scheduler(cfg, optimizer, num_train_steps):\n","        if cfg.scheduler=='linear':\n","            scheduler = get_linear_schedule_with_warmup(\n","                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n","            )\n","        elif cfg.scheduler=='cosine':\n","            scheduler = get_cosine_schedule_with_warmup(\n","                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n","            )\n","        return scheduler\n","    \n","    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n","    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n","\n","    # ====================================================\n","    # loop\n","    # ====================================================\n","    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    \n","    best_score = 0.\n","\n","    for epoch in range(CFG.epochs):\n","\n","        start_time = time.time()\n","\n","        # train\n","        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n","\n","        # eval\n","        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n","        predictions = predictions.reshape((len(valid_folds), CFG.max_len))\n","        \n","        # scoring\n","        char_probs = get_char_probs(valid_texts, predictions, CFG.tokenizer)\n","        results = get_results(char_probs, th=0.5)\n","        preds = get_predictions(results)\n","        score = get_score(valid_labels, preds)\n","\n","        elapsed = time.time() - start_time\n","\n","        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n","        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n","        if CFG.wandb:\n","            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n","                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n","                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n","                       f\"[fold{fold}] score\": score})\n","        \n","        if best_score < score:\n","            best_score = score\n","            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n","            torch.save({'model': model.state_dict(),\n","                        'predictions': predictions},\n","                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n","\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n","                             map_location=torch.device('cpu'))['predictions']\n","    valid_folds[[i for i in range(CFG.max_len)]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    \n","    return valid_folds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1F1pwTIV0Fd6"},"outputs":[],"source":["def get_result(oof_df):\n","    labels = create_labels_for_scoring(oof_df)\n","    predictions = oof_df[[i for i in range(CFG.max_len)]].values\n","    char_probs = get_char_probs(oof_df['pn_history'].values, predictions, CFG.tokenizer)\n","    results = get_results(char_probs, th=0.5)\n","    preds = get_predictions(results)\n","    score = get_score(labels, preds)\n","    LOGGER.info(f'Score: {score:<.4f}')"]},{"cell_type":"markdown","metadata":{"id":"838KSKWa0Ktz"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["607b464e4de24b1aa1888bc31ad9d65e","5f3243d2ec9b40879c10942910dc9b05","fbfc71adab044ac6968225f7d1ddb76b","8c2bf321200c4b49ad6c9700682d5db4","d42022d94be04cebb80fe80b96db6383","6d04cb97ac6d49e4b5cb2cd1e0f38329","f8701c82716d4839857648540928538c","867259f20ad8430e9399362566c029e0","4f3234cedea545c2822d29d91e82da85","c66956ab94ec4a98bda53fdb4c47cb41","ec3837bbf7c24500ae42321fa01650b1","1ede3793442c4ee5bb835a4343a034c0","71c881072b844b11aa657a879d7e3333","7787e9b4402a4d7ca2d8618aba9a154b","f015d1f3131347959de9c487c3a87b71","a1c50dc370084739b8d372a2ddcf5aab","5e95736ad6f34a3d95cdac9a8def775c","2466cef331c7466a8fd807296ba671c4","638984858f8e4af3b6b0c3eedc16f706","a20d17e491014c2587b902ee2f4adc55","afc78b53ad7d474b89f99df14970eda0","9c4901a451f346c8b61e5ead5e95ded9"]},"id":"no8nLUEy0J-k","outputId":"b4b86e3b-2859-4b4c-ac9b-48d008cc9cd0"},"outputs":[{"name":"stderr","output_type":"stream","text":["========== fold: 0 training ==========\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"607b464e4de24b1aa1888bc31ad9d65e","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/580 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ede3793442c4ee5bb835a4343a034c0","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/833M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2223] Elapsed 0m 0s (remain 34m 47s) Loss: 0.6161(0.6161) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2223] Elapsed 0m 35s (remain 12m 26s) Loss: 0.0077(0.0780) Grad: 1328.9683  LR: 0.00002000  \n","Epoch: [1][200/2223] Elapsed 1m 9s (remain 11m 43s) Loss: 0.0099(0.0535) Grad: 610.8670  LR: 0.00001998  \n","Epoch: [1][300/2223] Elapsed 1m 44s (remain 11m 6s) Loss: 0.0366(0.0429) Grad: 9664.3916  LR: 0.00001996  \n","Epoch: [1][400/2223] Elapsed 2m 18s (remain 10m 28s) Loss: 0.0343(0.0371) Grad: 5148.0039  LR: 0.00001994  \n","Epoch: [1][500/2223] Elapsed 2m 52s (remain 9m 53s) Loss: 0.0609(0.0339) Grad: 9247.7607  LR: 0.00001990  \n","Epoch: [1][600/2223] Elapsed 3m 26s (remain 9m 17s) Loss: 0.0050(0.0318) Grad: 657.6786  LR: 0.00001986  \n","Epoch: [1][700/2223] Elapsed 4m 0s (remain 8m 42s) Loss: 0.0210(0.0294) Grad: 1907.0551  LR: 0.00001980  \n","Epoch: [1][800/2223] Elapsed 4m 35s (remain 8m 8s) Loss: 0.0231(0.0277) Grad: 2584.5022  LR: 0.00001974  \n","Epoch: [1][900/2223] Elapsed 5m 9s (remain 7m 33s) Loss: 0.0069(0.0267) Grad: 626.6624  LR: 0.00001968  \n","Epoch: [1][1000/2223] Elapsed 5m 43s (remain 6m 59s) Loss: 0.0201(0.0257) Grad: 1101.7540  LR: 0.00001960  \n","Epoch: [1][1100/2223] Elapsed 6m 18s (remain 6m 25s) Loss: 0.0099(0.0246) Grad: 1718.6102  LR: 0.00001952  \n","Epoch: [1][1200/2223] Elapsed 6m 53s (remain 5m 51s) Loss: 0.0009(0.0237) Grad: 178.3679  LR: 0.00001943  \n","Epoch: [1][1300/2223] Elapsed 7m 27s (remain 5m 17s) Loss: 0.0120(0.0232) Grad: 1552.1727  LR: 0.00001933  \n","Epoch: [1][1400/2223] Elapsed 8m 2s (remain 4m 43s) Loss: 0.0121(0.0227) Grad: 1562.1260  LR: 0.00001923  \n","Epoch: [1][1500/2223] Elapsed 8m 37s (remain 4m 8s) Loss: 0.0022(0.0221) Grad: 309.8042  LR: 0.00001911  \n","Epoch: [1][1600/2223] Elapsed 9m 11s (remain 3m 34s) Loss: 0.0042(0.0217) Grad: 748.1940  LR: 0.00001899  \n","Epoch: [1][1700/2223] Elapsed 9m 46s (remain 2m 59s) Loss: 0.0533(0.0212) Grad: 5325.4849  LR: 0.00001887  \n","Epoch: [1][1800/2223] Elapsed 10m 21s (remain 2m 25s) Loss: 0.0175(0.0208) Grad: 2016.6265  LR: 0.00001873  \n","Epoch: [1][1900/2223] Elapsed 10m 55s (remain 1m 51s) Loss: 0.0025(0.0204) Grad: 328.4301  LR: 0.00001859  \n","Epoch: [1][2000/2223] Elapsed 11m 29s (remain 1m 16s) Loss: 0.0059(0.0200) Grad: 694.4608  LR: 0.00001844  \n","Epoch: [1][2100/2223] Elapsed 12m 4s (remain 0m 42s) Loss: 0.0053(0.0198) Grad: 893.0405  LR: 0.00001829  \n","Epoch: [1][2200/2223] Elapsed 12m 39s (remain 0m 7s) Loss: 0.0155(0.0194) Grad: 4404.3701  LR: 0.00001813  \n","Epoch: [1][2222/2223] Elapsed 12m 46s (remain 0m 0s) Loss: 0.0052(0.0194) Grad: 928.5145  LR: 0.00001809  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 30s) Loss: 0.0051(0.0051) \n","EVAL: [100/160] Elapsed 0m 21s (remain 0m 12s) Loss: 0.0053(0.0122) \n","EVAL: [159/160] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0003(0.0114) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.0194  avg_val_loss: 0.0114  time: 803s\n","Epoch 1 - Score: 0.8722\n","Epoch 1 - Save Best Score: 0.8722 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [2][0/2223] Elapsed 0m 0s (remain 29m 26s) Loss: 0.0165(0.0165) Grad: 14182.3818  LR: 0.00001809  \n","Epoch: [2][100/2223] Elapsed 0m 35s (remain 12m 32s) Loss: 0.0099(0.0107) Grad: 13777.0391  LR: 0.00001792  \n","Epoch: [2][200/2223] Elapsed 1m 10s (remain 11m 47s) Loss: 0.0001(0.0101) Grad: 291.7565  LR: 0.00001774  \n","Epoch: [2][300/2223] Elapsed 1m 44s (remain 11m 9s) Loss: 0.0025(0.0092) Grad: 17195.3359  LR: 0.00001756  \n","Epoch: [2][400/2223] Elapsed 2m 19s (remain 10m 33s) Loss: 0.0041(0.0091) Grad: 22377.4062  LR: 0.00001738  \n","Epoch: [2][500/2223] Elapsed 2m 54s (remain 9m 58s) Loss: 0.0012(0.0090) Grad: 10226.4395  LR: 0.00001718  \n","Epoch: [2][600/2223] Elapsed 3m 28s (remain 9m 22s) Loss: 0.0020(0.0091) Grad: 5175.6377  LR: 0.00001698  \n","Epoch: [2][700/2223] Elapsed 4m 3s (remain 8m 48s) Loss: 0.0006(0.0090) Grad: 3519.9941  LR: 0.00001678  \n","Epoch: [2][800/2223] Elapsed 4m 37s (remain 8m 13s) Loss: 0.0001(0.0091) Grad: 586.9261  LR: 0.00001657  \n","Epoch: [2][900/2223] Elapsed 5m 12s (remain 7m 38s) Loss: 0.0007(0.0091) Grad: 3876.3857  LR: 0.00001635  \n","Epoch: [2][1000/2223] Elapsed 5m 47s (remain 7m 3s) Loss: 0.0163(0.0093) Grad: 22576.4629  LR: 0.00001613  \n","Epoch: [2][1100/2223] Elapsed 6m 21s (remain 6m 28s) Loss: 0.0048(0.0093) Grad: 9770.2139  LR: 0.00001590  \n","Epoch: [2][1200/2223] Elapsed 6m 55s (remain 5m 53s) Loss: 0.0044(0.0095) Grad: 8838.3574  LR: 0.00001567  \n","Epoch: [2][1300/2223] Elapsed 7m 30s (remain 5m 18s) Loss: 0.0150(0.0095) Grad: 18098.0332  LR: 0.00001544  \n","Epoch: [2][1400/2223] Elapsed 8m 4s (remain 4m 44s) Loss: 0.0387(0.0094) Grad: 49355.6055  LR: 0.00001520  \n","Epoch: [2][1500/2223] Elapsed 8m 38s (remain 4m 9s) Loss: 0.0330(0.0093) Grad: 68213.0625  LR: 0.00001496  \n","Epoch: [2][1600/2223] Elapsed 9m 12s (remain 3m 34s) Loss: 0.0395(0.0092) Grad: 64659.4844  LR: 0.00001471  \n","Epoch: [2][1700/2223] Elapsed 9m 46s (remain 3m 0s) Loss: 0.0120(0.0092) Grad: 38942.7031  LR: 0.00001446  \n","Epoch: [2][1800/2223] Elapsed 10m 21s (remain 2m 25s) Loss: 0.0049(0.0091) Grad: 2930.8845  LR: 0.00001420  \n","Epoch: [2][1900/2223] Elapsed 10m 55s (remain 1m 51s) Loss: 0.0030(0.0091) Grad: 21883.1895  LR: 0.00001395  \n","Epoch: [2][2000/2223] Elapsed 11m 29s (remain 1m 16s) Loss: 0.0103(0.0091) Grad: 23114.9844  LR: 0.00001368  \n","Epoch: [2][2100/2223] Elapsed 12m 3s (remain 0m 42s) Loss: 0.0029(0.0091) Grad: 23137.9863  LR: 0.00001342  \n","Epoch: [2][2200/2223] Elapsed 12m 37s (remain 0m 7s) Loss: 0.0087(0.0091) Grad: 36016.4453  LR: 0.00001315  \n","Epoch: [2][2222/2223] Elapsed 12m 45s (remain 0m 0s) Loss: 0.0291(0.0091) Grad: 79795.4219  LR: 0.00001309  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 9s) Loss: 0.0059(0.0059) \n","EVAL: [100/160] Elapsed 0m 21s (remain 0m 12s) Loss: 0.0089(0.0126) \n","EVAL: [159/160] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0000(0.0120) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.0091  avg_val_loss: 0.0120  time: 802s\n","Epoch 2 - Score: 0.8858\n","Epoch 2 - Save Best Score: 0.8858 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [3][0/2223] Elapsed 0m 0s (remain 34m 25s) Loss: 0.0109(0.0109) Grad: 56224.9648  LR: 0.00001309  \n","Epoch: [3][100/2223] Elapsed 0m 35s (remain 12m 34s) Loss: 0.0126(0.0075) Grad: 29249.8125  LR: 0.00001282  \n","Epoch: [3][200/2223] Elapsed 1m 10s (remain 11m 49s) Loss: 0.0015(0.0074) Grad: 7728.4941  LR: 0.00001255  \n","Epoch: [3][300/2223] Elapsed 1m 44s (remain 11m 10s) Loss: 0.0012(0.0077) Grad: 4183.1821  LR: 0.00001228  \n","Epoch: [3][400/2223] Elapsed 2m 19s (remain 10m 33s) Loss: 0.0066(0.0080) Grad: 25301.3066  LR: 0.00001200  \n","Epoch: [3][500/2223] Elapsed 2m 53s (remain 9m 57s) Loss: 0.0054(0.0079) Grad: 44928.0898  LR: 0.00001172  \n","Epoch: [3][600/2223] Elapsed 3m 28s (remain 9m 22s) Loss: 0.0077(0.0080) Grad: 39171.6953  LR: 0.00001144  \n","Epoch: [3][700/2223] Elapsed 4m 2s (remain 8m 47s) Loss: 0.0001(0.0079) Grad: 240.4687  LR: 0.00001116  \n","Epoch: [3][800/2223] Elapsed 4m 37s (remain 8m 12s) Loss: 0.0003(0.0080) Grad: 2073.0259  LR: 0.00001088  \n","Epoch: [3][900/2223] Elapsed 5m 11s (remain 7m 37s) Loss: 0.0032(0.0078) Grad: 15475.8486  LR: 0.00001060  \n","Epoch: [3][1000/2223] Elapsed 5m 46s (remain 7m 3s) Loss: 0.0001(0.0078) Grad: 379.3353  LR: 0.00001032  \n","Epoch: [3][1100/2223] Elapsed 6m 20s (remain 6m 28s) Loss: 0.0102(0.0078) Grad: 52865.4766  LR: 0.00001004  \n","Epoch: [3][1200/2223] Elapsed 6m 55s (remain 5m 53s) Loss: 0.0018(0.0078) Grad: 5136.1050  LR: 0.00000975  \n","Epoch: [3][1300/2223] Elapsed 7m 29s (remain 5m 18s) Loss: 0.0083(0.0079) Grad: 19344.4160  LR: 0.00000947  \n","Epoch: [3][1400/2223] Elapsed 8m 4s (remain 4m 44s) Loss: 0.0006(0.0079) Grad: 21334.0996  LR: 0.00000919  \n","Epoch: [3][1500/2223] Elapsed 8m 39s (remain 4m 9s) Loss: 0.0159(0.0079) Grad: 14024.6797  LR: 0.00000891  \n","Epoch: [3][1600/2223] Elapsed 9m 13s (remain 3m 35s) Loss: 0.0001(0.0080) Grad: 376.0727  LR: 0.00000863  \n","Epoch: [3][1700/2223] Elapsed 9m 48s (remain 3m 0s) Loss: 0.0001(0.0079) Grad: 680.3496  LR: 0.00000835  \n","Epoch: [3][1800/2223] Elapsed 10m 22s (remain 2m 25s) Loss: 0.0032(0.0081) Grad: 14172.0469  LR: 0.00000807  \n","Epoch: [3][1900/2223] Elapsed 10m 57s (remain 1m 51s) Loss: 0.0035(0.0081) Grad: 17200.9766  LR: 0.00000779  \n","Epoch: [3][2000/2223] Elapsed 11m 31s (remain 1m 16s) Loss: 0.0000(0.0080) Grad: 221.3780  LR: 0.00000752  \n","Epoch: [3][2100/2223] Elapsed 12m 6s (remain 0m 42s) Loss: 0.0571(0.0080) Grad: 189446.5312  LR: 0.00000725  \n","Epoch: [3][2200/2223] Elapsed 12m 40s (remain 0m 7s) Loss: 0.0240(0.0080) Grad: 60903.7891  LR: 0.00000698  \n","Epoch: [3][2222/2223] Elapsed 12m 48s (remain 0m 0s) Loss: 0.0028(0.0080) Grad: 55892.0664  LR: 0.00000692  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 27s) Loss: 0.0089(0.0089) \n","EVAL: [100/160] Elapsed 0m 21s (remain 0m 12s) Loss: 0.0117(0.0140) \n","EVAL: [159/160] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0000(0.0132) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.0080  avg_val_loss: 0.0132  time: 803s\n","Epoch 3 - Score: 0.8939\n","Epoch 3 - Save Best Score: 0.8939 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [4][0/2223] Elapsed 0m 0s (remain 28m 55s) Loss: 0.0002(0.0002) Grad: 1463.5020  LR: 0.00000691  \n","Epoch: [4][100/2223] Elapsed 0m 35s (remain 12m 16s) Loss: 0.0024(0.0068) Grad: 13507.2959  LR: 0.00000665  \n","Epoch: [4][200/2223] Elapsed 1m 9s (remain 11m 35s) Loss: 0.0000(0.0065) Grad: 215.2360  LR: 0.00000638  \n","Epoch: [4][300/2223] Elapsed 1m 43s (remain 10m 57s) Loss: 0.0269(0.0068) Grad: 87943.7578  LR: 0.00000612  \n","Epoch: [4][400/2223] Elapsed 2m 16s (remain 10m 22s) Loss: 0.0080(0.0065) Grad: 39973.5938  LR: 0.00000586  \n","Epoch: [4][500/2223] Elapsed 2m 50s (remain 9m 47s) Loss: 0.0430(0.0063) Grad: 31682.9023  LR: 0.00000561  \n","Epoch: [4][600/2223] Elapsed 3m 25s (remain 9m 13s) Loss: 0.0004(0.0062) Grad: 3538.5864  LR: 0.00000535  \n","Epoch: [4][700/2223] Elapsed 3m 58s (remain 8m 38s) Loss: 0.0029(0.0064) Grad: 16185.3467  LR: 0.00000510  \n","Epoch: [4][800/2223] Elapsed 4m 33s (remain 8m 4s) Loss: 0.0048(0.0066) Grad: 14403.6816  LR: 0.00000486  \n","Epoch: [4][900/2223] Elapsed 5m 7s (remain 7m 30s) Loss: 0.0292(0.0065) Grad: 57545.2930  LR: 0.00000462  \n","Epoch: [4][1000/2223] Elapsed 5m 41s (remain 6m 56s) Loss: 0.0071(0.0066) Grad: 22731.0762  LR: 0.00000438  \n","Epoch: [4][1100/2223] Elapsed 6m 15s (remain 6m 22s) Loss: 0.0076(0.0065) Grad: 22044.4590  LR: 0.00000415  \n","Epoch: [4][1200/2223] Elapsed 6m 49s (remain 5m 48s) Loss: 0.0006(0.0066) Grad: 8276.9512  LR: 0.00000393  \n","Epoch: [4][1300/2223] Elapsed 7m 23s (remain 5m 14s) Loss: 0.0052(0.0064) Grad: 10657.6172  LR: 0.00000370  \n","Epoch: [4][1400/2223] Elapsed 7m 57s (remain 4m 40s) Loss: 0.0010(0.0065) Grad: 5100.2617  LR: 0.00000349  \n","Epoch: [4][1500/2223] Elapsed 8m 31s (remain 4m 5s) Loss: 0.0077(0.0064) Grad: 35725.3203  LR: 0.00000328  \n","Epoch: [4][1600/2223] Elapsed 9m 5s (remain 3m 31s) Loss: 0.0136(0.0065) Grad: 10223.8008  LR: 0.00000307  \n","Epoch: [4][1700/2223] Elapsed 9m 39s (remain 2m 57s) Loss: 0.0051(0.0065) Grad: 37283.5234  LR: 0.00000287  \n","Epoch: [4][1800/2223] Elapsed 10m 13s (remain 2m 23s) Loss: 0.0245(0.0066) Grad: 12296.8330  LR: 0.00000267  \n","Epoch: [4][1900/2223] Elapsed 10m 47s (remain 1m 49s) Loss: 0.0041(0.0067) Grad: 26903.3867  LR: 0.00000248  \n","Epoch: [4][2000/2223] Elapsed 11m 21s (remain 1m 15s) Loss: 0.0075(0.0068) Grad: 30526.7559  LR: 0.00000230  \n","Epoch: [4][2100/2223] Elapsed 11m 55s (remain 0m 41s) Loss: 0.0000(0.0067) Grad: 417.8877  LR: 0.00000212  \n","Epoch: [4][2200/2223] Elapsed 12m 29s (remain 0m 7s) Loss: 0.0163(0.0067) Grad: 219969.8906  LR: 0.00000195  \n","Epoch: [4][2222/2223] Elapsed 12m 37s (remain 0m 0s) Loss: 0.0100(0.0067) Grad: 36501.6602  LR: 0.00000192  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 9s) Loss: 0.0096(0.0096) \n","EVAL: [100/160] Elapsed 0m 21s (remain 0m 12s) Loss: 0.0121(0.0144) \n","EVAL: [159/160] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0000(0.0137) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.0067  avg_val_loss: 0.0137  time: 792s\n","Epoch 4 - Score: 0.8906\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [5][0/2223] Elapsed 0m 0s (remain 29m 2s) Loss: 0.0027(0.0027) Grad: 9453.9736  LR: 0.00000191  \n","Epoch: [5][100/2223] Elapsed 0m 34s (remain 12m 12s) Loss: 0.0157(0.0052) Grad: 25325.1680  LR: 0.00000175  \n","Epoch: [5][200/2223] Elapsed 1m 8s (remain 11m 33s) Loss: 0.0004(0.0046) Grad: 2738.4834  LR: 0.00000159  \n","Epoch: [5][300/2223] Elapsed 1m 42s (remain 10m 57s) Loss: 0.0043(0.0054) Grad: 13512.6523  LR: 0.00000144  \n","Epoch: [5][400/2223] Elapsed 2m 17s (remain 10m 22s) Loss: 0.0021(0.0052) Grad: 7618.0923  LR: 0.00000130  \n","Epoch: [5][500/2223] Elapsed 2m 51s (remain 9m 48s) Loss: 0.0096(0.0054) Grad: 12447.6494  LR: 0.00000117  \n","Epoch: [5][600/2223] Elapsed 3m 25s (remain 9m 13s) Loss: 0.0086(0.0057) Grad: 18117.2656  LR: 0.00000104  \n","Epoch: [5][700/2223] Elapsed 3m 59s (remain 8m 39s) Loss: 0.0166(0.0059) Grad: 27189.4004  LR: 0.00000092  \n","Epoch: [5][800/2223] Elapsed 4m 33s (remain 8m 5s) Loss: 0.0001(0.0058) Grad: 243.9691  LR: 0.00000080  \n","Epoch: [5][900/2223] Elapsed 5m 7s (remain 7m 31s) Loss: 0.0007(0.0057) Grad: 3571.1184  LR: 0.00000069  \n","Epoch: [5][1000/2223] Elapsed 5m 41s (remain 6m 57s) Loss: 0.0000(0.0058) Grad: 28.2647  LR: 0.00000059  \n","Epoch: [5][1100/2223] Elapsed 6m 15s (remain 6m 22s) Loss: 0.0078(0.0056) Grad: 29084.4453  LR: 0.00000050  \n","Epoch: [5][1200/2223] Elapsed 6m 49s (remain 5m 48s) Loss: 0.0000(0.0055) Grad: 16.9392  LR: 0.00000042  \n","Epoch: [5][1300/2223] Elapsed 7m 23s (remain 5m 14s) Loss: 0.0008(0.0055) Grad: 5662.2969  LR: 0.00000034  \n","Epoch: [5][1400/2223] Elapsed 7m 58s (remain 4m 40s) Loss: 0.0002(0.0055) Grad: 846.4016  LR: 0.00000027  \n","Epoch: [5][1500/2223] Elapsed 8m 32s (remain 4m 6s) Loss: 0.0017(0.0055) Grad: 11835.6953  LR: 0.00000021  \n","Epoch: [5][1600/2223] Elapsed 9m 6s (remain 3m 32s) Loss: 0.0027(0.0056) Grad: 15927.6514  LR: 0.00000016  \n","Epoch: [5][1700/2223] Elapsed 9m 40s (remain 2m 58s) Loss: 0.0058(0.0056) Grad: 13254.1553  LR: 0.00000011  \n","Epoch: [5][1800/2223] Elapsed 10m 15s (remain 2m 24s) Loss: 0.0007(0.0057) Grad: 5654.1196  LR: 0.00000007  \n","Epoch: [5][1900/2223] Elapsed 10m 49s (remain 1m 49s) Loss: 0.0098(0.0058) Grad: 41337.3320  LR: 0.00000004  \n","Epoch: [5][2000/2223] Elapsed 11m 23s (remain 1m 15s) Loss: 0.0047(0.0057) Grad: 39203.7461  LR: 0.00000002  \n","Epoch: [5][2100/2223] Elapsed 11m 57s (remain 0m 41s) Loss: 0.0023(0.0057) Grad: 30659.0859  LR: 0.00000001  \n","Epoch: [5][2200/2223] Elapsed 12m 31s (remain 0m 7s) Loss: 0.0050(0.0057) Grad: 46584.6211  LR: 0.00000000  \n","Epoch: [5][2222/2223] Elapsed 12m 39s (remain 0m 0s) Loss: 0.0077(0.0057) Grad: 24836.3223  LR: 0.00000000  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 8s) Loss: 0.0105(0.0105) \n","EVAL: [100/160] Elapsed 0m 21s (remain 0m 12s) Loss: 0.0130(0.0157) \n","EVAL: [159/160] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0000(0.0148) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.0057  avg_val_loss: 0.0148  time: 794s\n","Epoch 5 - Score: 0.8881\n","========== fold: 0 result ==========\n","Score: 0.8939\n","========== fold: 1 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2223] Elapsed 0m 0s (remain 22m 53s) Loss: 0.6901(0.6901) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2223] Elapsed 0m 34s (remain 11m 55s) Loss: 0.0088(0.0872) Grad: 2689.5547  LR: 0.00002000  \n","Epoch: [1][200/2223] Elapsed 1m 7s (remain 11m 20s) Loss: 0.0127(0.0590) Grad: 4564.1089  LR: 0.00001998  \n","Epoch: [1][300/2223] Elapsed 1m 41s (remain 10m 46s) Loss: 0.0224(0.0470) Grad: 7068.9604  LR: 0.00001996  \n","Epoch: [1][400/2223] Elapsed 2m 14s (remain 10m 12s) Loss: 0.0706(0.0402) Grad: 36290.2539  LR: 0.00001994  \n","Epoch: [1][500/2223] Elapsed 2m 48s (remain 9m 38s) Loss: 0.0088(0.0357) Grad: 3841.5520  LR: 0.00001990  \n","Epoch: [1][600/2223] Elapsed 3m 21s (remain 9m 4s) Loss: 0.0030(0.0334) Grad: 1613.9618  LR: 0.00001986  \n","Epoch: [1][700/2223] Elapsed 3m 55s (remain 8m 30s) Loss: 0.0086(0.0314) Grad: 2214.4392  LR: 0.00001980  \n","Epoch: [1][800/2223] Elapsed 4m 28s (remain 7m 57s) Loss: 0.0127(0.0296) Grad: 3938.8369  LR: 0.00001974  \n","Epoch: [1][900/2223] Elapsed 5m 2s (remain 7m 23s) Loss: 0.0074(0.0284) Grad: 2375.3997  LR: 0.00001968  \n","Epoch: [1][1000/2223] Elapsed 5m 35s (remain 6m 50s) Loss: 0.0074(0.0272) Grad: 2579.3291  LR: 0.00001960  \n","Epoch: [1][1100/2223] Elapsed 6m 9s (remain 6m 16s) Loss: 0.0105(0.0264) Grad: 1615.0729  LR: 0.00001952  \n","Epoch: [1][1200/2223] Elapsed 6m 42s (remain 5m 42s) Loss: 0.0118(0.0255) Grad: 1631.6492  LR: 0.00001943  \n","Epoch: [1][1300/2223] Elapsed 7m 16s (remain 5m 9s) Loss: 0.0042(0.0249) Grad: 2182.3164  LR: 0.00001933  \n","Epoch: [1][1400/2223] Elapsed 7m 50s (remain 4m 35s) Loss: 0.0025(0.0240) Grad: 620.3723  LR: 0.00001923  \n","Epoch: [1][1500/2223] Elapsed 8m 23s (remain 4m 2s) Loss: 0.0067(0.0234) Grad: 916.0529  LR: 0.00001911  \n","Epoch: [1][1600/2223] Elapsed 8m 57s (remain 3m 28s) Loss: 0.0233(0.0228) Grad: 5649.3525  LR: 0.00001899  \n","Epoch: [1][1700/2223] Elapsed 9m 30s (remain 2m 55s) Loss: 0.0100(0.0224) Grad: 4431.5522  LR: 0.00001887  \n","Epoch: [1][1800/2223] Elapsed 10m 4s (remain 2m 21s) Loss: 0.0013(0.0220) Grad: 982.8555  LR: 0.00001873  \n","Epoch: [1][1900/2223] Elapsed 10m 37s (remain 1m 48s) Loss: 0.0302(0.0214) Grad: 13781.1768  LR: 0.00001859  \n","Epoch: [1][2000/2223] Elapsed 11m 11s (remain 1m 14s) Loss: 0.0123(0.0210) Grad: 3907.0413  LR: 0.00001844  \n","Epoch: [1][2100/2223] Elapsed 11m 45s (remain 0m 40s) Loss: 0.0006(0.0207) Grad: 337.2966  LR: 0.00001829  \n","Epoch: [1][2200/2223] Elapsed 12m 18s (remain 0m 7s) Loss: 0.0148(0.0204) Grad: 3794.7036  LR: 0.00001813  \n","Epoch: [1][2222/2223] Elapsed 12m 26s (remain 0m 0s) Loss: 0.0004(0.0203) Grad: 494.2727  LR: 0.00001809  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 22s) Loss: 0.0327(0.0327) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0125(0.0144) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0002(0.0143) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.0203  avg_val_loss: 0.0143  time: 781s\n","Epoch 1 - Score: 0.8621\n","Epoch 1 - Save Best Score: 0.8621 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [2][0/2223] Elapsed 0m 0s (remain 31m 18s) Loss: 0.0032(0.0032) Grad: 12864.1943  LR: 0.00001809  \n","Epoch: [2][100/2223] Elapsed 0m 34s (remain 12m 4s) Loss: 0.0180(0.0114) Grad: 41050.6055  LR: 0.00001792  \n","Epoch: [2][200/2223] Elapsed 1m 7s (remain 11m 23s) Loss: 0.0228(0.0111) Grad: 55201.6406  LR: 0.00001774  \n","Epoch: [2][300/2223] Elapsed 1m 41s (remain 10m 47s) Loss: 0.0110(0.0113) Grad: 20604.1543  LR: 0.00001756  \n","Epoch: [2][400/2223] Elapsed 2m 14s (remain 10m 11s) Loss: 0.0015(0.0107) Grad: 4196.4424  LR: 0.00001738  \n","Epoch: [2][500/2223] Elapsed 2m 47s (remain 9m 36s) Loss: 0.0406(0.0106) Grad: 53006.5781  LR: 0.00001718  \n","Epoch: [2][600/2223] Elapsed 3m 21s (remain 9m 2s) Loss: 0.0143(0.0104) Grad: 17043.1172  LR: 0.00001698  \n","Epoch: [2][700/2223] Elapsed 3m 54s (remain 8m 29s) Loss: 0.0013(0.0100) Grad: 6188.1377  LR: 0.00001678  \n","Epoch: [2][800/2223] Elapsed 4m 28s (remain 7m 56s) Loss: 0.0009(0.0102) Grad: 2749.4336  LR: 0.00001657  \n","Epoch: [2][900/2223] Elapsed 5m 1s (remain 7m 22s) Loss: 0.0006(0.0102) Grad: 1944.6674  LR: 0.00001635  \n","Epoch: [2][1000/2223] Elapsed 5m 34s (remain 6m 48s) Loss: 0.0024(0.0099) Grad: 6689.7861  LR: 0.00001613  \n","Epoch: [2][1100/2223] Elapsed 6m 8s (remain 6m 15s) Loss: 0.0126(0.0100) Grad: 13666.5820  LR: 0.00001590  \n","Epoch: [2][1200/2223] Elapsed 6m 41s (remain 5m 41s) Loss: 0.0000(0.0099) Grad: 55.8769  LR: 0.00001567  \n","Epoch: [2][1300/2223] Elapsed 7m 14s (remain 5m 8s) Loss: 0.0079(0.0097) Grad: 11385.4229  LR: 0.00001544  \n","Epoch: [2][1400/2223] Elapsed 7m 48s (remain 4m 34s) Loss: 0.0014(0.0095) Grad: 18776.7871  LR: 0.00001520  \n","Epoch: [2][1500/2223] Elapsed 8m 21s (remain 4m 1s) Loss: 0.0087(0.0095) Grad: 17826.5645  LR: 0.00001496  \n","Epoch: [2][1600/2223] Elapsed 8m 55s (remain 3m 27s) Loss: 0.0010(0.0095) Grad: 3414.8628  LR: 0.00001471  \n","Epoch: [2][1700/2223] Elapsed 9m 28s (remain 2m 54s) Loss: 0.0007(0.0095) Grad: 2802.8501  LR: 0.00001446  \n","Epoch: [2][1800/2223] Elapsed 10m 1s (remain 2m 21s) Loss: 0.0075(0.0094) Grad: 8644.0605  LR: 0.00001420  \n","Epoch: [2][1900/2223] Elapsed 10m 35s (remain 1m 47s) Loss: 0.0012(0.0093) Grad: 6159.7803  LR: 0.00001395  \n","Epoch: [2][2000/2223] Elapsed 11m 8s (remain 1m 14s) Loss: 0.0196(0.0093) Grad: 88216.4922  LR: 0.00001368  \n","Epoch: [2][2100/2223] Elapsed 11m 41s (remain 0m 40s) Loss: 0.0001(0.0093) Grad: 662.0936  LR: 0.00001342  \n","Epoch: [2][2200/2223] Elapsed 12m 15s (remain 0m 7s) Loss: 0.0266(0.0093) Grad: 47253.5625  LR: 0.00001315  \n","Epoch: [2][2222/2223] Elapsed 12m 22s (remain 0m 0s) Loss: 0.0009(0.0093) Grad: 5901.9014  LR: 0.00001309  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 20s) Loss: 0.0311(0.0311) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0070(0.0137) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0000(0.0142) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.0093  avg_val_loss: 0.0142  time: 776s\n","Epoch 2 - Score: 0.8673\n","Epoch 2 - Save Best Score: 0.8673 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [3][0/2223] Elapsed 0m 0s (remain 29m 49s) Loss: 0.0037(0.0037) Grad: 5132.2598  LR: 0.00001309  \n","Epoch: [3][100/2223] Elapsed 0m 34s (remain 12m 6s) Loss: 0.0001(0.0079) Grad: 622.7388  LR: 0.00001282  \n","Epoch: [3][200/2223] Elapsed 1m 8s (remain 11m 27s) Loss: 0.0094(0.0069) Grad: 21084.9766  LR: 0.00001255  \n","Epoch: [3][300/2223] Elapsed 1m 41s (remain 10m 50s) Loss: 0.0054(0.0070) Grad: 40410.6992  LR: 0.00001228  \n","Epoch: [3][400/2223] Elapsed 2m 15s (remain 10m 16s) Loss: 0.0065(0.0074) Grad: 52948.8281  LR: 0.00001200  \n","Epoch: [3][500/2223] Elapsed 2m 49s (remain 9m 42s) Loss: 0.0001(0.0075) Grad: 746.5673  LR: 0.00001172  \n","Epoch: [3][600/2223] Elapsed 3m 22s (remain 9m 7s) Loss: 0.0034(0.0077) Grad: 6029.0259  LR: 0.00001144  \n","Epoch: [3][700/2223] Elapsed 3m 56s (remain 8m 33s) Loss: 0.0003(0.0077) Grad: 1601.5464  LR: 0.00001116  \n","Epoch: [3][800/2223] Elapsed 4m 29s (remain 7m 59s) Loss: 0.0014(0.0077) Grad: 83817.5781  LR: 0.00001088  \n","Epoch: [3][900/2223] Elapsed 5m 3s (remain 7m 25s) Loss: 0.0012(0.0077) Grad: 6342.4580  LR: 0.00001060  \n","Epoch: [3][1000/2223] Elapsed 5m 37s (remain 6m 51s) Loss: 0.0075(0.0078) Grad: 18584.1309  LR: 0.00001032  \n","Epoch: [3][1100/2223] Elapsed 6m 10s (remain 6m 17s) Loss: 0.0019(0.0079) Grad: 8528.9375  LR: 0.00001004  \n","Epoch: [3][1200/2223] Elapsed 6m 44s (remain 5m 44s) Loss: 0.0001(0.0079) Grad: 3397.4441  LR: 0.00000975  \n","Epoch: [3][1300/2223] Elapsed 7m 18s (remain 5m 10s) Loss: 0.0006(0.0079) Grad: 3742.5417  LR: 0.00000947  \n","Epoch: [3][1400/2223] Elapsed 7m 51s (remain 4m 36s) Loss: 0.0002(0.0078) Grad: 1296.2345  LR: 0.00000919  \n","Epoch: [3][1500/2223] Elapsed 8m 25s (remain 4m 3s) Loss: 0.0007(0.0079) Grad: 2915.7451  LR: 0.00000891  \n","Epoch: [3][1600/2223] Elapsed 8m 59s (remain 3m 29s) Loss: 0.0016(0.0079) Grad: 31040.7129  LR: 0.00000863  \n","Epoch: [3][1700/2223] Elapsed 9m 32s (remain 2m 55s) Loss: 0.0022(0.0079) Grad: 16351.6084  LR: 0.00000835  \n","Epoch: [3][1800/2223] Elapsed 10m 6s (remain 2m 22s) Loss: 0.0002(0.0078) Grad: 506.2309  LR: 0.00000807  \n","Epoch: [3][1900/2223] Elapsed 10m 40s (remain 1m 48s) Loss: 0.0001(0.0077) Grad: 390.0447  LR: 0.00000779  \n","Epoch: [3][2000/2223] Elapsed 11m 14s (remain 1m 14s) Loss: 0.0022(0.0078) Grad: 45856.4727  LR: 0.00000752  \n","Epoch: [3][2100/2223] Elapsed 11m 47s (remain 0m 41s) Loss: 0.0058(0.0077) Grad: 27475.9004  LR: 0.00000725  \n","Epoch: [3][2200/2223] Elapsed 12m 21s (remain 0m 7s) Loss: 0.0012(0.0077) Grad: 8182.6143  LR: 0.00000698  \n","Epoch: [3][2222/2223] Elapsed 12m 28s (remain 0m 0s) Loss: 0.0041(0.0077) Grad: 13661.3271  LR: 0.00000692  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 21s) Loss: 0.0284(0.0284) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0083(0.0133) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0000(0.0139) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.0077  avg_val_loss: 0.0139  time: 782s\n","Epoch 3 - Score: 0.8752\n","Epoch 3 - Save Best Score: 0.8752 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [4][0/2223] Elapsed 0m 0s (remain 32m 19s) Loss: 0.0098(0.0098) Grad: 33275.9219  LR: 0.00000691  \n","Epoch: [4][100/2223] Elapsed 0m 34s (remain 12m 13s) Loss: 0.0073(0.0069) Grad: 21344.7578  LR: 0.00000665  \n","Epoch: [4][200/2223] Elapsed 1m 8s (remain 11m 30s) Loss: 0.0003(0.0063) Grad: 1665.2297  LR: 0.00000638  \n","Epoch: [4][300/2223] Elapsed 1m 42s (remain 10m 53s) Loss: 0.0089(0.0059) Grad: 32263.7168  LR: 0.00000612  \n","Epoch: [4][400/2223] Elapsed 2m 16s (remain 10m 18s) Loss: 0.0011(0.0065) Grad: 5096.9731  LR: 0.00000586  \n","Epoch: [4][500/2223] Elapsed 2m 49s (remain 9m 43s) Loss: 0.0006(0.0063) Grad: 6901.6680  LR: 0.00000561  \n","Epoch: [4][600/2223] Elapsed 3m 23s (remain 9m 9s) Loss: 0.0307(0.0064) Grad: 37516.9492  LR: 0.00000535  \n","Epoch: [4][700/2223] Elapsed 3m 57s (remain 8m 35s) Loss: 0.0047(0.0064) Grad: 7753.4917  LR: 0.00000510  \n","Epoch: [4][800/2223] Elapsed 4m 30s (remain 8m 1s) Loss: 0.0024(0.0063) Grad: 38680.3398  LR: 0.00000486  \n","Epoch: [4][900/2223] Elapsed 5m 4s (remain 7m 27s) Loss: 0.0510(0.0065) Grad: 69565.8438  LR: 0.00000462  \n","Epoch: [4][1000/2223] Elapsed 5m 38s (remain 6m 53s) Loss: 0.0019(0.0063) Grad: 7245.5869  LR: 0.00000438  \n","Epoch: [4][1100/2223] Elapsed 6m 12s (remain 6m 19s) Loss: 0.0015(0.0062) Grad: 10388.0869  LR: 0.00000415  \n","Epoch: [4][1200/2223] Elapsed 6m 45s (remain 5m 45s) Loss: 0.0252(0.0063) Grad: 83542.5703  LR: 0.00000393  \n","Epoch: [4][1300/2223] Elapsed 7m 19s (remain 5m 11s) Loss: 0.0377(0.0063) Grad: 102042.8984  LR: 0.00000370  \n","Epoch: [4][1400/2223] Elapsed 7m 53s (remain 4m 37s) Loss: 0.0098(0.0064) Grad: 142346.2344  LR: 0.00000349  \n","Epoch: [4][1500/2223] Elapsed 8m 27s (remain 4m 4s) Loss: 0.0045(0.0063) Grad: 7990.5781  LR: 0.00000328  \n","Epoch: [4][1600/2223] Elapsed 9m 1s (remain 3m 30s) Loss: 0.0000(0.0063) Grad: 18.1692  LR: 0.00000307  \n","Epoch: [4][1700/2223] Elapsed 9m 34s (remain 2m 56s) Loss: 0.0004(0.0063) Grad: 1778.4038  LR: 0.00000287  \n","Epoch: [4][1800/2223] Elapsed 10m 8s (remain 2m 22s) Loss: 0.0027(0.0062) Grad: 23616.9375  LR: 0.00000267  \n","Epoch: [4][1900/2223] Elapsed 10m 42s (remain 1m 48s) Loss: 0.0001(0.0062) Grad: 475.5413  LR: 0.00000248  \n","Epoch: [4][2000/2223] Elapsed 11m 16s (remain 1m 15s) Loss: 0.0008(0.0063) Grad: 9400.8271  LR: 0.00000230  \n","Epoch: [4][2100/2223] Elapsed 11m 49s (remain 0m 41s) Loss: 0.0000(0.0063) Grad: 33.5204  LR: 0.00000212  \n","Epoch: [4][2200/2223] Elapsed 12m 23s (remain 0m 7s) Loss: 0.0003(0.0062) Grad: 9302.8398  LR: 0.00000195  \n","Epoch: [4][2222/2223] Elapsed 12m 31s (remain 0m 0s) Loss: 0.0000(0.0062) Grad: 266.3825  LR: 0.00000192  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 11s) Loss: 0.0307(0.0307) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0080(0.0146) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0000(0.0155) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.0062  avg_val_loss: 0.0155  time: 785s\n","Epoch 4 - Score: 0.8779\n","Epoch 4 - Save Best Score: 0.8779 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [5][0/2223] Elapsed 0m 0s (remain 30m 8s) Loss: 0.0005(0.0005) Grad: 4874.1992  LR: 0.00000191  \n","Epoch: [5][100/2223] Elapsed 0m 34s (remain 12m 7s) Loss: 0.0067(0.0042) Grad: 10231.1514  LR: 0.00000175  \n","Epoch: [5][200/2223] Elapsed 1m 8s (remain 11m 27s) Loss: 0.0005(0.0049) Grad: 4009.5845  LR: 0.00000159  \n","Epoch: [5][300/2223] Elapsed 1m 41s (remain 10m 50s) Loss: 0.0133(0.0051) Grad: 15777.6416  LR: 0.00000144  \n","Epoch: [5][400/2223] Elapsed 2m 15s (remain 10m 14s) Loss: 0.0101(0.0053) Grad: 11103.7871  LR: 0.00000130  \n","Epoch: [5][500/2223] Elapsed 2m 48s (remain 9m 40s) Loss: 0.0028(0.0049) Grad: 9265.8203  LR: 0.00000117  \n","Epoch: [5][600/2223] Elapsed 3m 22s (remain 9m 5s) Loss: 0.0001(0.0050) Grad: 615.9288  LR: 0.00000104  \n","Epoch: [5][700/2223] Elapsed 3m 55s (remain 8m 31s) Loss: 0.0002(0.0050) Grad: 877.7626  LR: 0.00000092  \n","Epoch: [5][800/2223] Elapsed 4m 29s (remain 7m 57s) Loss: 0.0000(0.0050) Grad: 56.7360  LR: 0.00000080  \n","Epoch: [5][900/2223] Elapsed 5m 2s (remain 7m 24s) Loss: 0.0000(0.0049) Grad: 13.1259  LR: 0.00000069  \n","Epoch: [5][1000/2223] Elapsed 5m 36s (remain 6m 50s) Loss: 0.0090(0.0049) Grad: 34514.9492  LR: 0.00000059  \n","Epoch: [5][1100/2223] Elapsed 6m 9s (remain 6m 16s) Loss: 0.0000(0.0049) Grad: 77.8592  LR: 0.00000050  \n","Epoch: [5][1200/2223] Elapsed 6m 42s (remain 5m 42s) Loss: 0.0024(0.0050) Grad: 9845.5518  LR: 0.00000042  \n","Epoch: [5][1300/2223] Elapsed 7m 16s (remain 5m 9s) Loss: 0.0223(0.0050) Grad: 34848.9922  LR: 0.00000034  \n","Epoch: [5][1400/2223] Elapsed 7m 50s (remain 4m 35s) Loss: 0.0015(0.0050) Grad: 5729.3228  LR: 0.00000027  \n","Epoch: [5][1500/2223] Elapsed 8m 23s (remain 4m 2s) Loss: 0.0007(0.0049) Grad: 4881.4731  LR: 0.00000021  \n","Epoch: [5][1600/2223] Elapsed 8m 57s (remain 3m 28s) Loss: 0.0034(0.0049) Grad: 7046.8403  LR: 0.00000016  \n","Epoch: [5][1700/2223] Elapsed 9m 30s (remain 2m 55s) Loss: 0.0025(0.0048) Grad: 13045.5312  LR: 0.00000011  \n","Epoch: [5][1800/2223] Elapsed 10m 4s (remain 2m 21s) Loss: 0.0093(0.0049) Grad: 26203.6875  LR: 0.00000007  \n","Epoch: [5][1900/2223] Elapsed 10m 37s (remain 1m 48s) Loss: 0.0001(0.0049) Grad: 1609.4196  LR: 0.00000004  \n","Epoch: [5][2000/2223] Elapsed 11m 11s (remain 1m 14s) Loss: 0.0001(0.0049) Grad: 2986.7686  LR: 0.00000002  \n","Epoch: [5][2100/2223] Elapsed 11m 44s (remain 0m 40s) Loss: 0.0000(0.0049) Grad: 67.7269  LR: 0.00000001  \n","Epoch: [5][2200/2223] Elapsed 12m 18s (remain 0m 7s) Loss: 0.0318(0.0050) Grad: 94828.2188  LR: 0.00000000  \n","Epoch: [5][2222/2223] Elapsed 12m 25s (remain 0m 0s) Loss: 0.0001(0.0050) Grad: 2633.9075  LR: 0.00000000  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 21s) Loss: 0.0324(0.0324) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0087(0.0156) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0000(0.0166) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.0050  avg_val_loss: 0.0166  time: 780s\n","Epoch 5 - Score: 0.8771\n","========== fold: 1 result ==========\n","Score: 0.8779\n","========== fold: 2 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2223] Elapsed 0m 0s (remain 23m 31s) Loss: 0.6544(0.6544) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2223] Elapsed 0m 34s (remain 12m 0s) Loss: 0.0167(0.0829) Grad: 2981.0222  LR: 0.00002000  \n","Epoch: [1][200/2223] Elapsed 1m 8s (remain 11m 24s) Loss: 0.0315(0.0567) Grad: 3195.4148  LR: 0.00001998  \n","Epoch: [1][300/2223] Elapsed 1m 41s (remain 10m 50s) Loss: 0.0224(0.0449) Grad: 2262.6101  LR: 0.00001996  \n","Epoch: [1][400/2223] Elapsed 2m 15s (remain 10m 15s) Loss: 0.0048(0.0388) Grad: 782.4653  LR: 0.00001994  \n","Epoch: [1][500/2223] Elapsed 2m 49s (remain 9m 41s) Loss: 0.0053(0.0342) Grad: 695.8144  LR: 0.00001990  \n","Epoch: [1][600/2223] Elapsed 3m 22s (remain 9m 7s) Loss: 0.0013(0.0315) Grad: 538.2804  LR: 0.00001986  \n","Epoch: [1][700/2223] Elapsed 3m 56s (remain 8m 33s) Loss: 0.0059(0.0293) Grad: 6194.9297  LR: 0.00001980  \n","Epoch: [1][800/2223] Elapsed 4m 30s (remain 7m 59s) Loss: 0.0036(0.0281) Grad: 740.8948  LR: 0.00001974  \n","Epoch: [1][900/2223] Elapsed 5m 4s (remain 7m 26s) Loss: 0.0188(0.0270) Grad: 1084.6780  LR: 0.00001968  \n","Epoch: [1][1000/2223] Elapsed 5m 37s (remain 6m 52s) Loss: 0.0051(0.0260) Grad: 426.4576  LR: 0.00001960  \n","Epoch: [1][1100/2223] Elapsed 6m 11s (remain 6m 18s) Loss: 0.0060(0.0252) Grad: 1977.8059  LR: 0.00001952  \n","Epoch: [1][1200/2223] Elapsed 6m 45s (remain 5m 44s) Loss: 0.0015(0.0244) Grad: 226.6893  LR: 0.00001943  \n","Epoch: [1][1300/2223] Elapsed 7m 19s (remain 5m 11s) Loss: 0.0052(0.0235) Grad: 771.6106  LR: 0.00001933  \n","Epoch: [1][1400/2223] Elapsed 7m 52s (remain 4m 37s) Loss: 0.0037(0.0230) Grad: 770.4589  LR: 0.00001923  \n","Epoch: [1][1500/2223] Elapsed 8m 26s (remain 4m 3s) Loss: 0.0027(0.0225) Grad: 340.6964  LR: 0.00001911  \n","Epoch: [1][1600/2223] Elapsed 9m 0s (remain 3m 29s) Loss: 0.0057(0.0219) Grad: 590.2625  LR: 0.00001899  \n","Epoch: [1][1700/2223] Elapsed 9m 33s (remain 2m 56s) Loss: 0.0168(0.0214) Grad: 1079.1144  LR: 0.00001887  \n","Epoch: [1][1800/2223] Elapsed 10m 7s (remain 2m 22s) Loss: 0.0093(0.0209) Grad: 801.0200  LR: 0.00001873  \n","Epoch: [1][1900/2223] Elapsed 10m 41s (remain 1m 48s) Loss: 0.0037(0.0205) Grad: 1866.7753  LR: 0.00001859  \n","Epoch: [1][2000/2223] Elapsed 11m 14s (remain 1m 14s) Loss: 0.0065(0.0202) Grad: 1496.1106  LR: 0.00001844  \n","Epoch: [1][2100/2223] Elapsed 11m 48s (remain 0m 41s) Loss: 0.0038(0.0198) Grad: 922.1919  LR: 0.00001829  \n","Epoch: [1][2200/2223] Elapsed 12m 22s (remain 0m 7s) Loss: 0.0540(0.0195) Grad: 3649.7212  LR: 0.00001813  \n","Epoch: [1][2222/2223] Elapsed 12m 29s (remain 0m 0s) Loss: 0.0214(0.0194) Grad: 3762.7356  LR: 0.00001809  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 18s) Loss: 0.0016(0.0016) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0121(0.0126) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0004(0.0115) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.0194  avg_val_loss: 0.0115  time: 784s\n","Epoch 1 - Score: 0.8706\n","Epoch 1 - Save Best Score: 0.8706 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [2][0/2223] Elapsed 0m 0s (remain 31m 20s) Loss: 0.0100(0.0100) Grad: 9506.4766  LR: 0.00001809  \n","Epoch: [2][100/2223] Elapsed 0m 34s (remain 12m 7s) Loss: 0.0006(0.0098) Grad: 3512.0781  LR: 0.00001792  \n","Epoch: [2][200/2223] Elapsed 1m 8s (remain 11m 24s) Loss: 0.0078(0.0096) Grad: 11192.7578  LR: 0.00001774  \n","Epoch: [2][300/2223] Elapsed 1m 41s (remain 10m 48s) Loss: 0.0515(0.0095) Grad: 56825.6523  LR: 0.00001756  \n","Epoch: [2][400/2223] Elapsed 2m 14s (remain 10m 12s) Loss: 0.0368(0.0095) Grad: 74165.8438  LR: 0.00001738  \n","Epoch: [2][500/2223] Elapsed 2m 48s (remain 9m 38s) Loss: 0.0020(0.0094) Grad: 5608.8237  LR: 0.00001718  \n","Epoch: [2][600/2223] Elapsed 3m 21s (remain 9m 3s) Loss: 0.0013(0.0094) Grad: 9037.6201  LR: 0.00001698  \n","Epoch: [2][700/2223] Elapsed 3m 55s (remain 8m 30s) Loss: 0.0001(0.0091) Grad: 344.4676  LR: 0.00001678  \n","Epoch: [2][800/2223] Elapsed 4m 28s (remain 7m 56s) Loss: 0.0121(0.0092) Grad: 16759.6348  LR: 0.00001657  \n","Epoch: [2][900/2223] Elapsed 5m 1s (remain 7m 22s) Loss: 0.0006(0.0091) Grad: 3828.5261  LR: 0.00001635  \n","Epoch: [2][1000/2223] Elapsed 5m 35s (remain 6m 49s) Loss: 0.0002(0.0090) Grad: 927.4016  LR: 0.00001613  \n","Epoch: [2][1100/2223] Elapsed 6m 8s (remain 6m 15s) Loss: 0.0346(0.0091) Grad: 94721.1016  LR: 0.00001590  \n","Epoch: [2][1200/2223] Elapsed 6m 42s (remain 5m 42s) Loss: 0.0030(0.0091) Grad: 11783.0469  LR: 0.00001567  \n","Epoch: [2][1300/2223] Elapsed 7m 15s (remain 5m 8s) Loss: 0.0006(0.0092) Grad: 2846.3467  LR: 0.00001544  \n","Epoch: [2][1400/2223] Elapsed 7m 49s (remain 4m 35s) Loss: 0.0021(0.0093) Grad: 6941.6953  LR: 0.00001520  \n","Epoch: [2][1500/2223] Elapsed 8m 22s (remain 4m 1s) Loss: 0.0075(0.0093) Grad: 20099.0664  LR: 0.00001496  \n","Epoch: [2][1600/2223] Elapsed 8m 55s (remain 3m 28s) Loss: 0.0001(0.0092) Grad: 370.2155  LR: 0.00001471  \n","Epoch: [2][1700/2223] Elapsed 9m 29s (remain 2m 54s) Loss: 0.0011(0.0092) Grad: 2834.6052  LR: 0.00001446  \n","Epoch: [2][1800/2223] Elapsed 10m 2s (remain 2m 21s) Loss: 0.0044(0.0090) Grad: 7657.6265  LR: 0.00001420  \n","Epoch: [2][1900/2223] Elapsed 10m 36s (remain 1m 47s) Loss: 0.0031(0.0090) Grad: 11177.9766  LR: 0.00001395  \n","Epoch: [2][2000/2223] Elapsed 11m 9s (remain 1m 14s) Loss: 0.0183(0.0090) Grad: 27448.8867  LR: 0.00001368  \n","Epoch: [2][2100/2223] Elapsed 11m 42s (remain 0m 40s) Loss: 0.0084(0.0090) Grad: 104520.6562  LR: 0.00001342  \n","Epoch: [2][2200/2223] Elapsed 12m 16s (remain 0m 7s) Loss: 0.0049(0.0089) Grad: 21243.7520  LR: 0.00001315  \n","Epoch: [2][2222/2223] Elapsed 12m 23s (remain 0m 0s) Loss: 0.0017(0.0088) Grad: 33896.8281  LR: 0.00001309  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 21s) Loss: 0.0016(0.0016) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0064(0.0139) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0000(0.0123) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.0088  avg_val_loss: 0.0123  time: 778s\n","Epoch 2 - Score: 0.8759\n","Epoch 2 - Save Best Score: 0.8759 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [3][0/2223] Elapsed 0m 0s (remain 31m 48s) Loss: 0.0028(0.0028) Grad: 9772.4580  LR: 0.00001309  \n","Epoch: [3][100/2223] Elapsed 0m 34s (remain 12m 6s) Loss: 0.0050(0.0065) Grad: 16725.9902  LR: 0.00001282  \n","Epoch: [3][200/2223] Elapsed 1m 8s (remain 11m 25s) Loss: 0.0006(0.0082) Grad: 2779.3354  LR: 0.00001255  \n","Epoch: [3][300/2223] Elapsed 1m 41s (remain 10m 48s) Loss: 0.0029(0.0088) Grad: 15625.0518  LR: 0.00001228  \n","Epoch: [3][400/2223] Elapsed 2m 14s (remain 10m 13s) Loss: 0.0007(0.0086) Grad: 2695.5349  LR: 0.00001200  \n","Epoch: [3][500/2223] Elapsed 2m 48s (remain 9m 38s) Loss: 0.0001(0.0084) Grad: 444.8204  LR: 0.00001172  \n","Epoch: [3][600/2223] Elapsed 3m 22s (remain 9m 5s) Loss: 0.0160(0.0081) Grad: 48619.0078  LR: 0.00001144  \n","Epoch: [3][700/2223] Elapsed 3m 55s (remain 8m 31s) Loss: 0.0056(0.0081) Grad: 9383.0439  LR: 0.00001116  \n","Epoch: [3][800/2223] Elapsed 4m 28s (remain 7m 57s) Loss: 0.0014(0.0079) Grad: 7991.9873  LR: 0.00001088  \n","Epoch: [3][900/2223] Elapsed 5m 2s (remain 7m 23s) Loss: 0.0057(0.0077) Grad: 13545.1396  LR: 0.00001060  \n","Epoch: [3][1000/2223] Elapsed 5m 35s (remain 6m 49s) Loss: 0.0043(0.0079) Grad: 5515.3506  LR: 0.00001032  \n","Epoch: [3][1100/2223] Elapsed 6m 8s (remain 6m 16s) Loss: 0.0123(0.0077) Grad: 64159.1133  LR: 0.00001004  \n","Epoch: [3][1200/2223] Elapsed 6m 42s (remain 5m 42s) Loss: 0.0009(0.0077) Grad: 3148.7227  LR: 0.00000975  \n","Epoch: [3][1300/2223] Elapsed 7m 15s (remain 5m 8s) Loss: 0.0036(0.0077) Grad: 12808.4482  LR: 0.00000947  \n","Epoch: [3][1400/2223] Elapsed 7m 49s (remain 4m 35s) Loss: 0.0002(0.0077) Grad: 1465.6111  LR: 0.00000919  \n","Epoch: [3][1500/2223] Elapsed 8m 22s (remain 4m 1s) Loss: 0.0096(0.0078) Grad: 24000.3926  LR: 0.00000891  \n","Epoch: [3][1600/2223] Elapsed 8m 56s (remain 3m 28s) Loss: 0.0246(0.0078) Grad: 42659.3281  LR: 0.00000863  \n","Epoch: [3][1700/2223] Elapsed 9m 29s (remain 2m 54s) Loss: 0.0062(0.0077) Grad: 15520.1357  LR: 0.00000835  \n","Epoch: [3][1800/2223] Elapsed 10m 3s (remain 2m 21s) Loss: 0.0096(0.0078) Grad: 12253.0791  LR: 0.00000807  \n","Epoch: [3][1900/2223] Elapsed 10m 36s (remain 1m 47s) Loss: 0.0040(0.0078) Grad: 11507.9941  LR: 0.00000779  \n","Epoch: [3][2000/2223] Elapsed 11m 9s (remain 1m 14s) Loss: 0.0000(0.0078) Grad: 196.6224  LR: 0.00000752  \n","Epoch: [3][2100/2223] Elapsed 11m 43s (remain 0m 40s) Loss: 0.0527(0.0077) Grad: 169905.0625  LR: 0.00000725  \n","Epoch: [3][2200/2223] Elapsed 12m 16s (remain 0m 7s) Loss: 0.0026(0.0077) Grad: 15109.4268  LR: 0.00000698  \n","Epoch: [3][2222/2223] Elapsed 12m 24s (remain 0m 0s) Loss: 0.0043(0.0077) Grad: 27440.9902  LR: 0.00000692  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 12s) Loss: 0.0054(0.0054) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0082(0.0143) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0000(0.0132) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.0077  avg_val_loss: 0.0132  time: 778s\n","Epoch 3 - Score: 0.8821\n","Epoch 3 - Save Best Score: 0.8821 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [4][0/2223] Elapsed 0m 0s (remain 29m 57s) Loss: 0.0053(0.0053) Grad: 8791.1963  LR: 0.00000691  \n","Epoch: [4][100/2223] Elapsed 0m 34s (remain 12m 9s) Loss: 0.0069(0.0044) Grad: 22613.5996  LR: 0.00000665  \n","Epoch: [4][200/2223] Elapsed 1m 8s (remain 11m 28s) Loss: 0.0019(0.0054) Grad: 43883.1836  LR: 0.00000638  \n","Epoch: [4][300/2223] Elapsed 1m 42s (remain 10m 51s) Loss: 0.0001(0.0064) Grad: 492.0399  LR: 0.00000612  \n","Epoch: [4][400/2223] Elapsed 2m 15s (remain 10m 16s) Loss: 0.0038(0.0073) Grad: 14300.0186  LR: 0.00000586  \n","Epoch: [4][500/2223] Elapsed 2m 49s (remain 9m 41s) Loss: 0.0015(0.0072) Grad: 12172.4033  LR: 0.00000561  \n","Epoch: [4][600/2223] Elapsed 3m 22s (remain 9m 7s) Loss: 0.0007(0.0068) Grad: 3372.0962  LR: 0.00000535  \n","Epoch: [4][700/2223] Elapsed 3m 56s (remain 8m 33s) Loss: 0.0083(0.0066) Grad: 13704.0312  LR: 0.00000510  \n","Epoch: [4][800/2223] Elapsed 4m 29s (remain 7m 59s) Loss: 0.0026(0.0064) Grad: 16195.8291  LR: 0.00000486  \n","Epoch: [4][900/2223] Elapsed 5m 3s (remain 7m 25s) Loss: 0.0006(0.0065) Grad: 2381.8406  LR: 0.00000462  \n","Epoch: [4][1000/2223] Elapsed 5m 36s (remain 6m 51s) Loss: 0.0040(0.0063) Grad: 4131.4365  LR: 0.00000438  \n","Epoch: [4][1100/2223] Elapsed 6m 10s (remain 6m 17s) Loss: 0.0021(0.0063) Grad: 7500.3276  LR: 0.00000415  \n","Epoch: [4][1200/2223] Elapsed 6m 44s (remain 5m 43s) Loss: 0.0002(0.0061) Grad: 1424.6284  LR: 0.00000393  \n","Epoch: [4][1300/2223] Elapsed 7m 17s (remain 5m 10s) Loss: 0.0013(0.0063) Grad: 8336.1953  LR: 0.00000370  \n","Epoch: [4][1400/2223] Elapsed 7m 51s (remain 4m 36s) Loss: 0.0015(0.0064) Grad: 6058.7549  LR: 0.00000349  \n","Epoch: [4][1500/2223] Elapsed 8m 24s (remain 4m 2s) Loss: 0.0000(0.0064) Grad: 130.6716  LR: 0.00000328  \n","Epoch: [4][1600/2223] Elapsed 8m 58s (remain 3m 29s) Loss: 0.0001(0.0064) Grad: 455.0998  LR: 0.00000307  \n","Epoch: [4][1700/2223] Elapsed 9m 31s (remain 2m 55s) Loss: 0.0142(0.0064) Grad: 28454.6016  LR: 0.00000287  \n","Epoch: [4][1800/2223] Elapsed 10m 5s (remain 2m 21s) Loss: 0.0001(0.0064) Grad: 602.6296  LR: 0.00000267  \n","Epoch: [4][1900/2223] Elapsed 10m 39s (remain 1m 48s) Loss: 0.0029(0.0063) Grad: 16734.7383  LR: 0.00000248  \n","Epoch: [4][2000/2223] Elapsed 11m 12s (remain 1m 14s) Loss: 0.0049(0.0063) Grad: 25474.4902  LR: 0.00000230  \n","Epoch: [4][2100/2223] Elapsed 11m 46s (remain 0m 41s) Loss: 0.0442(0.0064) Grad: 125764.1172  LR: 0.00000212  \n","Epoch: [4][2200/2223] Elapsed 12m 19s (remain 0m 7s) Loss: 0.0035(0.0064) Grad: 31831.1543  LR: 0.00000195  \n","Epoch: [4][2222/2223] Elapsed 12m 26s (remain 0m 0s) Loss: 0.0014(0.0064) Grad: 13930.6328  LR: 0.00000192  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 11s) Loss: 0.0066(0.0066) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0096(0.0156) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0000(0.0142) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.0064  avg_val_loss: 0.0142  time: 781s\n","Epoch 4 - Score: 0.8821\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [5][0/2223] Elapsed 0m 0s (remain 29m 11s) Loss: 0.0184(0.0184) Grad: 34610.7188  LR: 0.00000191  \n","Epoch: [5][100/2223] Elapsed 0m 34s (remain 12m 3s) Loss: 0.0024(0.0054) Grad: 11208.2676  LR: 0.00000175  \n","Epoch: [5][200/2223] Elapsed 1m 8s (remain 11m 25s) Loss: 0.0002(0.0063) Grad: 2172.6938  LR: 0.00000159  \n","Epoch: [5][300/2223] Elapsed 1m 41s (remain 10m 49s) Loss: 0.0001(0.0057) Grad: 445.7188  LR: 0.00000144  \n","Epoch: [5][400/2223] Elapsed 2m 15s (remain 10m 14s) Loss: 0.0040(0.0057) Grad: 40500.0664  LR: 0.00000130  \n","Epoch: [5][500/2223] Elapsed 2m 48s (remain 9m 40s) Loss: 0.0003(0.0057) Grad: 2346.2302  LR: 0.00000117  \n","Epoch: [5][600/2223] Elapsed 3m 22s (remain 9m 6s) Loss: 0.0001(0.0058) Grad: 608.3282  LR: 0.00000104  \n","Epoch: [5][700/2223] Elapsed 3m 55s (remain 8m 32s) Loss: 0.0156(0.0059) Grad: 73828.8750  LR: 0.00000092  \n","Epoch: [5][800/2223] Elapsed 4m 29s (remain 7m 58s) Loss: 0.0032(0.0057) Grad: 25918.2910  LR: 0.00000080  \n","Epoch: [5][900/2223] Elapsed 5m 3s (remain 7m 24s) Loss: 0.0053(0.0055) Grad: 11690.7178  LR: 0.00000069  \n","Epoch: [5][1000/2223] Elapsed 5m 36s (remain 6m 50s) Loss: 0.0012(0.0054) Grad: 10014.7744  LR: 0.00000059  \n","Epoch: [5][1100/2223] Elapsed 6m 10s (remain 6m 17s) Loss: 0.0088(0.0054) Grad: 18924.1172  LR: 0.00000050  \n","Epoch: [5][1200/2223] Elapsed 6m 43s (remain 5m 43s) Loss: 0.0056(0.0055) Grad: 68134.3359  LR: 0.00000042  \n","Epoch: [5][1300/2223] Elapsed 7m 17s (remain 5m 9s) Loss: 0.0039(0.0055) Grad: 4949.3052  LR: 0.00000034  \n","Epoch: [5][1400/2223] Elapsed 7m 50s (remain 4m 36s) Loss: 0.0002(0.0055) Grad: 1450.1379  LR: 0.00000027  \n","Epoch: [5][1500/2223] Elapsed 8m 24s (remain 4m 2s) Loss: 0.0112(0.0054) Grad: 36932.4141  LR: 0.00000021  \n","Epoch: [5][1600/2223] Elapsed 8m 57s (remain 3m 29s) Loss: 0.0029(0.0054) Grad: 18607.9688  LR: 0.00000016  \n","Epoch: [5][1700/2223] Elapsed 9m 31s (remain 2m 55s) Loss: 0.0057(0.0054) Grad: 21798.1465  LR: 0.00000011  \n","Epoch: [5][1800/2223] Elapsed 10m 4s (remain 2m 21s) Loss: 0.0009(0.0054) Grad: 8890.2764  LR: 0.00000007  \n","Epoch: [5][1900/2223] Elapsed 10m 38s (remain 1m 48s) Loss: 0.0025(0.0054) Grad: 17363.8652  LR: 0.00000004  \n","Epoch: [5][2000/2223] Elapsed 11m 11s (remain 1m 14s) Loss: 0.0000(0.0054) Grad: 177.6707  LR: 0.00000002  \n","Epoch: [5][2100/2223] Elapsed 11m 45s (remain 0m 40s) Loss: 0.0000(0.0053) Grad: 268.8589  LR: 0.00000001  \n","Epoch: [5][2200/2223] Elapsed 12m 18s (remain 0m 7s) Loss: 0.0000(0.0054) Grad: 509.2362  LR: 0.00000000  \n","Epoch: [5][2222/2223] Elapsed 12m 26s (remain 0m 0s) Loss: 0.0040(0.0054) Grad: 29223.9375  LR: 0.00000000  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 12s) Loss: 0.0106(0.0106) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0092(0.0162) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0000(0.0149) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.0054  avg_val_loss: 0.0149  time: 780s\n","Epoch 5 - Score: 0.8846\n","Epoch 5 - Save Best Score: 0.8846 Model\n","========== fold: 2 result ==========\n","Score: 0.8846\n","========== fold: 3 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2223] Elapsed 0m 0s (remain 21m 16s) Loss: 0.5354(0.5354) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2223] Elapsed 0m 34s (remain 12m 3s) Loss: 0.0693(0.0787) Grad: 11566.8330  LR: 0.00002000  \n","Epoch: [1][200/2223] Elapsed 1m 7s (remain 11m 22s) Loss: 0.0348(0.0551) Grad: 8863.2080  LR: 0.00001998  \n","Epoch: [1][300/2223] Elapsed 1m 41s (remain 10m 46s) Loss: 0.0421(0.0453) Grad: 6610.3833  LR: 0.00001996  \n","Epoch: [1][400/2223] Elapsed 2m 14s (remain 10m 11s) Loss: 0.0142(0.0392) Grad: 6265.7256  LR: 0.00001994  \n","Epoch: [1][500/2223] Elapsed 2m 47s (remain 9m 37s) Loss: 0.0051(0.0359) Grad: 1831.8226  LR: 0.00001990  \n","Epoch: [1][600/2223] Elapsed 3m 21s (remain 9m 3s) Loss: 0.0060(0.0330) Grad: 2552.3870  LR: 0.00001986  \n","Epoch: [1][700/2223] Elapsed 3m 54s (remain 8m 29s) Loss: 0.0141(0.0309) Grad: 2766.4885  LR: 0.00001980  \n","Epoch: [1][800/2223] Elapsed 4m 28s (remain 7m 56s) Loss: 0.0254(0.0290) Grad: 1857.0863  LR: 0.00001974  \n","Epoch: [1][900/2223] Elapsed 5m 1s (remain 7m 22s) Loss: 0.0076(0.0278) Grad: 2720.0593  LR: 0.00001968  \n","Epoch: [1][1000/2223] Elapsed 5m 35s (remain 6m 49s) Loss: 0.0318(0.0268) Grad: 7497.5854  LR: 0.00001960  \n","Epoch: [1][1100/2223] Elapsed 6m 8s (remain 6m 15s) Loss: 0.0017(0.0259) Grad: 5329.8364  LR: 0.00001952  \n","Epoch: [1][1200/2223] Elapsed 6m 42s (remain 5m 42s) Loss: 0.0262(0.0251) Grad: 3654.3965  LR: 0.00001943  \n","Epoch: [1][1300/2223] Elapsed 7m 16s (remain 5m 9s) Loss: 0.0028(0.0242) Grad: 1821.6382  LR: 0.00001933  \n","Epoch: [1][1400/2223] Elapsed 7m 49s (remain 4m 35s) Loss: 0.0080(0.0237) Grad: 2215.4209  LR: 0.00001923  \n","Epoch: [1][1500/2223] Elapsed 8m 23s (remain 4m 2s) Loss: 0.0034(0.0232) Grad: 2761.4084  LR: 0.00001911  \n","Epoch: [1][1600/2223] Elapsed 8m 56s (remain 3m 28s) Loss: 0.0017(0.0227) Grad: 509.0018  LR: 0.00001899  \n","Epoch: [1][1700/2223] Elapsed 9m 30s (remain 2m 55s) Loss: 0.0046(0.0222) Grad: 779.1265  LR: 0.00001887  \n","Epoch: [1][1800/2223] Elapsed 10m 3s (remain 2m 21s) Loss: 0.0164(0.0218) Grad: 2853.6792  LR: 0.00001873  \n","Epoch: [1][1900/2223] Elapsed 10m 37s (remain 1m 47s) Loss: 0.0247(0.0214) Grad: 2609.5615  LR: 0.00001859  \n","Epoch: [1][2000/2223] Elapsed 11m 10s (remain 1m 14s) Loss: 0.0016(0.0210) Grad: 391.2204  LR: 0.00001844  \n","Epoch: [1][2100/2223] Elapsed 11m 44s (remain 0m 40s) Loss: 0.0078(0.0206) Grad: 2373.2747  LR: 0.00001829  \n","Epoch: [1][2200/2223] Elapsed 12m 17s (remain 0m 7s) Loss: 0.0025(0.0202) Grad: 1103.2677  LR: 0.00001813  \n","Epoch: [1][2222/2223] Elapsed 12m 25s (remain 0m 0s) Loss: 0.0054(0.0201) Grad: 994.3516  LR: 0.00001809  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 10s) Loss: 0.0096(0.0096) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0001(0.0183) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0003(0.0148) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.0201  avg_val_loss: 0.0148  time: 779s\n","Epoch 1 - Score: 0.8639\n","Epoch 1 - Save Best Score: 0.8639 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [2][0/2223] Elapsed 0m 0s (remain 29m 57s) Loss: 0.0005(0.0005) Grad: 1012.9467  LR: 0.00001809  \n","Epoch: [2][100/2223] Elapsed 0m 34s (remain 12m 13s) Loss: 0.0247(0.0111) Grad: 20974.2949  LR: 0.00001792  \n","Epoch: [2][200/2223] Elapsed 1m 8s (remain 11m 30s) Loss: 0.0117(0.0095) Grad: 17669.4180  LR: 0.00001774  \n","Epoch: [2][300/2223] Elapsed 1m 42s (remain 10m 53s) Loss: 0.0026(0.0096) Grad: 10334.3271  LR: 0.00001756  \n","Epoch: [2][400/2223] Elapsed 2m 16s (remain 10m 18s) Loss: 0.0006(0.0092) Grad: 2341.6694  LR: 0.00001738  \n","Epoch: [2][500/2223] Elapsed 2m 49s (remain 9m 43s) Loss: 0.0051(0.0096) Grad: 3562.9636  LR: 0.00001718  \n","Epoch: [2][600/2223] Elapsed 3m 23s (remain 9m 9s) Loss: 0.0048(0.0093) Grad: 15606.7900  LR: 0.00001698  \n","Epoch: [2][700/2223] Elapsed 3m 57s (remain 8m 35s) Loss: 0.0020(0.0096) Grad: 4967.7271  LR: 0.00001678  \n","Epoch: [2][800/2223] Elapsed 4m 31s (remain 8m 1s) Loss: 0.0032(0.0095) Grad: 6105.1211  LR: 0.00001657  \n","Epoch: [2][900/2223] Elapsed 5m 4s (remain 7m 27s) Loss: 0.0013(0.0097) Grad: 4007.6382  LR: 0.00001635  \n","Epoch: [2][1000/2223] Elapsed 5m 38s (remain 6m 53s) Loss: 0.0104(0.0098) Grad: 18673.6973  LR: 0.00001613  \n","Epoch: [2][1100/2223] Elapsed 6m 12s (remain 6m 19s) Loss: 0.0016(0.0099) Grad: 4172.1592  LR: 0.00001590  \n","Epoch: [2][1200/2223] Elapsed 6m 45s (remain 5m 45s) Loss: 0.0039(0.0097) Grad: 18265.3965  LR: 0.00001567  \n","Epoch: [2][1300/2223] Elapsed 7m 19s (remain 5m 11s) Loss: 0.0038(0.0098) Grad: 19028.5469  LR: 0.00001544  \n","Epoch: [2][1400/2223] Elapsed 7m 53s (remain 4m 37s) Loss: 0.0126(0.0098) Grad: 11351.7686  LR: 0.00001520  \n","Epoch: [2][1500/2223] Elapsed 8m 27s (remain 4m 4s) Loss: 0.0049(0.0096) Grad: 5190.9238  LR: 0.00001496  \n","Epoch: [2][1600/2223] Elapsed 9m 0s (remain 3m 30s) Loss: 0.0014(0.0095) Grad: 7265.1875  LR: 0.00001471  \n","Epoch: [2][1700/2223] Elapsed 9m 34s (remain 2m 56s) Loss: 0.0004(0.0096) Grad: 1106.0958  LR: 0.00001446  \n","Epoch: [2][1800/2223] Elapsed 10m 8s (remain 2m 22s) Loss: 0.0005(0.0095) Grad: 1435.2233  LR: 0.00001420  \n","Epoch: [2][1900/2223] Elapsed 10m 41s (remain 1m 48s) Loss: 0.0001(0.0094) Grad: 591.8214  LR: 0.00001395  \n","Epoch: [2][2000/2223] Elapsed 11m 15s (remain 1m 14s) Loss: 0.0216(0.0093) Grad: 61223.9062  LR: 0.00001368  \n","Epoch: [2][2100/2223] Elapsed 11m 48s (remain 0m 41s) Loss: 0.0150(0.0093) Grad: 45172.2031  LR: 0.00001342  \n","Epoch: [2][2200/2223] Elapsed 12m 22s (remain 0m 7s) Loss: 0.0311(0.0095) Grad: 88477.3281  LR: 0.00001315  \n","Epoch: [2][2222/2223] Elapsed 12m 29s (remain 0m 0s) Loss: 0.0075(0.0095) Grad: 50426.6641  LR: 0.00001309  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 17s) Loss: 0.0045(0.0045) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0000(0.0160) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0000(0.0134) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.0095  avg_val_loss: 0.0134  time: 783s\n","Epoch 2 - Score: 0.8734\n","Epoch 2 - Save Best Score: 0.8734 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [3][0/2223] Elapsed 0m 0s (remain 27m 45s) Loss: 0.0029(0.0029) Grad: 7627.6992  LR: 0.00001309  \n","Epoch: [3][100/2223] Elapsed 0m 34s (remain 12m 4s) Loss: 0.0001(0.0079) Grad: 151.7465  LR: 0.00001282  \n","Epoch: [3][200/2223] Elapsed 1m 8s (remain 11m 24s) Loss: 0.0018(0.0087) Grad: 9379.7617  LR: 0.00001255  \n","Epoch: [3][300/2223] Elapsed 1m 41s (remain 10m 48s) Loss: 0.0037(0.0074) Grad: 35815.4375  LR: 0.00001228  \n","Epoch: [3][400/2223] Elapsed 2m 14s (remain 10m 12s) Loss: 0.0072(0.0074) Grad: 20878.8887  LR: 0.00001200  \n","Epoch: [3][500/2223] Elapsed 2m 48s (remain 9m 38s) Loss: 0.0016(0.0075) Grad: 6408.3032  LR: 0.00001172  \n","Epoch: [3][600/2223] Elapsed 3m 21s (remain 9m 4s) Loss: 0.0106(0.0077) Grad: 18972.6582  LR: 0.00001144  \n","Epoch: [3][700/2223] Elapsed 3m 55s (remain 8m 30s) Loss: 0.0000(0.0077) Grad: 140.2009  LR: 0.00001116  \n","Epoch: [3][800/2223] Elapsed 4m 28s (remain 7m 56s) Loss: 0.0000(0.0076) Grad: 120.1017  LR: 0.00001088  \n","Epoch: [3][900/2223] Elapsed 5m 2s (remain 7m 23s) Loss: 0.0001(0.0077) Grad: 2425.4182  LR: 0.00001060  \n","Epoch: [3][1000/2223] Elapsed 5m 35s (remain 6m 49s) Loss: 0.0004(0.0078) Grad: 2683.5486  LR: 0.00001032  \n","Epoch: [3][1100/2223] Elapsed 6m 9s (remain 6m 16s) Loss: 0.0000(0.0077) Grad: 27.1466  LR: 0.00001004  \n","Epoch: [3][1200/2223] Elapsed 6m 42s (remain 5m 42s) Loss: 0.0000(0.0077) Grad: 148.7868  LR: 0.00000975  \n","Epoch: [3][1300/2223] Elapsed 7m 16s (remain 5m 9s) Loss: 0.0029(0.0077) Grad: 17010.9219  LR: 0.00000947  \n","Epoch: [3][1400/2223] Elapsed 7m 49s (remain 4m 35s) Loss: 0.0016(0.0077) Grad: 5817.4941  LR: 0.00000919  \n","Epoch: [3][1500/2223] Elapsed 8m 23s (remain 4m 2s) Loss: 0.0143(0.0078) Grad: 118209.6719  LR: 0.00000891  \n","Epoch: [3][1600/2223] Elapsed 8m 57s (remain 3m 28s) Loss: 0.0155(0.0079) Grad: 40034.2266  LR: 0.00000863  \n","Epoch: [3][1700/2223] Elapsed 9m 30s (remain 2m 55s) Loss: 0.0283(0.0078) Grad: 23036.6211  LR: 0.00000835  \n","Epoch: [3][1800/2223] Elapsed 10m 4s (remain 2m 21s) Loss: 0.0087(0.0079) Grad: 10892.7539  LR: 0.00000807  \n","Epoch: [3][1900/2223] Elapsed 10m 37s (remain 1m 48s) Loss: 0.0000(0.0078) Grad: 98.8972  LR: 0.00000779  \n","Epoch: [3][2000/2223] Elapsed 11m 11s (remain 1m 14s) Loss: 0.0057(0.0078) Grad: 50666.9727  LR: 0.00000752  \n","Epoch: [3][2100/2223] Elapsed 11m 45s (remain 0m 40s) Loss: 0.0011(0.0078) Grad: 21318.1348  LR: 0.00000725  \n","Epoch: [3][2200/2223] Elapsed 12m 18s (remain 0m 7s) Loss: 0.0002(0.0078) Grad: 3057.2290  LR: 0.00000698  \n","Epoch: [3][2222/2223] Elapsed 12m 25s (remain 0m 0s) Loss: 0.0201(0.0079) Grad: 67637.1953  LR: 0.00000692  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 10s) Loss: 0.0027(0.0027) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0000(0.0181) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0000(0.0152) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.0079  avg_val_loss: 0.0152  time: 780s\n","Epoch 3 - Score: 0.8741\n","Epoch 3 - Save Best Score: 0.8741 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [4][0/2223] Elapsed 0m 0s (remain 27m 59s) Loss: 0.0057(0.0057) Grad: 24197.2832  LR: 0.00000691  \n","Epoch: [4][100/2223] Elapsed 0m 35s (remain 12m 16s) Loss: 0.0019(0.0066) Grad: 16872.2070  LR: 0.00000665  \n","Epoch: [4][200/2223] Elapsed 1m 8s (remain 11m 32s) Loss: 0.0055(0.0066) Grad: 30992.4941  LR: 0.00000638  \n","Epoch: [4][300/2223] Elapsed 1m 42s (remain 10m 54s) Loss: 0.0035(0.0066) Grad: 41453.7109  LR: 0.00000612  \n","Epoch: [4][400/2223] Elapsed 2m 16s (remain 10m 19s) Loss: 0.0000(0.0064) Grad: 94.1818  LR: 0.00000586  \n","Epoch: [4][500/2223] Elapsed 2m 50s (remain 9m 44s) Loss: 0.0001(0.0062) Grad: 618.8358  LR: 0.00000561  \n","Epoch: [4][600/2223] Elapsed 3m 23s (remain 9m 9s) Loss: 0.0009(0.0061) Grad: 4365.0371  LR: 0.00000535  \n","Epoch: [4][700/2223] Elapsed 3m 57s (remain 8m 35s) Loss: 0.0001(0.0060) Grad: 526.0206  LR: 0.00000510  \n","Epoch: [4][800/2223] Elapsed 4m 31s (remain 8m 1s) Loss: 0.0045(0.0060) Grad: 37266.8164  LR: 0.00000486  \n","Epoch: [4][900/2223] Elapsed 5m 5s (remain 7m 27s) Loss: 0.0014(0.0063) Grad: 14940.2949  LR: 0.00000462  \n","Epoch: [4][1000/2223] Elapsed 5m 38s (remain 6m 53s) Loss: 0.0001(0.0062) Grad: 266.9102  LR: 0.00000438  \n","Epoch: [4][1100/2223] Elapsed 6m 12s (remain 6m 19s) Loss: 0.0109(0.0061) Grad: 10148.2227  LR: 0.00000415  \n","Epoch: [4][1200/2223] Elapsed 6m 46s (remain 5m 45s) Loss: 0.0001(0.0061) Grad: 391.9446  LR: 0.00000393  \n","Epoch: [4][1300/2223] Elapsed 7m 20s (remain 5m 11s) Loss: 0.0040(0.0062) Grad: 10660.3262  LR: 0.00000370  \n","Epoch: [4][1400/2223] Elapsed 7m 53s (remain 4m 38s) Loss: 0.0014(0.0064) Grad: 7522.0615  LR: 0.00000349  \n","Epoch: [4][1500/2223] Elapsed 8m 27s (remain 4m 4s) Loss: 0.0002(0.0063) Grad: 797.5931  LR: 0.00000328  \n","Epoch: [4][1600/2223] Elapsed 9m 1s (remain 3m 30s) Loss: 0.0001(0.0063) Grad: 235.9879  LR: 0.00000307  \n","Epoch: [4][1700/2223] Elapsed 9m 34s (remain 2m 56s) Loss: 0.0004(0.0063) Grad: 3454.3762  LR: 0.00000287  \n","Epoch: [4][1800/2223] Elapsed 10m 8s (remain 2m 22s) Loss: 0.0431(0.0062) Grad: 82940.9688  LR: 0.00000267  \n","Epoch: [4][1900/2223] Elapsed 10m 42s (remain 1m 48s) Loss: 0.0035(0.0061) Grad: 12754.6357  LR: 0.00000248  \n","Epoch: [4][2000/2223] Elapsed 11m 15s (remain 1m 14s) Loss: 0.0026(0.0062) Grad: 17856.7207  LR: 0.00000230  \n","Epoch: [4][2100/2223] Elapsed 11m 49s (remain 0m 41s) Loss: 0.0065(0.0062) Grad: 37352.5430  LR: 0.00000212  \n","Epoch: [4][2200/2223] Elapsed 12m 23s (remain 0m 7s) Loss: 0.0001(0.0062) Grad: 1382.6908  LR: 0.00000195  \n","Epoch: [4][2222/2223] Elapsed 12m 30s (remain 0m 0s) Loss: 0.0055(0.0062) Grad: 34569.9258  LR: 0.00000192  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 18s) Loss: 0.0024(0.0024) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0000(0.0200) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0000(0.0169) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.0062  avg_val_loss: 0.0169  time: 785s\n","Epoch 4 - Score: 0.8713\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [5][0/2223] Elapsed 0m 0s (remain 28m 51s) Loss: 0.0278(0.0278) Grad: 23940.5957  LR: 0.00000191  \n","Epoch: [5][100/2223] Elapsed 0m 34s (remain 11m 58s) Loss: 0.0077(0.0052) Grad: 16297.8369  LR: 0.00000175  \n","Epoch: [5][200/2223] Elapsed 1m 7s (remain 11m 19s) Loss: 0.0000(0.0045) Grad: 81.8926  LR: 0.00000159  \n","Epoch: [5][300/2223] Elapsed 1m 41s (remain 10m 45s) Loss: 0.0014(0.0054) Grad: 3656.4709  LR: 0.00000144  \n","Epoch: [5][400/2223] Elapsed 2m 14s (remain 10m 11s) Loss: 0.0003(0.0052) Grad: 3551.3074  LR: 0.00000130  \n","Epoch: [5][500/2223] Elapsed 2m 47s (remain 9m 37s) Loss: 0.0061(0.0049) Grad: 12739.2871  LR: 0.00000117  \n","Epoch: [5][600/2223] Elapsed 3m 21s (remain 9m 3s) Loss: 0.0090(0.0050) Grad: 21709.3770  LR: 0.00000104  \n","Epoch: [5][700/2223] Elapsed 3m 54s (remain 8m 29s) Loss: 0.0025(0.0050) Grad: 11175.8320  LR: 0.00000092  \n","Epoch: [5][800/2223] Elapsed 4m 28s (remain 7m 56s) Loss: 0.0007(0.0050) Grad: 3911.3640  LR: 0.00000080  \n","Epoch: [5][900/2223] Elapsed 5m 1s (remain 7m 22s) Loss: 0.0006(0.0050) Grad: 5785.1309  LR: 0.00000069  \n","Epoch: [5][1000/2223] Elapsed 5m 35s (remain 6m 49s) Loss: 0.0071(0.0049) Grad: 9077.0830  LR: 0.00000059  \n","Epoch: [5][1100/2223] Elapsed 6m 8s (remain 6m 15s) Loss: 0.0349(0.0051) Grad: 33746.6445  LR: 0.00000050  \n","Epoch: [5][1200/2223] Elapsed 6m 42s (remain 5m 42s) Loss: 0.0086(0.0051) Grad: 22825.2910  LR: 0.00000042  \n","Epoch: [5][1300/2223] Elapsed 7m 15s (remain 5m 8s) Loss: 0.0048(0.0051) Grad: 44608.8477  LR: 0.00000034  \n","Epoch: [5][1400/2223] Elapsed 7m 49s (remain 4m 35s) Loss: 0.0007(0.0051) Grad: 16886.4766  LR: 0.00000027  \n","Epoch: [5][1500/2223] Elapsed 8m 23s (remain 4m 1s) Loss: 0.0014(0.0051) Grad: 34294.2383  LR: 0.00000021  \n","Epoch: [5][1600/2223] Elapsed 8m 56s (remain 3m 28s) Loss: 0.0004(0.0051) Grad: 2004.1388  LR: 0.00000016  \n","Epoch: [5][1700/2223] Elapsed 9m 30s (remain 2m 54s) Loss: 0.0009(0.0050) Grad: 9590.3838  LR: 0.00000011  \n","Epoch: [5][1800/2223] Elapsed 10m 3s (remain 2m 21s) Loss: 0.0135(0.0051) Grad: 26844.9492  LR: 0.00000007  \n","Epoch: [5][1900/2223] Elapsed 10m 37s (remain 1m 47s) Loss: 0.0039(0.0052) Grad: 8452.4961  LR: 0.00000004  \n","Epoch: [5][2000/2223] Elapsed 11m 10s (remain 1m 14s) Loss: 0.0002(0.0051) Grad: 5471.2119  LR: 0.00000002  \n","Epoch: [5][2100/2223] Elapsed 11m 44s (remain 0m 40s) Loss: 0.0000(0.0051) Grad: 99.2300  LR: 0.00000001  \n","Epoch: [5][2200/2223] Elapsed 12m 17s (remain 0m 7s) Loss: 0.0147(0.0052) Grad: 133880.3906  LR: 0.00000000  \n","Epoch: [5][2222/2223] Elapsed 12m 24s (remain 0m 0s) Loss: 0.0029(0.0052) Grad: 35860.3828  LR: 0.00000000  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 25s) Loss: 0.0062(0.0062) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0000(0.0209) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0000(0.0177) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.0052  avg_val_loss: 0.0177  time: 779s\n","Epoch 5 - Score: 0.8726\n","========== fold: 3 result ==========\n","Score: 0.8741\n","========== fold: 4 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2223] Elapsed 0m 0s (remain 20m 38s) Loss: 0.6615(0.6615) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2223] Elapsed 0m 34s (remain 11m 55s) Loss: 0.0081(0.0798) Grad: 1461.0142  LR: 0.00002000  \n","Epoch: [1][200/2223] Elapsed 1m 7s (remain 11m 20s) Loss: 0.0200(0.0552) Grad: 2476.5679  LR: 0.00001998  \n","Epoch: [1][300/2223] Elapsed 1m 41s (remain 10m 47s) Loss: 0.0354(0.0452) Grad: 3595.7688  LR: 0.00001996  \n","Epoch: [1][400/2223] Elapsed 2m 14s (remain 10m 13s) Loss: 0.0340(0.0397) Grad: 2015.5282  LR: 0.00001994  \n","Epoch: [1][500/2223] Elapsed 2m 48s (remain 9m 38s) Loss: 0.0022(0.0356) Grad: 454.2492  LR: 0.00001990  \n","Epoch: [1][600/2223] Elapsed 3m 21s (remain 9m 4s) Loss: 0.0304(0.0326) Grad: 7776.4819  LR: 0.00001986  \n","Epoch: [1][700/2223] Elapsed 3m 55s (remain 8m 30s) Loss: 0.0079(0.0305) Grad: 2424.2107  LR: 0.00001980  \n","Epoch: [1][800/2223] Elapsed 4m 28s (remain 7m 57s) Loss: 0.0111(0.0286) Grad: 1751.9072  LR: 0.00001974  \n","Epoch: [1][900/2223] Elapsed 5m 2s (remain 7m 23s) Loss: 0.0153(0.0273) Grad: 1288.5734  LR: 0.00001968  \n","Epoch: [1][1000/2223] Elapsed 5m 35s (remain 6m 49s) Loss: 0.0077(0.0259) Grad: 3274.7939  LR: 0.00001960  \n","Epoch: [1][1100/2223] Elapsed 6m 9s (remain 6m 16s) Loss: 0.0452(0.0248) Grad: 2295.9929  LR: 0.00001952  \n","Epoch: [1][1200/2223] Elapsed 6m 42s (remain 5m 42s) Loss: 0.0045(0.0238) Grad: 781.1731  LR: 0.00001943  \n","Epoch: [1][1300/2223] Elapsed 7m 16s (remain 5m 9s) Loss: 0.0003(0.0231) Grad: 40.8078  LR: 0.00001933  \n","Epoch: [1][1400/2223] Elapsed 7m 49s (remain 4m 35s) Loss: 0.0030(0.0227) Grad: 2189.0110  LR: 0.00001923  \n","Epoch: [1][1500/2223] Elapsed 8m 23s (remain 4m 2s) Loss: 0.0214(0.0221) Grad: 1626.3201  LR: 0.00001911  \n","Epoch: [1][1600/2223] Elapsed 8m 56s (remain 3m 28s) Loss: 0.0013(0.0216) Grad: 910.0557  LR: 0.00001899  \n","Epoch: [1][1700/2223] Elapsed 9m 30s (remain 2m 54s) Loss: 0.0016(0.0212) Grad: 228.7048  LR: 0.00001887  \n","Epoch: [1][1800/2223] Elapsed 10m 3s (remain 2m 21s) Loss: 0.0005(0.0208) Grad: 80.7355  LR: 0.00001873  \n","Epoch: [1][1900/2223] Elapsed 10m 36s (remain 1m 47s) Loss: 0.0038(0.0204) Grad: 1240.9202  LR: 0.00001859  \n","Epoch: [1][2000/2223] Elapsed 11m 10s (remain 1m 14s) Loss: 0.0224(0.0202) Grad: 3558.4929  LR: 0.00001844  \n","Epoch: [1][2100/2223] Elapsed 11m 43s (remain 0m 40s) Loss: 0.0053(0.0197) Grad: 1059.0994  LR: 0.00001829  \n","Epoch: [1][2200/2223] Elapsed 12m 17s (remain 0m 7s) Loss: 0.0073(0.0195) Grad: 1650.8126  LR: 0.00001813  \n","Epoch: [1][2222/2223] Elapsed 12m 24s (remain 0m 0s) Loss: 0.0033(0.0194) Grad: 1000.9263  LR: 0.00001809  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 13s) Loss: 0.0046(0.0046) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0133(0.0133) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0003(0.0114) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.0194  avg_val_loss: 0.0114  time: 778s\n","Epoch 1 - Score: 0.8792\n","Epoch 1 - Save Best Score: 0.8792 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [2][0/2223] Elapsed 0m 0s (remain 28m 23s) Loss: 0.0119(0.0119) Grad: 14939.0801  LR: 0.00001809  \n","Epoch: [2][100/2223] Elapsed 0m 34s (remain 12m 6s) Loss: 0.0244(0.0084) Grad: 26492.0547  LR: 0.00001792  \n","Epoch: [2][200/2223] Elapsed 1m 8s (remain 11m 24s) Loss: 0.0014(0.0087) Grad: 4734.2002  LR: 0.00001774  \n","Epoch: [2][300/2223] Elapsed 1m 41s (remain 10m 48s) Loss: 0.0062(0.0080) Grad: 11676.4756  LR: 0.00001756  \n","Epoch: [2][400/2223] Elapsed 2m 15s (remain 10m 13s) Loss: 0.0003(0.0080) Grad: 1281.0446  LR: 0.00001738  \n","Epoch: [2][500/2223] Elapsed 2m 48s (remain 9m 39s) Loss: 0.0135(0.0082) Grad: 59559.6719  LR: 0.00001718  \n","Epoch: [2][600/2223] Elapsed 3m 21s (remain 9m 5s) Loss: 0.0002(0.0084) Grad: 442.2105  LR: 0.00001698  \n","Epoch: [2][700/2223] Elapsed 3m 55s (remain 8m 31s) Loss: 0.0155(0.0084) Grad: 96588.8906  LR: 0.00001678  \n","Epoch: [2][800/2223] Elapsed 4m 28s (remain 7m 57s) Loss: 0.0011(0.0084) Grad: 3573.5813  LR: 0.00001657  \n","Epoch: [2][900/2223] Elapsed 5m 2s (remain 7m 23s) Loss: 0.0003(0.0085) Grad: 2640.2988  LR: 0.00001635  \n","Epoch: [2][1000/2223] Elapsed 5m 36s (remain 6m 50s) Loss: 0.0008(0.0085) Grad: 5430.6177  LR: 0.00001613  \n","Epoch: [2][1100/2223] Elapsed 6m 9s (remain 6m 16s) Loss: 0.0047(0.0086) Grad: 40649.3555  LR: 0.00001590  \n","Epoch: [2][1200/2223] Elapsed 6m 43s (remain 5m 43s) Loss: 0.0000(0.0086) Grad: 146.5251  LR: 0.00001567  \n","Epoch: [2][1300/2223] Elapsed 7m 16s (remain 5m 9s) Loss: 0.0138(0.0086) Grad: 17052.4609  LR: 0.00001544  \n","Epoch: [2][1400/2223] Elapsed 7m 50s (remain 4m 36s) Loss: 0.0278(0.0087) Grad: 67223.6172  LR: 0.00001520  \n","Epoch: [2][1500/2223] Elapsed 8m 24s (remain 4m 2s) Loss: 0.0140(0.0086) Grad: 52515.4414  LR: 0.00001496  \n","Epoch: [2][1600/2223] Elapsed 8m 57s (remain 3m 28s) Loss: 0.0202(0.0087) Grad: 60105.4609  LR: 0.00001471  \n","Epoch: [2][1700/2223] Elapsed 9m 31s (remain 2m 55s) Loss: 0.0083(0.0088) Grad: 12474.2676  LR: 0.00001446  \n","Epoch: [2][1800/2223] Elapsed 10m 4s (remain 2m 21s) Loss: 0.0077(0.0088) Grad: 16370.3291  LR: 0.00001420  \n","Epoch: [2][1900/2223] Elapsed 10m 38s (remain 1m 48s) Loss: 0.0011(0.0088) Grad: 4545.0952  LR: 0.00001395  \n","Epoch: [2][2000/2223] Elapsed 11m 12s (remain 1m 14s) Loss: 0.0072(0.0089) Grad: 76923.9062  LR: 0.00001368  \n","Epoch: [2][2100/2223] Elapsed 11m 45s (remain 0m 40s) Loss: 0.0041(0.0089) Grad: 16709.6602  LR: 0.00001342  \n","Epoch: [2][2200/2223] Elapsed 12m 19s (remain 0m 7s) Loss: 0.0002(0.0089) Grad: 1601.5505  LR: 0.00001315  \n","Epoch: [2][2222/2223] Elapsed 12m 26s (remain 0m 0s) Loss: 0.0061(0.0089) Grad: 13754.9824  LR: 0.00001309  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 11s) Loss: 0.0049(0.0049) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0267(0.0134) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0000(0.0117) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.0089  avg_val_loss: 0.0117  time: 781s\n","Epoch 2 - Score: 0.8831\n","Epoch 2 - Save Best Score: 0.8831 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [3][0/2223] Elapsed 0m 0s (remain 30m 29s) Loss: 0.0051(0.0051) Grad: 19714.6582  LR: 0.00001309  \n","Epoch: [3][100/2223] Elapsed 0m 34s (remain 12m 1s) Loss: 0.0081(0.0063) Grad: 13689.8477  LR: 0.00001282  \n","Epoch: [3][200/2223] Elapsed 1m 7s (remain 11m 20s) Loss: 0.0001(0.0074) Grad: 1454.4967  LR: 0.00001255  \n","Epoch: [3][300/2223] Elapsed 1m 41s (remain 10m 45s) Loss: 0.0011(0.0079) Grad: 14590.6680  LR: 0.00001228  \n","Epoch: [3][400/2223] Elapsed 2m 14s (remain 10m 10s) Loss: 0.0177(0.0081) Grad: 64752.8789  LR: 0.00001200  \n","Epoch: [3][500/2223] Elapsed 2m 47s (remain 9m 35s) Loss: 0.0033(0.0080) Grad: 6762.5029  LR: 0.00001172  \n","Epoch: [3][600/2223] Elapsed 3m 20s (remain 9m 1s) Loss: 0.0054(0.0079) Grad: 18045.6719  LR: 0.00001144  \n","Epoch: [3][700/2223] Elapsed 3m 54s (remain 8m 28s) Loss: 0.0116(0.0081) Grad: 127085.6172  LR: 0.00001116  \n","Epoch: [3][800/2223] Elapsed 4m 27s (remain 7m 54s) Loss: 0.0163(0.0081) Grad: 15599.9531  LR: 0.00001088  \n","Epoch: [3][900/2223] Elapsed 5m 0s (remain 7m 21s) Loss: 0.0077(0.0079) Grad: 15446.0010  LR: 0.00001060  \n","Epoch: [3][1000/2223] Elapsed 5m 34s (remain 6m 47s) Loss: 0.0183(0.0078) Grad: 19491.9316  LR: 0.00001032  \n","Epoch: [3][1100/2223] Elapsed 6m 7s (remain 6m 14s) Loss: 0.0022(0.0079) Grad: 15446.6113  LR: 0.00001004  \n","Epoch: [3][1200/2223] Elapsed 6m 40s (remain 5m 41s) Loss: 0.0196(0.0078) Grad: 129022.8594  LR: 0.00000975  \n","Epoch: [3][1300/2223] Elapsed 7m 14s (remain 5m 7s) Loss: 0.0067(0.0077) Grad: 15747.7490  LR: 0.00000947  \n","Epoch: [3][1400/2223] Elapsed 7m 47s (remain 4m 34s) Loss: 0.0054(0.0077) Grad: 14810.8525  LR: 0.00000919  \n","Epoch: [3][1500/2223] Elapsed 8m 21s (remain 4m 1s) Loss: 0.0206(0.0077) Grad: 81602.9766  LR: 0.00000891  \n","Epoch: [3][1600/2223] Elapsed 8m 54s (remain 3m 27s) Loss: 0.0002(0.0077) Grad: 1006.7744  LR: 0.00000863  \n","Epoch: [3][1700/2223] Elapsed 9m 28s (remain 2m 54s) Loss: 0.0018(0.0077) Grad: 8847.6855  LR: 0.00000835  \n","Epoch: [3][1800/2223] Elapsed 10m 1s (remain 2m 20s) Loss: 0.0147(0.0076) Grad: 52979.0234  LR: 0.00000807  \n","Epoch: [3][1900/2223] Elapsed 10m 34s (remain 1m 47s) Loss: 0.0032(0.0076) Grad: 19953.4570  LR: 0.00000779  \n","Epoch: [3][2000/2223] Elapsed 11m 8s (remain 1m 14s) Loss: 0.0028(0.0076) Grad: 24217.2383  LR: 0.00000752  \n","Epoch: [3][2100/2223] Elapsed 11m 41s (remain 0m 40s) Loss: 0.0120(0.0075) Grad: 35495.1484  LR: 0.00000725  \n","Epoch: [3][2200/2223] Elapsed 12m 15s (remain 0m 7s) Loss: 0.0105(0.0075) Grad: 45791.5195  LR: 0.00000698  \n","Epoch: [3][2222/2223] Elapsed 12m 22s (remain 0m 0s) Loss: 0.0023(0.0075) Grad: 21197.8105  LR: 0.00000692  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 19s) Loss: 0.0034(0.0034) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0275(0.0153) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0000(0.0132) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.0075  avg_val_loss: 0.0132  time: 776s\n","Epoch 3 - Score: 0.8857\n","Epoch 3 - Save Best Score: 0.8857 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [4][0/2223] Elapsed 0m 0s (remain 28m 49s) Loss: 0.0039(0.0039) Grad: 12013.0811  LR: 0.00000691  \n","Epoch: [4][100/2223] Elapsed 0m 35s (remain 12m 15s) Loss: 0.0021(0.0069) Grad: 3685.9216  LR: 0.00000665  \n","Epoch: [4][200/2223] Elapsed 1m 8s (remain 11m 33s) Loss: 0.0057(0.0058) Grad: 35263.8672  LR: 0.00000638  \n","Epoch: [4][300/2223] Elapsed 1m 42s (remain 10m 54s) Loss: 0.0001(0.0063) Grad: 406.8341  LR: 0.00000612  \n","Epoch: [4][400/2223] Elapsed 2m 16s (remain 10m 19s) Loss: 0.0002(0.0059) Grad: 1998.2437  LR: 0.00000586  \n","Epoch: [4][500/2223] Elapsed 2m 50s (remain 9m 44s) Loss: 0.0035(0.0060) Grad: 9631.8486  LR: 0.00000561  \n","Epoch: [4][600/2223] Elapsed 3m 23s (remain 9m 10s) Loss: 0.0109(0.0061) Grad: 38427.6719  LR: 0.00000535  \n","Epoch: [4][700/2223] Elapsed 3m 57s (remain 8m 36s) Loss: 0.0092(0.0059) Grad: 22726.4766  LR: 0.00000510  \n","Epoch: [4][800/2223] Elapsed 4m 31s (remain 8m 2s) Loss: 0.0002(0.0060) Grad: 1122.6021  LR: 0.00000486  \n","Epoch: [4][900/2223] Elapsed 5m 5s (remain 7m 28s) Loss: 0.0055(0.0060) Grad: 11565.1260  LR: 0.00000462  \n","Epoch: [4][1000/2223] Elapsed 5m 39s (remain 6m 53s) Loss: 0.0081(0.0060) Grad: 30156.1797  LR: 0.00000438  \n","Epoch: [4][1100/2223] Elapsed 6m 12s (remain 6m 19s) Loss: 0.0000(0.0059) Grad: 157.5782  LR: 0.00000415  \n","Epoch: [4][1200/2223] Elapsed 6m 46s (remain 5m 46s) Loss: 0.0060(0.0060) Grad: 17332.9180  LR: 0.00000393  \n","Epoch: [4][1300/2223] Elapsed 7m 20s (remain 5m 12s) Loss: 0.0001(0.0060) Grad: 253.2006  LR: 0.00000370  \n","Epoch: [4][1400/2223] Elapsed 7m 54s (remain 4m 38s) Loss: 0.0042(0.0061) Grad: 12492.4062  LR: 0.00000349  \n","Epoch: [4][1500/2223] Elapsed 8m 28s (remain 4m 4s) Loss: 0.0065(0.0061) Grad: 20648.5605  LR: 0.00000328  \n","Epoch: [4][1600/2223] Elapsed 9m 2s (remain 3m 30s) Loss: 0.0151(0.0062) Grad: 33350.4727  LR: 0.00000307  \n","Epoch: [4][1700/2223] Elapsed 9m 36s (remain 2m 56s) Loss: 0.0001(0.0062) Grad: 212.0276  LR: 0.00000287  \n","Epoch: [4][1800/2223] Elapsed 10m 10s (remain 2m 23s) Loss: 0.0000(0.0063) Grad: 89.3324  LR: 0.00000267  \n","Epoch: [4][1900/2223] Elapsed 10m 44s (remain 1m 49s) Loss: 0.0023(0.0063) Grad: 11906.0000  LR: 0.00000248  \n","Epoch: [4][2000/2223] Elapsed 11m 17s (remain 1m 15s) Loss: 0.0122(0.0064) Grad: 36437.3164  LR: 0.00000230  \n","Epoch: [4][2100/2223] Elapsed 11m 51s (remain 0m 41s) Loss: 0.0022(0.0064) Grad: 19643.5488  LR: 0.00000212  \n","Epoch: [4][2200/2223] Elapsed 12m 25s (remain 0m 7s) Loss: 0.0011(0.0065) Grad: 15774.5254  LR: 0.00000195  \n","Epoch: [4][2222/2223] Elapsed 12m 32s (remain 0m 0s) Loss: 0.0068(0.0065) Grad: 28962.9707  LR: 0.00000192  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 9s) Loss: 0.0036(0.0036) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0136(0.0149) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0000(0.0131) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.0065  avg_val_loss: 0.0131  time: 787s\n","Epoch 4 - Score: 0.8890\n","Epoch 4 - Save Best Score: 0.8890 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [5][0/2223] Elapsed 0m 0s (remain 30m 7s) Loss: 0.0030(0.0030) Grad: 12042.2412  LR: 0.00000191  \n","Epoch: [5][100/2223] Elapsed 0m 34s (remain 12m 2s) Loss: 0.0012(0.0039) Grad: 6222.3701  LR: 0.00000175  \n","Epoch: [5][200/2223] Elapsed 1m 7s (remain 11m 22s) Loss: 0.0008(0.0046) Grad: 7006.4771  LR: 0.00000159  \n","Epoch: [5][300/2223] Elapsed 1m 41s (remain 10m 46s) Loss: 0.0035(0.0047) Grad: 194924.9844  LR: 0.00000144  \n","Epoch: [5][400/2223] Elapsed 2m 14s (remain 10m 11s) Loss: 0.0022(0.0050) Grad: 33848.3008  LR: 0.00000130  \n","Epoch: [5][500/2223] Elapsed 2m 47s (remain 9m 37s) Loss: 0.0103(0.0051) Grad: 19209.4473  LR: 0.00000117  \n","Epoch: [5][600/2223] Elapsed 3m 21s (remain 9m 3s) Loss: 0.0004(0.0051) Grad: 2636.3098  LR: 0.00000104  \n","Epoch: [5][700/2223] Elapsed 3m 54s (remain 8m 29s) Loss: 0.0019(0.0051) Grad: 10451.2178  LR: 0.00000092  \n","Epoch: [5][800/2223] Elapsed 4m 27s (remain 7m 55s) Loss: 0.0023(0.0052) Grad: 14312.6201  LR: 0.00000080  \n","Epoch: [5][900/2223] Elapsed 5m 1s (remain 7m 22s) Loss: 0.0003(0.0051) Grad: 3094.1270  LR: 0.00000069  \n","Epoch: [5][1000/2223] Elapsed 5m 34s (remain 6m 48s) Loss: 0.0025(0.0051) Grad: 16220.6221  LR: 0.00000059  \n","Epoch: [5][1100/2223] Elapsed 6m 8s (remain 6m 15s) Loss: 0.0005(0.0052) Grad: 7826.2212  LR: 0.00000050  \n","Epoch: [5][1200/2223] Elapsed 6m 41s (remain 5m 41s) Loss: 0.0000(0.0052) Grad: 77.4342  LR: 0.00000042  \n","Epoch: [5][1300/2223] Elapsed 7m 15s (remain 5m 8s) Loss: 0.0145(0.0051) Grad: 15145.7705  LR: 0.00000034  \n","Epoch: [5][1400/2223] Elapsed 7m 48s (remain 4m 34s) Loss: 0.0002(0.0051) Grad: 1248.1555  LR: 0.00000027  \n","Epoch: [5][1500/2223] Elapsed 8m 22s (remain 4m 1s) Loss: 0.0081(0.0052) Grad: 35459.1641  LR: 0.00000021  \n","Epoch: [5][1600/2223] Elapsed 8m 55s (remain 3m 28s) Loss: 0.0323(0.0053) Grad: 42988.9414  LR: 0.00000016  \n","Epoch: [5][1700/2223] Elapsed 9m 28s (remain 2m 54s) Loss: 0.0005(0.0053) Grad: 2575.7522  LR: 0.00000011  \n","Epoch: [5][1800/2223] Elapsed 10m 2s (remain 2m 21s) Loss: 0.0009(0.0054) Grad: 6693.6748  LR: 0.00000007  \n","Epoch: [5][1900/2223] Elapsed 10m 35s (remain 1m 47s) Loss: 0.0001(0.0054) Grad: 730.7125  LR: 0.00000004  \n","Epoch: [5][2000/2223] Elapsed 11m 9s (remain 1m 14s) Loss: 0.0008(0.0054) Grad: 39535.7891  LR: 0.00000002  \n","Epoch: [5][2100/2223] Elapsed 11m 42s (remain 0m 40s) Loss: 0.0022(0.0053) Grad: 58365.4336  LR: 0.00000001  \n","Epoch: [5][2200/2223] Elapsed 12m 16s (remain 0m 7s) Loss: 0.0010(0.0053) Grad: 9282.0889  LR: 0.00000000  \n","Epoch: [5][2222/2223] Elapsed 12m 23s (remain 0m 0s) Loss: 0.0067(0.0053) Grad: 75857.1250  LR: 0.00000000  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 19s) Loss: 0.0031(0.0031) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0179(0.0162) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0000(0.0143) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.0053  avg_val_loss: 0.0143  time: 777s\n","Epoch 5 - Score: 0.8895\n","Epoch 5 - Save Best Score: 0.8895 Model\n","========== fold: 4 result ==========\n","Score: 0.8895\n","========== fold: 5 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2225] Elapsed 0m 0s (remain 22m 53s) Loss: 0.4291(0.4291) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2225] Elapsed 0m 34s (remain 12m 0s) Loss: 0.0499(0.0678) Grad: 14485.2783  LR: 0.00002000  \n","Epoch: [1][200/2225] Elapsed 1m 7s (remain 11m 21s) Loss: 0.0453(0.0463) Grad: 12613.0674  LR: 0.00001998  \n","Epoch: [1][300/2225] Elapsed 1m 41s (remain 10m 45s) Loss: 0.0344(0.0387) Grad: 8799.7002  LR: 0.00001996  \n","Epoch: [1][400/2225] Elapsed 2m 14s (remain 10m 11s) Loss: 0.0128(0.0349) Grad: 1946.0585  LR: 0.00001994  \n","Epoch: [1][500/2225] Elapsed 2m 47s (remain 9m 37s) Loss: 0.0300(0.0317) Grad: 5422.0688  LR: 0.00001990  \n","Epoch: [1][600/2225] Elapsed 3m 21s (remain 9m 3s) Loss: 0.0080(0.0296) Grad: 1804.5939  LR: 0.00001986  \n","Epoch: [1][700/2225] Elapsed 3m 54s (remain 8m 30s) Loss: 0.0502(0.0280) Grad: 7980.9185  LR: 0.00001980  \n","Epoch: [1][800/2225] Elapsed 4m 27s (remain 7m 56s) Loss: 0.0310(0.0266) Grad: 3018.5952  LR: 0.00001975  \n","Epoch: [1][900/2225] Elapsed 5m 1s (remain 7m 22s) Loss: 0.0272(0.0253) Grad: 7954.8789  LR: 0.00001968  \n","Epoch: [1][1000/2225] Elapsed 5m 34s (remain 6m 49s) Loss: 0.0010(0.0244) Grad: 338.6385  LR: 0.00001960  \n","Epoch: [1][1100/2225] Elapsed 6m 8s (remain 6m 15s) Loss: 0.0870(0.0234) Grad: 29318.9590  LR: 0.00001952  \n","Epoch: [1][1200/2225] Elapsed 6m 41s (remain 5m 42s) Loss: 0.0042(0.0226) Grad: 1967.0406  LR: 0.00001943  \n","Epoch: [1][1300/2225] Elapsed 7m 15s (remain 5m 9s) Loss: 0.0137(0.0221) Grad: 1924.0002  LR: 0.00001933  \n","Epoch: [1][1400/2225] Elapsed 7m 48s (remain 4m 35s) Loss: 0.0107(0.0218) Grad: 5867.2402  LR: 0.00001923  \n","Epoch: [1][1500/2225] Elapsed 8m 22s (remain 4m 2s) Loss: 0.0121(0.0214) Grad: 5118.3423  LR: 0.00001912  \n","Epoch: [1][1600/2225] Elapsed 8m 55s (remain 3m 28s) Loss: 0.0503(0.0210) Grad: 24925.0918  LR: 0.00001900  \n","Epoch: [1][1700/2225] Elapsed 9m 28s (remain 2m 55s) Loss: 0.0492(0.0207) Grad: 9144.2812  LR: 0.00001887  \n","Epoch: [1][1800/2225] Elapsed 10m 1s (remain 2m 21s) Loss: 0.0146(0.0204) Grad: 1512.3621  LR: 0.00001873  \n","Epoch: [1][1900/2225] Elapsed 10m 34s (remain 1m 48s) Loss: 0.0089(0.0200) Grad: 821.3641  LR: 0.00001859  \n","Epoch: [1][2000/2225] Elapsed 11m 7s (remain 1m 14s) Loss: 0.0133(0.0198) Grad: 1426.3851  LR: 0.00001845  \n","Epoch: [1][2100/2225] Elapsed 11m 40s (remain 0m 41s) Loss: 0.0010(0.0196) Grad: 548.6464  LR: 0.00001829  \n","Epoch: [1][2200/2225] Elapsed 12m 14s (remain 0m 8s) Loss: 0.0035(0.0193) Grad: 2374.8806  LR: 0.00001813  \n","Epoch: [1][2224/2225] Elapsed 12m 22s (remain 0m 0s) Loss: 0.0021(0.0193) Grad: 1288.1400  LR: 0.00001809  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 15s) Loss: 0.0018(0.0018) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0169(0.0125) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0040(0.0112) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.0193  avg_val_loss: 0.0112  time: 775s\n","Epoch 1 - Score: 0.8747\n","Epoch 1 - Save Best Score: 0.8747 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [2][0/2225] Elapsed 0m 0s (remain 29m 53s) Loss: 0.0015(0.0015) Grad: 4189.9990  LR: 0.00001809  \n","Epoch: [2][100/2225] Elapsed 0m 34s (remain 12m 3s) Loss: 0.0077(0.0105) Grad: 14033.4932  LR: 0.00001792  \n","Epoch: [2][200/2225] Elapsed 1m 7s (remain 11m 23s) Loss: 0.0310(0.0104) Grad: 33037.4531  LR: 0.00001774  \n","Epoch: [2][300/2225] Elapsed 1m 41s (remain 10m 47s) Loss: 0.0034(0.0098) Grad: 12225.4131  LR: 0.00001756  \n","Epoch: [2][400/2225] Elapsed 2m 14s (remain 10m 12s) Loss: 0.0076(0.0095) Grad: 24512.6230  LR: 0.00001737  \n","Epoch: [2][500/2225] Elapsed 2m 48s (remain 9m 38s) Loss: 0.0067(0.0100) Grad: 18767.9062  LR: 0.00001718  \n","Epoch: [2][600/2225] Elapsed 3m 21s (remain 9m 4s) Loss: 0.0054(0.0097) Grad: 29263.4727  LR: 0.00001698  \n","Epoch: [2][700/2225] Elapsed 3m 54s (remain 8m 30s) Loss: 0.0145(0.0096) Grad: 18042.8672  LR: 0.00001678  \n","Epoch: [2][800/2225] Elapsed 4m 28s (remain 7m 56s) Loss: 0.0051(0.0095) Grad: 8925.5801  LR: 0.00001657  \n","Epoch: [2][900/2225] Elapsed 5m 1s (remain 7m 23s) Loss: 0.0132(0.0094) Grad: 76862.1094  LR: 0.00001635  \n","Epoch: [2][1000/2225] Elapsed 5m 34s (remain 6m 49s) Loss: 0.0057(0.0095) Grad: 13333.4570  LR: 0.00001613  \n","Epoch: [2][1100/2225] Elapsed 6m 8s (remain 6m 15s) Loss: 0.0038(0.0096) Grad: 20635.4668  LR: 0.00001590  \n","Epoch: [2][1200/2225] Elapsed 6m 41s (remain 5m 42s) Loss: 0.0028(0.0096) Grad: 7240.8745  LR: 0.00001567  \n","Epoch: [2][1300/2225] Elapsed 7m 14s (remain 5m 8s) Loss: 0.0046(0.0095) Grad: 12695.4531  LR: 0.00001544  \n","Epoch: [2][1400/2225] Elapsed 7m 48s (remain 4m 35s) Loss: 0.0030(0.0095) Grad: 4003.9153  LR: 0.00001520  \n","Epoch: [2][1500/2225] Elapsed 8m 21s (remain 4m 2s) Loss: 0.0094(0.0094) Grad: 6072.5020  LR: 0.00001496  \n","Epoch: [2][1600/2225] Elapsed 8m 55s (remain 3m 28s) Loss: 0.0160(0.0094) Grad: 12195.0615  LR: 0.00001471  \n","Epoch: [2][1700/2225] Elapsed 9m 28s (remain 2m 55s) Loss: 0.0042(0.0094) Grad: 4387.3218  LR: 0.00001446  \n","Epoch: [2][1800/2225] Elapsed 10m 1s (remain 2m 21s) Loss: 0.0013(0.0094) Grad: 1494.0739  LR: 0.00001420  \n","Epoch: [2][1900/2225] Elapsed 10m 35s (remain 1m 48s) Loss: 0.0018(0.0094) Grad: 6592.3428  LR: 0.00001395  \n","Epoch: [2][2000/2225] Elapsed 11m 8s (remain 1m 14s) Loss: 0.0036(0.0094) Grad: 5044.4463  LR: 0.00001369  \n","Epoch: [2][2100/2225] Elapsed 11m 41s (remain 0m 41s) Loss: 0.0007(0.0094) Grad: 7576.9043  LR: 0.00001342  \n","Epoch: [2][2200/2225] Elapsed 12m 14s (remain 0m 8s) Loss: 0.0117(0.0094) Grad: 10826.8428  LR: 0.00001316  \n","Epoch: [2][2224/2225] Elapsed 12m 22s (remain 0m 0s) Loss: 0.0032(0.0093) Grad: 5758.0923  LR: 0.00001309  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 8s) Loss: 0.0007(0.0007) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0267(0.0148) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0056(0.0127) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.0093  avg_val_loss: 0.0127  time: 776s\n","Epoch 2 - Score: 0.8823\n","Epoch 2 - Save Best Score: 0.8823 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [3][0/2225] Elapsed 0m 0s (remain 29m 14s) Loss: 0.0025(0.0025) Grad: 7723.8096  LR: 0.00001309  \n","Epoch: [3][100/2225] Elapsed 0m 34s (remain 11m 59s) Loss: 0.0564(0.0057) Grad: 51527.2773  LR: 0.00001282  \n","Epoch: [3][200/2225] Elapsed 1m 7s (remain 11m 19s) Loss: 0.0078(0.0063) Grad: 19680.3945  LR: 0.00001255  \n","Epoch: [3][300/2225] Elapsed 1m 40s (remain 10m 44s) Loss: 0.0002(0.0068) Grad: 533.8224  LR: 0.00001227  \n","Epoch: [3][400/2225] Elapsed 2m 13s (remain 10m 9s) Loss: 0.0000(0.0069) Grad: 108.3056  LR: 0.00001200  \n","Epoch: [3][500/2225] Elapsed 2m 47s (remain 9m 35s) Loss: 0.0028(0.0071) Grad: 5679.8931  LR: 0.00001172  \n","Epoch: [3][600/2225] Elapsed 3m 20s (remain 9m 1s) Loss: 0.0419(0.0073) Grad: 39179.8789  LR: 0.00001144  \n","Epoch: [3][700/2225] Elapsed 3m 53s (remain 8m 27s) Loss: 0.0038(0.0074) Grad: 9903.4961  LR: 0.00001116  \n","Epoch: [3][800/2225] Elapsed 4m 26s (remain 7m 54s) Loss: 0.0000(0.0075) Grad: 86.3878  LR: 0.00001088  \n","Epoch: [3][900/2225] Elapsed 5m 0s (remain 7m 20s) Loss: 0.0244(0.0074) Grad: 26106.8730  LR: 0.00001060  \n","Epoch: [3][1000/2225] Elapsed 5m 33s (remain 6m 47s) Loss: 0.0002(0.0072) Grad: 994.5655  LR: 0.00001032  \n","Epoch: [3][1100/2225] Elapsed 6m 6s (remain 6m 14s) Loss: 0.0106(0.0073) Grad: 59391.3906  LR: 0.00001003  \n","Epoch: [3][1200/2225] Elapsed 6m 39s (remain 5m 40s) Loss: 0.0002(0.0072) Grad: 1680.9117  LR: 0.00000975  \n","Epoch: [3][1300/2225] Elapsed 7m 12s (remain 5m 7s) Loss: 0.0025(0.0074) Grad: 14122.4619  LR: 0.00000947  \n","Epoch: [3][1400/2225] Elapsed 7m 46s (remain 4m 34s) Loss: 0.0001(0.0073) Grad: 650.7861  LR: 0.00000919  \n","Epoch: [3][1500/2225] Elapsed 8m 19s (remain 4m 0s) Loss: 0.0001(0.0074) Grad: 469.7487  LR: 0.00000891  \n","Epoch: [3][1600/2225] Elapsed 8m 52s (remain 3m 27s) Loss: 0.0001(0.0074) Grad: 202.6342  LR: 0.00000863  \n","Epoch: [3][1700/2225] Elapsed 9m 26s (remain 2m 54s) Loss: 0.0016(0.0074) Grad: 7481.1455  LR: 0.00000835  \n","Epoch: [3][1800/2225] Elapsed 9m 59s (remain 2m 21s) Loss: 0.0644(0.0075) Grad: 54304.8477  LR: 0.00000807  \n","Epoch: [3][1900/2225] Elapsed 10m 32s (remain 1m 47s) Loss: 0.0037(0.0075) Grad: 5489.9575  LR: 0.00000779  \n","Epoch: [3][2000/2225] Elapsed 11m 6s (remain 1m 14s) Loss: 0.0011(0.0076) Grad: 6875.2271  LR: 0.00000752  \n","Epoch: [3][2100/2225] Elapsed 11m 39s (remain 0m 41s) Loss: 0.0020(0.0075) Grad: 13682.0732  LR: 0.00000725  \n","Epoch: [3][2200/2225] Elapsed 12m 12s (remain 0m 7s) Loss: 0.0032(0.0076) Grad: 52113.9609  LR: 0.00000698  \n","Epoch: [3][2224/2225] Elapsed 12m 20s (remain 0m 0s) Loss: 0.0012(0.0076) Grad: 21830.8633  LR: 0.00000691  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 9s) Loss: 0.0012(0.0012) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0274(0.0135) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0029(0.0115) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.0076  avg_val_loss: 0.0115  time: 775s\n","Epoch 3 - Score: 0.8942\n","Epoch 3 - Save Best Score: 0.8942 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [4][0/2225] Elapsed 0m 0s (remain 32m 20s) Loss: 0.0030(0.0030) Grad: 5993.6602  LR: 0.00000691  \n","Epoch: [4][100/2225] Elapsed 0m 34s (remain 12m 6s) Loss: 0.0002(0.0062) Grad: 4972.1611  LR: 0.00000664  \n","Epoch: [4][200/2225] Elapsed 1m 7s (remain 11m 23s) Loss: 0.0077(0.0056) Grad: 22539.6406  LR: 0.00000638  \n","Epoch: [4][300/2225] Elapsed 1m 41s (remain 10m 46s) Loss: 0.0540(0.0057) Grad: 81956.9766  LR: 0.00000612  \n","Epoch: [4][400/2225] Elapsed 2m 14s (remain 10m 11s) Loss: 0.0000(0.0054) Grad: 31.1237  LR: 0.00000586  \n","Epoch: [4][500/2225] Elapsed 2m 47s (remain 9m 37s) Loss: 0.0069(0.0054) Grad: 36393.5312  LR: 0.00000560  \n","Epoch: [4][600/2225] Elapsed 3m 21s (remain 9m 3s) Loss: 0.0005(0.0054) Grad: 2725.8245  LR: 0.00000535  \n","Epoch: [4][700/2225] Elapsed 3m 54s (remain 8m 29s) Loss: 0.0000(0.0055) Grad: 85.5533  LR: 0.00000510  \n","Epoch: [4][800/2225] Elapsed 4m 27s (remain 7m 55s) Loss: 0.0018(0.0056) Grad: 7863.2056  LR: 0.00000486  \n","Epoch: [4][900/2225] Elapsed 5m 0s (remain 7m 22s) Loss: 0.0000(0.0056) Grad: 29.4377  LR: 0.00000462  \n","Epoch: [4][1000/2225] Elapsed 5m 33s (remain 6m 48s) Loss: 0.0000(0.0057) Grad: 227.4650  LR: 0.00000438  \n","Epoch: [4][1100/2225] Elapsed 6m 7s (remain 6m 14s) Loss: 0.0008(0.0058) Grad: 10502.4551  LR: 0.00000415  \n","Epoch: [4][1200/2225] Elapsed 6m 40s (remain 5m 41s) Loss: 0.0006(0.0058) Grad: 3479.9844  LR: 0.00000392  \n","Epoch: [4][1300/2225] Elapsed 7m 13s (remain 5m 8s) Loss: 0.0044(0.0059) Grad: 40426.7461  LR: 0.00000370  \n","Epoch: [4][1400/2225] Elapsed 7m 46s (remain 4m 34s) Loss: 0.0040(0.0059) Grad: 19670.2461  LR: 0.00000348  \n","Epoch: [4][1500/2225] Elapsed 8m 20s (remain 4m 1s) Loss: 0.0085(0.0060) Grad: 14550.6924  LR: 0.00000327  \n","Epoch: [4][1600/2225] Elapsed 8m 53s (remain 3m 28s) Loss: 0.0103(0.0060) Grad: 27068.4238  LR: 0.00000307  \n","Epoch: [4][1700/2225] Elapsed 9m 27s (remain 2m 54s) Loss: 0.0070(0.0060) Grad: 54390.5469  LR: 0.00000287  \n","Epoch: [4][1800/2225] Elapsed 10m 0s (remain 2m 21s) Loss: 0.0019(0.0058) Grad: 17875.9062  LR: 0.00000267  \n","Epoch: [4][1900/2225] Elapsed 10m 33s (remain 1m 47s) Loss: 0.0002(0.0059) Grad: 1339.3516  LR: 0.00000248  \n","Epoch: [4][2000/2225] Elapsed 11m 6s (remain 1m 14s) Loss: 0.0054(0.0059) Grad: 40248.6562  LR: 0.00000230  \n","Epoch: [4][2100/2225] Elapsed 11m 40s (remain 0m 41s) Loss: 0.0001(0.0060) Grad: 2847.7444  LR: 0.00000212  \n","Epoch: [4][2200/2225] Elapsed 12m 13s (remain 0m 7s) Loss: 0.0017(0.0061) Grad: 8053.6011  LR: 0.00000195  \n","Epoch: [4][2224/2225] Elapsed 12m 21s (remain 0m 0s) Loss: 0.0002(0.0061) Grad: 2820.0273  LR: 0.00000191  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 18s) Loss: 0.0005(0.0005) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0332(0.0174) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0031(0.0145) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.0061  avg_val_loss: 0.0145  time: 774s\n","Epoch 4 - Score: 0.8918\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [5][0/2225] Elapsed 0m 0s (remain 29m 20s) Loss: 0.0025(0.0025) Grad: 14951.0850  LR: 0.00000191  \n","Epoch: [5][100/2225] Elapsed 0m 34s (remain 11m 56s) Loss: 0.0002(0.0034) Grad: 6518.2666  LR: 0.00000175  \n","Epoch: [5][200/2225] Elapsed 1m 7s (remain 11m 18s) Loss: 0.0008(0.0040) Grad: 6580.3555  LR: 0.00000159  \n","Epoch: [5][300/2225] Elapsed 1m 40s (remain 10m 44s) Loss: 0.0060(0.0049) Grad: 21787.3613  LR: 0.00000144  \n","Epoch: [5][400/2225] Elapsed 2m 14s (remain 10m 9s) Loss: 0.0000(0.0049) Grad: 335.4110  LR: 0.00000130  \n","Epoch: [5][500/2225] Elapsed 2m 47s (remain 9m 35s) Loss: 0.0000(0.0048) Grad: 63.6538  LR: 0.00000116  \n","Epoch: [5][600/2225] Elapsed 3m 20s (remain 9m 2s) Loss: 0.0078(0.0051) Grad: 21784.3594  LR: 0.00000103  \n","Epoch: [5][700/2225] Elapsed 3m 54s (remain 8m 29s) Loss: 0.0002(0.0050) Grad: 696.1024  LR: 0.00000091  \n","Epoch: [5][800/2225] Elapsed 4m 27s (remain 7m 55s) Loss: 0.0057(0.0051) Grad: 24956.0059  LR: 0.00000080  \n","Epoch: [5][900/2225] Elapsed 5m 1s (remain 7m 22s) Loss: 0.0050(0.0051) Grad: 30178.4473  LR: 0.00000069  \n","Epoch: [5][1000/2225] Elapsed 5m 34s (remain 6m 48s) Loss: 0.0011(0.0051) Grad: 13704.0195  LR: 0.00000059  \n","Epoch: [5][1100/2225] Elapsed 6m 7s (remain 6m 15s) Loss: 0.0321(0.0050) Grad: 75307.5547  LR: 0.00000050  \n","Epoch: [5][1200/2225] Elapsed 6m 41s (remain 5m 42s) Loss: 0.0079(0.0050) Grad: 12069.6582  LR: 0.00000042  \n","Epoch: [5][1300/2225] Elapsed 7m 14s (remain 5m 8s) Loss: 0.0048(0.0051) Grad: 13786.8027  LR: 0.00000034  \n","Epoch: [5][1400/2225] Elapsed 7m 48s (remain 4m 35s) Loss: 0.0038(0.0051) Grad: 20386.9629  LR: 0.00000027  \n","Epoch: [5][1500/2225] Elapsed 8m 21s (remain 4m 1s) Loss: 0.0390(0.0050) Grad: 141290.4844  LR: 0.00000021  \n","Epoch: [5][1600/2225] Elapsed 8m 54s (remain 3m 28s) Loss: 0.0001(0.0051) Grad: 486.8863  LR: 0.00000016  \n","Epoch: [5][1700/2225] Elapsed 9m 28s (remain 2m 55s) Loss: 0.0001(0.0050) Grad: 237.1928  LR: 0.00000011  \n","Epoch: [5][1800/2225] Elapsed 10m 1s (remain 2m 21s) Loss: 0.0000(0.0050) Grad: 301.8307  LR: 0.00000007  \n","Epoch: [5][1900/2225] Elapsed 10m 35s (remain 1m 48s) Loss: 0.0001(0.0050) Grad: 296.2875  LR: 0.00000004  \n","Epoch: [5][2000/2225] Elapsed 11m 8s (remain 1m 14s) Loss: 0.0070(0.0050) Grad: 25699.3301  LR: 0.00000002  \n","Epoch: [5][2100/2225] Elapsed 11m 41s (remain 0m 41s) Loss: 0.0000(0.0049) Grad: 19.2632  LR: 0.00000001  \n","Epoch: [5][2200/2225] Elapsed 12m 14s (remain 0m 8s) Loss: 0.0000(0.0049) Grad: 228.8507  LR: 0.00000000  \n","Epoch: [5][2224/2225] Elapsed 12m 22s (remain 0m 0s) Loss: 0.0120(0.0049) Grad: 6274.4370  LR: 0.00000000  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 9s) Loss: 0.0005(0.0005) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0336(0.0174) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0039(0.0148) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.0049  avg_val_loss: 0.0148  time: 777s\n","Epoch 5 - Score: 0.8969\n","Epoch 5 - Save Best Score: 0.8969 Model\n","========== fold: 5 result ==========\n","Score: 0.8969\n","========== fold: 6 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2225] Elapsed 0m 0s (remain 22m 44s) Loss: 1.3912(1.3912) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2225] Elapsed 0m 34s (remain 11m 58s) Loss: 0.0123(0.1374) Grad: 1047.1008  LR: 0.00002000  \n","Epoch: [1][200/2225] Elapsed 1m 7s (remain 11m 21s) Loss: 0.0135(0.0847) Grad: 1619.7441  LR: 0.00001998  \n","Epoch: [1][300/2225] Elapsed 1m 41s (remain 10m 47s) Loss: 0.0019(0.0662) Grad: 368.0647  LR: 0.00001996  \n","Epoch: [1][400/2225] Elapsed 2m 14s (remain 10m 12s) Loss: 0.0061(0.0544) Grad: 883.1979  LR: 0.00001994  \n","Epoch: [1][500/2225] Elapsed 2m 48s (remain 9m 39s) Loss: 0.0239(0.0476) Grad: 741.1251  LR: 0.00001990  \n","Epoch: [1][600/2225] Elapsed 3m 21s (remain 9m 5s) Loss: 0.0036(0.0435) Grad: 832.3278  LR: 0.00001986  \n","Epoch: [1][700/2225] Elapsed 3m 55s (remain 8m 31s) Loss: 0.0088(0.0399) Grad: 975.3997  LR: 0.00001980  \n","Epoch: [1][800/2225] Elapsed 4m 28s (remain 7m 58s) Loss: 0.0189(0.0372) Grad: 1883.4176  LR: 0.00001975  \n","Epoch: [1][900/2225] Elapsed 5m 2s (remain 7m 24s) Loss: 0.0315(0.0348) Grad: 2468.6792  LR: 0.00001968  \n","Epoch: [1][1000/2225] Elapsed 5m 35s (remain 6m 50s) Loss: 0.0006(0.0330) Grad: 114.8145  LR: 0.00001960  \n","Epoch: [1][1100/2225] Elapsed 6m 9s (remain 6m 17s) Loss: 0.0066(0.0317) Grad: 561.0740  LR: 0.00001952  \n","Epoch: [1][1200/2225] Elapsed 6m 42s (remain 5m 43s) Loss: 0.0054(0.0303) Grad: 509.2224  LR: 0.00001943  \n","Epoch: [1][1300/2225] Elapsed 7m 16s (remain 5m 9s) Loss: 0.0069(0.0292) Grad: 661.9564  LR: 0.00001933  \n","Epoch: [1][1400/2225] Elapsed 7m 49s (remain 4m 36s) Loss: 0.0365(0.0283) Grad: 3223.4707  LR: 0.00001923  \n","Epoch: [1][1500/2225] Elapsed 8m 23s (remain 4m 2s) Loss: 0.0246(0.0274) Grad: 3888.6665  LR: 0.00001912  \n","Epoch: [1][1600/2225] Elapsed 8m 56s (remain 3m 29s) Loss: 0.0292(0.0265) Grad: 2366.8738  LR: 0.00001900  \n","Epoch: [1][1700/2225] Elapsed 9m 30s (remain 2m 55s) Loss: 0.0349(0.0258) Grad: 3248.9412  LR: 0.00001887  \n","Epoch: [1][1800/2225] Elapsed 10m 3s (remain 2m 22s) Loss: 0.0059(0.0251) Grad: 885.3896  LR: 0.00001873  \n","Epoch: [1][1900/2225] Elapsed 10m 37s (remain 1m 48s) Loss: 0.0100(0.0245) Grad: 882.7976  LR: 0.00001859  \n","Epoch: [1][2000/2225] Elapsed 11m 10s (remain 1m 15s) Loss: 0.0064(0.0240) Grad: 894.5484  LR: 0.00001845  \n","Epoch: [1][2100/2225] Elapsed 11m 44s (remain 0m 41s) Loss: 0.0402(0.0234) Grad: 24559.1680  LR: 0.00001829  \n","Epoch: [1][2200/2225] Elapsed 12m 17s (remain 0m 8s) Loss: 0.0037(0.0230) Grad: 2427.1094  LR: 0.00001813  \n","Epoch: [1][2224/2225] Elapsed 12m 25s (remain 0m 0s) Loss: 0.0031(0.0228) Grad: 786.0623  LR: 0.00001809  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 17s) Loss: 0.0059(0.0059) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0066(0.0155) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0167(0.0137) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.0228  avg_val_loss: 0.0137  time: 780s\n","Epoch 1 - Score: 0.8738\n","Epoch 1 - Save Best Score: 0.8738 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [2][0/2225] Elapsed 0m 0s (remain 34m 45s) Loss: 0.0095(0.0095) Grad: 18870.0605  LR: 0.00001809  \n","Epoch: [2][100/2225] Elapsed 0m 34s (remain 12m 12s) Loss: 0.0405(0.0109) Grad: 80050.7500  LR: 0.00001792  \n","Epoch: [2][200/2225] Elapsed 1m 8s (remain 11m 27s) Loss: 0.0059(0.0104) Grad: 52163.1055  LR: 0.00001774  \n","Epoch: [2][300/2225] Elapsed 1m 41s (remain 10m 51s) Loss: 0.0206(0.0100) Grad: 29399.0723  LR: 0.00001756  \n","Epoch: [2][400/2225] Elapsed 2m 15s (remain 10m 16s) Loss: 0.0037(0.0097) Grad: 3075.4626  LR: 0.00001737  \n","Epoch: [2][500/2225] Elapsed 2m 48s (remain 9m 41s) Loss: 0.0595(0.0094) Grad: 106703.3594  LR: 0.00001718  \n","Epoch: [2][600/2225] Elapsed 3m 22s (remain 9m 7s) Loss: 0.0090(0.0095) Grad: 26256.2227  LR: 0.00001698  \n","Epoch: [2][700/2225] Elapsed 3m 55s (remain 8m 33s) Loss: 0.0039(0.0093) Grad: 12325.3848  LR: 0.00001678  \n","Epoch: [2][800/2225] Elapsed 4m 29s (remain 7m 59s) Loss: 0.0102(0.0096) Grad: 21857.8555  LR: 0.00001657  \n","Epoch: [2][900/2225] Elapsed 5m 2s (remain 7m 25s) Loss: 0.0220(0.0094) Grad: 19401.0781  LR: 0.00001635  \n","Epoch: [2][1000/2225] Elapsed 5m 36s (remain 6m 51s) Loss: 0.0002(0.0092) Grad: 961.2686  LR: 0.00001613  \n","Epoch: [2][1100/2225] Elapsed 6m 9s (remain 6m 17s) Loss: 0.0061(0.0093) Grad: 20317.8242  LR: 0.00001590  \n","Epoch: [2][1200/2225] Elapsed 6m 43s (remain 5m 43s) Loss: 0.0077(0.0093) Grad: 11050.7490  LR: 0.00001567  \n","Epoch: [2][1300/2225] Elapsed 7m 17s (remain 5m 10s) Loss: 0.0058(0.0095) Grad: 33845.3984  LR: 0.00001544  \n","Epoch: [2][1400/2225] Elapsed 7m 50s (remain 4m 36s) Loss: 0.0010(0.0095) Grad: 5730.3281  LR: 0.00001520  \n","Epoch: [2][1500/2225] Elapsed 8m 24s (remain 4m 3s) Loss: 0.0057(0.0095) Grad: 31071.5352  LR: 0.00001496  \n","Epoch: [2][1600/2225] Elapsed 8m 58s (remain 3m 29s) Loss: 0.0415(0.0095) Grad: 83020.5391  LR: 0.00001471  \n","Epoch: [2][1700/2225] Elapsed 9m 31s (remain 2m 56s) Loss: 0.0259(0.0095) Grad: 30753.5938  LR: 0.00001446  \n","Epoch: [2][1800/2225] Elapsed 10m 4s (remain 2m 22s) Loss: 0.0025(0.0095) Grad: 4207.2983  LR: 0.00001420  \n","Epoch: [2][1900/2225] Elapsed 10m 38s (remain 1m 48s) Loss: 0.0390(0.0094) Grad: 32706.7754  LR: 0.00001395  \n","Epoch: [2][2000/2225] Elapsed 11m 12s (remain 1m 15s) Loss: 0.0097(0.0093) Grad: 25992.3633  LR: 0.00001369  \n","Epoch: [2][2100/2225] Elapsed 11m 45s (remain 0m 41s) Loss: 0.0049(0.0093) Grad: 22577.7695  LR: 0.00001342  \n","Epoch: [2][2200/2225] Elapsed 12m 19s (remain 0m 8s) Loss: 0.0142(0.0093) Grad: 54941.3320  LR: 0.00001316  \n","Epoch: [2][2224/2225] Elapsed 12m 27s (remain 0m 0s) Loss: 0.0003(0.0093) Grad: 2797.0657  LR: 0.00001309  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 17s) Loss: 0.0069(0.0069) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0029(0.0151) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0115(0.0136) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.0093  avg_val_loss: 0.0136  time: 781s\n","Epoch 2 - Score: 0.8803\n","Epoch 2 - Save Best Score: 0.8803 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [3][0/2225] Elapsed 0m 0s (remain 30m 20s) Loss: 0.0009(0.0009) Grad: 1678.0970  LR: 0.00001309  \n","Epoch: [3][100/2225] Elapsed 0m 34s (remain 12m 3s) Loss: 0.0002(0.0063) Grad: 998.0414  LR: 0.00001282  \n","Epoch: [3][200/2225] Elapsed 1m 7s (remain 11m 19s) Loss: 0.0002(0.0072) Grad: 434.3836  LR: 0.00001255  \n","Epoch: [3][300/2225] Elapsed 1m 40s (remain 10m 45s) Loss: 0.0019(0.0080) Grad: 7652.9790  LR: 0.00001227  \n","Epoch: [3][400/2225] Elapsed 2m 14s (remain 10m 10s) Loss: 0.0072(0.0079) Grad: 11052.0605  LR: 0.00001200  \n","Epoch: [3][500/2225] Elapsed 2m 47s (remain 9m 36s) Loss: 0.0006(0.0081) Grad: 8643.5000  LR: 0.00001172  \n","Epoch: [3][600/2225] Elapsed 3m 20s (remain 9m 2s) Loss: 0.0082(0.0080) Grad: 21303.1543  LR: 0.00001144  \n","Epoch: [3][700/2225] Elapsed 3m 54s (remain 8m 29s) Loss: 0.0165(0.0081) Grad: 9342.3848  LR: 0.00001116  \n","Epoch: [3][800/2225] Elapsed 4m 27s (remain 7m 55s) Loss: 0.0010(0.0081) Grad: 3425.6348  LR: 0.00001088  \n","Epoch: [3][900/2225] Elapsed 5m 0s (remain 7m 22s) Loss: 0.0008(0.0079) Grad: 3776.3086  LR: 0.00001060  \n","Epoch: [3][1000/2225] Elapsed 5m 34s (remain 6m 48s) Loss: 0.0185(0.0081) Grad: 26319.8203  LR: 0.00001032  \n","Epoch: [3][1100/2225] Elapsed 6m 7s (remain 6m 15s) Loss: 0.0015(0.0080) Grad: 6024.6382  LR: 0.00001003  \n","Epoch: [3][1200/2225] Elapsed 6m 40s (remain 5m 41s) Loss: 0.0010(0.0081) Grad: 3986.7441  LR: 0.00000975  \n","Epoch: [3][1300/2225] Elapsed 7m 14s (remain 5m 8s) Loss: 0.0031(0.0082) Grad: 3906.8027  LR: 0.00000947  \n","Epoch: [3][1400/2225] Elapsed 7m 47s (remain 4m 35s) Loss: 0.0064(0.0081) Grad: 14347.2012  LR: 0.00000919  \n","Epoch: [3][1500/2225] Elapsed 8m 20s (remain 4m 1s) Loss: 0.0026(0.0081) Grad: 10938.2393  LR: 0.00000891  \n","Epoch: [3][1600/2225] Elapsed 8m 54s (remain 3m 28s) Loss: 0.0032(0.0081) Grad: 8047.7637  LR: 0.00000863  \n","Epoch: [3][1700/2225] Elapsed 9m 27s (remain 2m 54s) Loss: 0.0181(0.0080) Grad: 35736.5508  LR: 0.00000835  \n","Epoch: [3][1800/2225] Elapsed 10m 0s (remain 2m 21s) Loss: 0.0001(0.0081) Grad: 253.3307  LR: 0.00000807  \n","Epoch: [3][1900/2225] Elapsed 10m 34s (remain 1m 48s) Loss: 0.0021(0.0080) Grad: 13210.8350  LR: 0.00000779  \n","Epoch: [3][2000/2225] Elapsed 11m 7s (remain 1m 14s) Loss: 0.0005(0.0080) Grad: 4309.3867  LR: 0.00000752  \n","Epoch: [3][2100/2225] Elapsed 11m 40s (remain 0m 41s) Loss: 0.0037(0.0080) Grad: 46967.5352  LR: 0.00000725  \n","Epoch: [3][2200/2225] Elapsed 12m 13s (remain 0m 8s) Loss: 0.0057(0.0080) Grad: 23526.7344  LR: 0.00000698  \n","Epoch: [3][2224/2225] Elapsed 12m 21s (remain 0m 0s) Loss: 0.0061(0.0080) Grad: 35644.0352  LR: 0.00000691  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 9s) Loss: 0.0041(0.0041) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0040(0.0160) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0042(0.0143) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.0080  avg_val_loss: 0.0143  time: 776s\n","Epoch 3 - Score: 0.8827\n","Epoch 3 - Save Best Score: 0.8827 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [4][0/2225] Elapsed 0m 0s (remain 34m 30s) Loss: 0.0066(0.0066) Grad: 27552.8262  LR: 0.00000691  \n","Epoch: [4][100/2225] Elapsed 0m 34s (remain 12m 6s) Loss: 0.0004(0.0062) Grad: 13613.9297  LR: 0.00000664  \n","Epoch: [4][200/2225] Elapsed 1m 8s (remain 11m 26s) Loss: 0.0074(0.0069) Grad: 31923.9199  LR: 0.00000638  \n","Epoch: [4][300/2225] Elapsed 1m 41s (remain 10m 49s) Loss: 0.0167(0.0068) Grad: 49237.7930  LR: 0.00000612  \n","Epoch: [4][400/2225] Elapsed 2m 14s (remain 10m 14s) Loss: 0.0051(0.0066) Grad: 32428.8906  LR: 0.00000586  \n","Epoch: [4][500/2225] Elapsed 2m 48s (remain 9m 39s) Loss: 0.0083(0.0067) Grad: 12917.6514  LR: 0.00000560  \n","Epoch: [4][600/2225] Elapsed 3m 21s (remain 9m 5s) Loss: 0.0113(0.0069) Grad: 26059.1895  LR: 0.00000535  \n","Epoch: [4][700/2225] Elapsed 3m 55s (remain 8m 31s) Loss: 0.0028(0.0068) Grad: 5942.4478  LR: 0.00000510  \n","Epoch: [4][800/2225] Elapsed 4m 28s (remain 7m 57s) Loss: 0.0000(0.0066) Grad: 38.6882  LR: 0.00000486  \n","Epoch: [4][900/2225] Elapsed 5m 2s (remain 7m 24s) Loss: 0.0008(0.0068) Grad: 10996.0830  LR: 0.00000462  \n","Epoch: [4][1000/2225] Elapsed 5m 35s (remain 6m 50s) Loss: 0.0305(0.0068) Grad: 26313.7070  LR: 0.00000438  \n","Epoch: [4][1100/2225] Elapsed 6m 9s (remain 6m 16s) Loss: 0.0001(0.0069) Grad: 384.4181  LR: 0.00000415  \n","Epoch: [4][1200/2225] Elapsed 6m 42s (remain 5m 43s) Loss: 0.0024(0.0068) Grad: 8508.6230  LR: 0.00000392  \n","Epoch: [4][1300/2225] Elapsed 7m 16s (remain 5m 9s) Loss: 0.0007(0.0069) Grad: 8427.4248  LR: 0.00000370  \n","Epoch: [4][1400/2225] Elapsed 7m 49s (remain 4m 36s) Loss: 0.0004(0.0069) Grad: 1531.8285  LR: 0.00000348  \n","Epoch: [4][1500/2225] Elapsed 8m 23s (remain 4m 2s) Loss: 0.0013(0.0067) Grad: 6232.3354  LR: 0.00000327  \n","Epoch: [4][1600/2225] Elapsed 8m 56s (remain 3m 29s) Loss: 0.0000(0.0068) Grad: 158.9984  LR: 0.00000307  \n","Epoch: [4][1700/2225] Elapsed 9m 30s (remain 2m 55s) Loss: 0.0068(0.0068) Grad: 8847.8086  LR: 0.00000287  \n","Epoch: [4][1800/2225] Elapsed 10m 3s (remain 2m 22s) Loss: 0.0038(0.0069) Grad: 34381.7422  LR: 0.00000267  \n","Epoch: [4][1900/2225] Elapsed 10m 37s (remain 1m 48s) Loss: 0.0040(0.0068) Grad: 15905.0693  LR: 0.00000248  \n","Epoch: [4][2000/2225] Elapsed 11m 10s (remain 1m 15s) Loss: 0.0012(0.0068) Grad: 32627.0430  LR: 0.00000230  \n","Epoch: [4][2100/2225] Elapsed 11m 44s (remain 0m 41s) Loss: 0.0033(0.0068) Grad: 27304.4238  LR: 0.00000212  \n","Epoch: [4][2200/2225] Elapsed 12m 17s (remain 0m 8s) Loss: 0.0002(0.0068) Grad: 2625.4380  LR: 0.00000195  \n","Epoch: [4][2224/2225] Elapsed 12m 25s (remain 0m 0s) Loss: 0.0003(0.0068) Grad: 5365.0210  LR: 0.00000191  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 20s) Loss: 0.0057(0.0057) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0020(0.0177) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0077(0.0160) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.0068  avg_val_loss: 0.0160  time: 779s\n","Epoch 4 - Score: 0.8800\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [5][0/2225] Elapsed 0m 0s (remain 31m 10s) Loss: 0.0006(0.0006) Grad: 4663.8135  LR: 0.00000191  \n","Epoch: [5][100/2225] Elapsed 0m 34s (remain 11m 57s) Loss: 0.0119(0.0072) Grad: 21639.9023  LR: 0.00000175  \n","Epoch: [5][200/2225] Elapsed 1m 7s (remain 11m 20s) Loss: 0.0577(0.0069) Grad: 97705.2422  LR: 0.00000159  \n","Epoch: [5][300/2225] Elapsed 1m 40s (remain 10m 45s) Loss: 0.0001(0.0064) Grad: 535.0598  LR: 0.00000144  \n","Epoch: [5][400/2225] Elapsed 2m 14s (remain 10m 11s) Loss: 0.0012(0.0063) Grad: 7821.5239  LR: 0.00000130  \n","Epoch: [5][500/2225] Elapsed 2m 47s (remain 9m 37s) Loss: 0.0071(0.0062) Grad: 81914.1172  LR: 0.00000116  \n","Epoch: [5][600/2225] Elapsed 3m 21s (remain 9m 3s) Loss: 0.0004(0.0059) Grad: 1550.3351  LR: 0.00000103  \n","Epoch: [5][700/2225] Elapsed 3m 54s (remain 8m 30s) Loss: 0.0006(0.0060) Grad: 5448.4150  LR: 0.00000091  \n","Epoch: [5][800/2225] Elapsed 4m 28s (remain 7m 56s) Loss: 0.0085(0.0059) Grad: 23561.3711  LR: 0.00000080  \n","Epoch: [5][900/2225] Elapsed 5m 1s (remain 7m 23s) Loss: 0.0002(0.0060) Grad: 1989.5078  LR: 0.00000069  \n","Epoch: [5][1000/2225] Elapsed 5m 35s (remain 6m 49s) Loss: 0.0004(0.0059) Grad: 3516.2817  LR: 0.00000059  \n","Epoch: [5][1100/2225] Elapsed 6m 8s (remain 6m 16s) Loss: 0.0003(0.0059) Grad: 4521.6685  LR: 0.00000050  \n","Epoch: [5][1200/2225] Elapsed 6m 41s (remain 5m 42s) Loss: 0.0075(0.0061) Grad: 67744.2578  LR: 0.00000042  \n","Epoch: [5][1300/2225] Elapsed 7m 15s (remain 5m 9s) Loss: 0.0030(0.0060) Grad: 13048.0947  LR: 0.00000034  \n","Epoch: [5][1400/2225] Elapsed 7m 49s (remain 4m 35s) Loss: 0.0001(0.0059) Grad: 1127.7075  LR: 0.00000027  \n","Epoch: [5][1500/2225] Elapsed 8m 22s (remain 4m 2s) Loss: 0.0177(0.0058) Grad: 573045.3750  LR: 0.00000021  \n","Epoch: [5][1600/2225] Elapsed 8m 55s (remain 3m 28s) Loss: 0.0066(0.0058) Grad: 20295.9160  LR: 0.00000016  \n","Epoch: [5][1700/2225] Elapsed 9m 29s (remain 2m 55s) Loss: 0.0170(0.0058) Grad: 40760.0859  LR: 0.00000011  \n","Epoch: [5][1800/2225] Elapsed 10m 2s (remain 2m 21s) Loss: 0.0013(0.0057) Grad: 19648.5918  LR: 0.00000007  \n","Epoch: [5][1900/2225] Elapsed 10m 36s (remain 1m 48s) Loss: 0.0304(0.0057) Grad: 61185.6953  LR: 0.00000004  \n","Epoch: [5][2000/2225] Elapsed 11m 9s (remain 1m 14s) Loss: 0.0000(0.0057) Grad: 184.4988  LR: 0.00000002  \n","Epoch: [5][2100/2225] Elapsed 11m 42s (remain 0m 41s) Loss: 0.0014(0.0057) Grad: 13495.7900  LR: 0.00000001  \n","Epoch: [5][2200/2225] Elapsed 12m 16s (remain 0m 8s) Loss: 0.0014(0.0057) Grad: 14939.3750  LR: 0.00000000  \n","Epoch: [5][2224/2225] Elapsed 12m 24s (remain 0m 0s) Loss: 0.0000(0.0056) Grad: 206.9686  LR: 0.00000000  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 11s) Loss: 0.0051(0.0051) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0018(0.0181) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0065(0.0166) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.0056  avg_val_loss: 0.0166  time: 778s\n","Epoch 5 - Score: 0.8821\n","========== fold: 6 result ==========\n","Score: 0.8827\n","========== fold: 7 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2225] Elapsed 0m 0s (remain 22m 19s) Loss: 0.3558(0.3558) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2225] Elapsed 0m 34s (remain 11m 55s) Loss: 0.0300(0.0666) Grad: 5372.5386  LR: 0.00002000  \n","Epoch: [1][200/2225] Elapsed 1m 7s (remain 11m 18s) Loss: 0.0116(0.0450) Grad: 6806.6323  LR: 0.00001998  \n","Epoch: [1][300/2225] Elapsed 1m 40s (remain 10m 44s) Loss: 0.0425(0.0386) Grad: 8446.7549  LR: 0.00001996  \n","Epoch: [1][400/2225] Elapsed 2m 14s (remain 10m 10s) Loss: 0.0010(0.0342) Grad: 296.7914  LR: 0.00001994  \n","Epoch: [1][500/2225] Elapsed 2m 47s (remain 9m 36s) Loss: 0.0142(0.0314) Grad: 3451.6553  LR: 0.00001990  \n","Epoch: [1][600/2225] Elapsed 3m 20s (remain 9m 2s) Loss: 0.0031(0.0293) Grad: 1434.9569  LR: 0.00001986  \n","Epoch: [1][700/2225] Elapsed 3m 53s (remain 8m 28s) Loss: 0.0556(0.0276) Grad: 9427.9385  LR: 0.00001980  \n","Epoch: [1][800/2225] Elapsed 4m 27s (remain 7m 55s) Loss: 0.0109(0.0259) Grad: 6016.8911  LR: 0.00001975  \n","Epoch: [1][900/2225] Elapsed 5m 0s (remain 7m 21s) Loss: 0.0294(0.0254) Grad: 4812.1421  LR: 0.00001968  \n","Epoch: [1][1000/2225] Elapsed 5m 34s (remain 6m 48s) Loss: 0.0008(0.0243) Grad: 258.7160  LR: 0.00001960  \n","Epoch: [1][1100/2225] Elapsed 6m 7s (remain 6m 15s) Loss: 0.0045(0.0233) Grad: 8233.2832  LR: 0.00001952  \n","Epoch: [1][1200/2225] Elapsed 6m 40s (remain 5m 41s) Loss: 0.0054(0.0225) Grad: 1083.6191  LR: 0.00001943  \n","Epoch: [1][1300/2225] Elapsed 7m 14s (remain 5m 8s) Loss: 0.0197(0.0220) Grad: 20368.0449  LR: 0.00001933  \n","Epoch: [1][1400/2225] Elapsed 7m 47s (remain 4m 34s) Loss: 0.0021(0.0213) Grad: 533.2297  LR: 0.00001923  \n","Epoch: [1][1500/2225] Elapsed 8m 20s (remain 4m 1s) Loss: 0.0082(0.0209) Grad: 2149.1206  LR: 0.00001912  \n","Epoch: [1][1600/2225] Elapsed 8m 53s (remain 3m 28s) Loss: 0.0019(0.0206) Grad: 2383.4968  LR: 0.00001900  \n","Epoch: [1][1700/2225] Elapsed 9m 26s (remain 2m 54s) Loss: 0.0154(0.0203) Grad: 1234.7271  LR: 0.00001887  \n","Epoch: [1][1800/2225] Elapsed 10m 0s (remain 2m 21s) Loss: 0.0009(0.0199) Grad: 378.9963  LR: 0.00001873  \n","Epoch: [1][1900/2225] Elapsed 10m 33s (remain 1m 47s) Loss: 0.0124(0.0196) Grad: 2435.9111  LR: 0.00001859  \n","Epoch: [1][2000/2225] Elapsed 11m 6s (remain 1m 14s) Loss: 0.0168(0.0193) Grad: 4205.5308  LR: 0.00001845  \n","Epoch: [1][2100/2225] Elapsed 11m 39s (remain 0m 41s) Loss: 0.0126(0.0190) Grad: 4248.0967  LR: 0.00001829  \n","Epoch: [1][2200/2225] Elapsed 12m 13s (remain 0m 7s) Loss: 0.0028(0.0189) Grad: 662.9750  LR: 0.00001813  \n","Epoch: [1][2224/2225] Elapsed 12m 21s (remain 0m 0s) Loss: 0.0690(0.0188) Grad: 14234.9990  LR: 0.00001809  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 13s) Loss: 0.0022(0.0022) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0155(0.0147) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0005(0.0129) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.0188  avg_val_loss: 0.0129  time: 775s\n","Epoch 1 - Score: 0.8573\n","Epoch 1 - Save Best Score: 0.8573 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [2][0/2225] Elapsed 0m 0s (remain 29m 57s) Loss: 0.0062(0.0062) Grad: 15011.7373  LR: 0.00001809  \n","Epoch: [2][100/2225] Elapsed 0m 34s (remain 12m 2s) Loss: 0.0084(0.0091) Grad: 8866.0371  LR: 0.00001792  \n","Epoch: [2][200/2225] Elapsed 1m 7s (remain 11m 21s) Loss: 0.0007(0.0093) Grad: 2563.2576  LR: 0.00001774  \n","Epoch: [2][300/2225] Elapsed 1m 40s (remain 10m 45s) Loss: 0.0083(0.0093) Grad: 14962.5322  LR: 0.00001756  \n","Epoch: [2][400/2225] Elapsed 2m 14s (remain 10m 10s) Loss: 0.0093(0.0094) Grad: 13990.6533  LR: 0.00001737  \n","Epoch: [2][500/2225] Elapsed 2m 47s (remain 9m 36s) Loss: 0.0025(0.0093) Grad: 9383.1572  LR: 0.00001718  \n","Epoch: [2][600/2225] Elapsed 3m 21s (remain 9m 3s) Loss: 0.0081(0.0093) Grad: 20795.7676  LR: 0.00001698  \n","Epoch: [2][700/2225] Elapsed 3m 54s (remain 8m 29s) Loss: 0.0021(0.0093) Grad: 17545.3301  LR: 0.00001678  \n","Epoch: [2][800/2225] Elapsed 4m 27s (remain 7m 55s) Loss: 0.0328(0.0093) Grad: 14597.7383  LR: 0.00001657  \n","Epoch: [2][900/2225] Elapsed 5m 0s (remain 7m 22s) Loss: 0.0005(0.0094) Grad: 1395.1476  LR: 0.00001635  \n","Epoch: [2][1000/2225] Elapsed 5m 34s (remain 6m 48s) Loss: 0.0070(0.0093) Grad: 15829.8984  LR: 0.00001613  \n","Epoch: [2][1100/2225] Elapsed 6m 7s (remain 6m 15s) Loss: 0.0111(0.0092) Grad: 38429.4961  LR: 0.00001590  \n","Epoch: [2][1200/2225] Elapsed 6m 40s (remain 5m 41s) Loss: 0.0249(0.0093) Grad: 20433.4141  LR: 0.00001567  \n","Epoch: [2][1300/2225] Elapsed 7m 13s (remain 5m 8s) Loss: 0.0089(0.0092) Grad: 16068.6475  LR: 0.00001544  \n","Epoch: [2][1400/2225] Elapsed 7m 47s (remain 4m 34s) Loss: 0.0048(0.0091) Grad: 21807.2148  LR: 0.00001520  \n","Epoch: [2][1500/2225] Elapsed 8m 21s (remain 4m 1s) Loss: 0.0006(0.0091) Grad: 4594.2612  LR: 0.00001496  \n","Epoch: [2][1600/2225] Elapsed 8m 54s (remain 3m 28s) Loss: 0.0002(0.0091) Grad: 616.8396  LR: 0.00001471  \n","Epoch: [2][1700/2225] Elapsed 9m 27s (remain 2m 54s) Loss: 0.0329(0.0091) Grad: 35808.7734  LR: 0.00001446  \n","Epoch: [2][1800/2225] Elapsed 10m 0s (remain 2m 21s) Loss: 0.0070(0.0092) Grad: 9936.0449  LR: 0.00001420  \n","Epoch: [2][1900/2225] Elapsed 10m 34s (remain 1m 48s) Loss: 0.0030(0.0092) Grad: 7739.4902  LR: 0.00001395  \n","Epoch: [2][2000/2225] Elapsed 11m 7s (remain 1m 14s) Loss: 0.0006(0.0091) Grad: 10624.9717  LR: 0.00001369  \n","Epoch: [2][2100/2225] Elapsed 11m 40s (remain 0m 41s) Loss: 0.0275(0.0092) Grad: 143218.9531  LR: 0.00001342  \n","Epoch: [2][2200/2225] Elapsed 12m 14s (remain 0m 8s) Loss: 0.0091(0.0092) Grad: 42968.2188  LR: 0.00001316  \n","Epoch: [2][2224/2225] Elapsed 12m 22s (remain 0m 0s) Loss: 0.0007(0.0092) Grad: 16381.8662  LR: 0.00001309  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 19s) Loss: 0.0004(0.0004) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0119(0.0171) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0005(0.0147) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.0092  avg_val_loss: 0.0147  time: 775s\n","Epoch 2 - Score: 0.8659\n","Epoch 2 - Save Best Score: 0.8659 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [3][0/2225] Elapsed 0m 0s (remain 29m 39s) Loss: 0.0000(0.0000) Grad: 89.2016  LR: 0.00001309  \n","Epoch: [3][100/2225] Elapsed 0m 34s (remain 12m 4s) Loss: 0.0008(0.0062) Grad: 2815.6538  LR: 0.00001282  \n","Epoch: [3][200/2225] Elapsed 1m 7s (remain 11m 20s) Loss: 0.0043(0.0072) Grad: 10163.5918  LR: 0.00001255  \n","Epoch: [3][300/2225] Elapsed 1m 40s (remain 10m 44s) Loss: 0.0003(0.0066) Grad: 1107.2355  LR: 0.00001227  \n","Epoch: [3][400/2225] Elapsed 2m 14s (remain 10m 9s) Loss: 0.0330(0.0068) Grad: 68128.7422  LR: 0.00001200  \n","Epoch: [3][500/2225] Elapsed 2m 47s (remain 9m 35s) Loss: 0.0091(0.0067) Grad: 9821.3887  LR: 0.00001172  \n","Epoch: [3][600/2225] Elapsed 3m 20s (remain 9m 1s) Loss: 0.0023(0.0073) Grad: 5012.0645  LR: 0.00001144  \n","Epoch: [3][700/2225] Elapsed 3m 53s (remain 8m 28s) Loss: 0.0053(0.0072) Grad: 12521.3652  LR: 0.00001116  \n","Epoch: [3][800/2225] Elapsed 4m 27s (remain 7m 54s) Loss: 0.0121(0.0075) Grad: 48023.9609  LR: 0.00001088  \n","Epoch: [3][900/2225] Elapsed 5m 0s (remain 7m 21s) Loss: 0.0216(0.0075) Grad: 47755.4141  LR: 0.00001060  \n","Epoch: [3][1000/2225] Elapsed 5m 33s (remain 6m 47s) Loss: 0.0029(0.0076) Grad: 10602.1826  LR: 0.00001032  \n","Epoch: [3][1100/2225] Elapsed 6m 6s (remain 6m 14s) Loss: 0.0130(0.0076) Grad: 22671.7188  LR: 0.00001003  \n","Epoch: [3][1200/2225] Elapsed 6m 39s (remain 5m 40s) Loss: 0.0065(0.0077) Grad: 6840.1875  LR: 0.00000975  \n","Epoch: [3][1300/2225] Elapsed 7m 13s (remain 5m 7s) Loss: 0.0010(0.0077) Grad: 4232.6387  LR: 0.00000947  \n","Epoch: [3][1400/2225] Elapsed 7m 46s (remain 4m 34s) Loss: 0.0084(0.0075) Grad: 23885.6016  LR: 0.00000919  \n","Epoch: [3][1500/2225] Elapsed 8m 19s (remain 4m 1s) Loss: 0.0001(0.0074) Grad: 389.7983  LR: 0.00000891  \n","Epoch: [3][1600/2225] Elapsed 8m 52s (remain 3m 27s) Loss: 0.0244(0.0074) Grad: 35631.7539  LR: 0.00000863  \n","Epoch: [3][1700/2225] Elapsed 9m 26s (remain 2m 54s) Loss: 0.0043(0.0075) Grad: 44242.5938  LR: 0.00000835  \n","Epoch: [3][1800/2225] Elapsed 9m 59s (remain 2m 21s) Loss: 0.0034(0.0075) Grad: 7849.4683  LR: 0.00000807  \n","Epoch: [3][1900/2225] Elapsed 10m 32s (remain 1m 47s) Loss: 0.0004(0.0076) Grad: 2743.2566  LR: 0.00000779  \n","Epoch: [3][2000/2225] Elapsed 11m 5s (remain 1m 14s) Loss: 0.0000(0.0075) Grad: 109.1536  LR: 0.00000752  \n","Epoch: [3][2100/2225] Elapsed 11m 39s (remain 0m 41s) Loss: 0.0004(0.0076) Grad: 3085.1204  LR: 0.00000725  \n","Epoch: [3][2200/2225] Elapsed 12m 12s (remain 0m 7s) Loss: 0.0142(0.0076) Grad: 52041.3047  LR: 0.00000698  \n","Epoch: [3][2224/2225] Elapsed 12m 20s (remain 0m 0s) Loss: 0.0014(0.0077) Grad: 24348.7715  LR: 0.00000691  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 9s) Loss: 0.0010(0.0010) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0228(0.0180) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0002(0.0156) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.0077  avg_val_loss: 0.0156  time: 774s\n","Epoch 3 - Score: 0.8621\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [4][0/2225] Elapsed 0m 0s (remain 29m 56s) Loss: 0.0000(0.0000) Grad: 118.2268  LR: 0.00000691  \n","Epoch: [4][100/2225] Elapsed 0m 33s (remain 11m 54s) Loss: 0.0001(0.0046) Grad: 287.8953  LR: 0.00000664  \n","Epoch: [4][200/2225] Elapsed 1m 7s (remain 11m 15s) Loss: 0.0138(0.0055) Grad: 24523.6270  LR: 0.00000638  \n","Epoch: [4][300/2225] Elapsed 1m 40s (remain 10m 39s) Loss: 0.0037(0.0060) Grad: 15624.5977  LR: 0.00000612  \n","Epoch: [4][400/2225] Elapsed 2m 13s (remain 10m 6s) Loss: 0.0002(0.0058) Grad: 1565.4685  LR: 0.00000586  \n","Epoch: [4][500/2225] Elapsed 2m 46s (remain 9m 33s) Loss: 0.0000(0.0059) Grad: 45.9392  LR: 0.00000560  \n","Epoch: [4][600/2225] Elapsed 3m 19s (remain 8m 59s) Loss: 0.0001(0.0058) Grad: 1026.3807  LR: 0.00000535  \n","Epoch: [4][700/2225] Elapsed 3m 52s (remain 8m 25s) Loss: 0.0005(0.0059) Grad: 4416.7842  LR: 0.00000510  \n","Epoch: [4][800/2225] Elapsed 4m 25s (remain 7m 52s) Loss: 0.0114(0.0058) Grad: 34841.1406  LR: 0.00000486  \n","Epoch: [4][900/2225] Elapsed 4m 58s (remain 7m 18s) Loss: 0.0061(0.0057) Grad: 20647.8770  LR: 0.00000462  \n","Epoch: [4][1000/2225] Elapsed 5m 31s (remain 6m 45s) Loss: 0.0228(0.0058) Grad: 42503.4688  LR: 0.00000438  \n","Epoch: [4][1100/2225] Elapsed 6m 4s (remain 6m 12s) Loss: 0.0001(0.0058) Grad: 569.8680  LR: 0.00000415  \n","Epoch: [4][1200/2225] Elapsed 6m 37s (remain 5m 39s) Loss: 0.0002(0.0058) Grad: 1595.3058  LR: 0.00000392  \n","Epoch: [4][1300/2225] Elapsed 7m 10s (remain 5m 5s) Loss: 0.0016(0.0058) Grad: 24700.0723  LR: 0.00000370  \n","Epoch: [4][1400/2225] Elapsed 7m 43s (remain 4m 32s) Loss: 0.0006(0.0058) Grad: 4413.1309  LR: 0.00000348  \n","Epoch: [4][1500/2225] Elapsed 8m 17s (remain 3m 59s) Loss: 0.0068(0.0058) Grad: 43652.3086  LR: 0.00000327  \n","Epoch: [4][1600/2225] Elapsed 8m 50s (remain 3m 26s) Loss: 0.0020(0.0058) Grad: 8480.9180  LR: 0.00000307  \n","Epoch: [4][1700/2225] Elapsed 9m 23s (remain 2m 53s) Loss: 0.0000(0.0058) Grad: 72.3093  LR: 0.00000287  \n","Epoch: [4][1800/2225] Elapsed 9m 56s (remain 2m 20s) Loss: 0.0067(0.0059) Grad: 22757.7812  LR: 0.00000267  \n","Epoch: [4][1900/2225] Elapsed 10m 29s (remain 1m 47s) Loss: 0.0006(0.0060) Grad: 3126.5688  LR: 0.00000248  \n","Epoch: [4][2000/2225] Elapsed 11m 2s (remain 1m 14s) Loss: 0.0016(0.0062) Grad: 16345.4209  LR: 0.00000230  \n","Epoch: [4][2100/2225] Elapsed 11m 35s (remain 0m 41s) Loss: 0.0005(0.0061) Grad: 10433.7607  LR: 0.00000212  \n","Epoch: [4][2200/2225] Elapsed 12m 8s (remain 0m 7s) Loss: 0.0005(0.0062) Grad: 7512.4438  LR: 0.00000195  \n","Epoch: [4][2224/2225] Elapsed 12m 16s (remain 0m 0s) Loss: 0.0679(0.0062) Grad: 177742.4375  LR: 0.00000191  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 19s) Loss: 0.0003(0.0003) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0259(0.0195) \n","EVAL: [157/158] Elapsed 0m 31s (remain 0m 0s) Loss: 0.0005(0.0175) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.0062  avg_val_loss: 0.0175  time: 769s\n","Epoch 4 - Score: 0.8664\n","Epoch 4 - Save Best Score: 0.8664 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [5][0/2225] Elapsed 0m 0s (remain 28m 21s) Loss: 0.0038(0.0038) Grad: 9535.6826  LR: 0.00000191  \n","Epoch: [5][100/2225] Elapsed 0m 34s (remain 12m 0s) Loss: 0.0009(0.0054) Grad: 7130.0776  LR: 0.00000175  \n","Epoch: [5][200/2225] Elapsed 1m 7s (remain 11m 20s) Loss: 0.0001(0.0050) Grad: 625.7336  LR: 0.00000159  \n","Epoch: [5][300/2225] Elapsed 1m 40s (remain 10m 43s) Loss: 0.0032(0.0051) Grad: 8785.3340  LR: 0.00000144  \n","Epoch: [5][400/2225] Elapsed 2m 14s (remain 10m 9s) Loss: 0.0000(0.0053) Grad: 78.6847  LR: 0.00000130  \n","Epoch: [5][500/2225] Elapsed 2m 47s (remain 9m 35s) Loss: 0.0045(0.0054) Grad: 9244.0938  LR: 0.00000116  \n","Epoch: [5][600/2225] Elapsed 3m 20s (remain 9m 2s) Loss: 0.0086(0.0056) Grad: 6768.5806  LR: 0.00000103  \n","Epoch: [5][700/2225] Elapsed 3m 53s (remain 8m 28s) Loss: 0.0083(0.0055) Grad: 24332.7168  LR: 0.00000091  \n","Epoch: [5][800/2225] Elapsed 4m 26s (remain 7m 54s) Loss: 0.0014(0.0055) Grad: 6087.2021  LR: 0.00000080  \n","Epoch: [5][900/2225] Elapsed 5m 0s (remain 7m 20s) Loss: 0.0598(0.0055) Grad: 74498.6953  LR: 0.00000069  \n","Epoch: [5][1000/2225] Elapsed 5m 33s (remain 6m 47s) Loss: 0.0011(0.0056) Grad: 8897.0645  LR: 0.00000059  \n","Epoch: [5][1100/2225] Elapsed 6m 6s (remain 6m 14s) Loss: 0.0008(0.0054) Grad: 15169.7959  LR: 0.00000050  \n","Epoch: [5][1200/2225] Elapsed 6m 39s (remain 5m 40s) Loss: 0.0006(0.0053) Grad: 3477.5210  LR: 0.00000042  \n","Epoch: [5][1300/2225] Elapsed 7m 13s (remain 5m 7s) Loss: 0.0000(0.0052) Grad: 128.8067  LR: 0.00000034  \n","Epoch: [5][1400/2225] Elapsed 7m 46s (remain 4m 34s) Loss: 0.0955(0.0051) Grad: 93106.3750  LR: 0.00000027  \n","Epoch: [5][1500/2225] Elapsed 8m 19s (remain 4m 1s) Loss: 0.0406(0.0051) Grad: 143373.4688  LR: 0.00000021  \n","Epoch: [5][1600/2225] Elapsed 8m 52s (remain 3m 27s) Loss: 0.0008(0.0051) Grad: 3996.6379  LR: 0.00000016  \n","Epoch: [5][1700/2225] Elapsed 9m 26s (remain 2m 54s) Loss: 0.0095(0.0050) Grad: 45482.3047  LR: 0.00000011  \n","Epoch: [5][1800/2225] Elapsed 9m 59s (remain 2m 21s) Loss: 0.0001(0.0049) Grad: 463.5459  LR: 0.00000007  \n","Epoch: [5][1900/2225] Elapsed 10m 32s (remain 1m 47s) Loss: 0.0050(0.0049) Grad: 14207.9277  LR: 0.00000004  \n","Epoch: [5][2000/2225] Elapsed 11m 5s (remain 1m 14s) Loss: 0.0048(0.0049) Grad: 56219.3555  LR: 0.00000002  \n","Epoch: [5][2100/2225] Elapsed 11m 39s (remain 0m 41s) Loss: 0.0007(0.0049) Grad: 24151.9902  LR: 0.00000001  \n","Epoch: [5][2200/2225] Elapsed 12m 12s (remain 0m 7s) Loss: 0.1481(0.0051) Grad: 90842.5000  LR: 0.00000000  \n","Epoch: [5][2224/2225] Elapsed 12m 20s (remain 0m 0s) Loss: 0.0343(0.0051) Grad: 92173.0234  LR: 0.00000000  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 11s) Loss: 0.0002(0.0002) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0237(0.0212) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0002(0.0190) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.0051  avg_val_loss: 0.0190  time: 774s\n","Epoch 5 - Score: 0.8666\n","Epoch 5 - Save Best Score: 0.8666 Model\n","========== fold: 7 result ==========\n","Score: 0.8666\n","========== fold: 8 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2225] Elapsed 0m 0s (remain 21m 13s) Loss: 0.4320(0.4320) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2225] Elapsed 0m 34s (remain 12m 2s) Loss: 0.0239(0.0620) Grad: 3646.4275  LR: 0.00002000  \n","Epoch: [1][200/2225] Elapsed 1m 8s (remain 11m 25s) Loss: 0.0141(0.0450) Grad: 3995.2903  LR: 0.00001998  \n","Epoch: [1][300/2225] Elapsed 1m 41s (remain 10m 51s) Loss: 0.0107(0.0380) Grad: 2241.0288  LR: 0.00001996  \n","Epoch: [1][400/2225] Elapsed 2m 15s (remain 10m 16s) Loss: 0.0100(0.0340) Grad: 3836.2581  LR: 0.00001994  \n","Epoch: [1][500/2225] Elapsed 2m 49s (remain 9m 42s) Loss: 0.0135(0.0313) Grad: 4816.4824  LR: 0.00001990  \n","Epoch: [1][600/2225] Elapsed 3m 22s (remain 9m 8s) Loss: 0.0122(0.0297) Grad: 6456.9082  LR: 0.00001986  \n","Epoch: [1][700/2225] Elapsed 3m 56s (remain 8m 34s) Loss: 0.0112(0.0282) Grad: 3819.4973  LR: 0.00001980  \n","Epoch: [1][800/2225] Elapsed 4m 30s (remain 8m 0s) Loss: 0.0037(0.0268) Grad: 2007.1469  LR: 0.00001975  \n","Epoch: [1][900/2225] Elapsed 5m 3s (remain 7m 26s) Loss: 0.0166(0.0255) Grad: 4417.0181  LR: 0.00001968  \n","Epoch: [1][1000/2225] Elapsed 5m 37s (remain 6m 52s) Loss: 0.0099(0.0245) Grad: 3424.8069  LR: 0.00001960  \n","Epoch: [1][1100/2225] Elapsed 6m 11s (remain 6m 19s) Loss: 0.0101(0.0238) Grad: 2124.6558  LR: 0.00001952  \n","Epoch: [1][1200/2225] Elapsed 6m 45s (remain 5m 45s) Loss: 0.0097(0.0231) Grad: 1233.1863  LR: 0.00001943  \n","Epoch: [1][1300/2225] Elapsed 7m 18s (remain 5m 11s) Loss: 0.0224(0.0225) Grad: 2875.6335  LR: 0.00001933  \n","Epoch: [1][1400/2225] Elapsed 7m 52s (remain 4m 37s) Loss: 0.0006(0.0219) Grad: 215.1853  LR: 0.00001923  \n","Epoch: [1][1500/2225] Elapsed 8m 26s (remain 4m 4s) Loss: 0.0064(0.0215) Grad: 2137.2932  LR: 0.00001912  \n","Epoch: [1][1600/2225] Elapsed 8m 59s (remain 3m 30s) Loss: 0.0124(0.0209) Grad: 3042.7568  LR: 0.00001900  \n","Epoch: [1][1700/2225] Elapsed 9m 33s (remain 2m 56s) Loss: 0.0572(0.0206) Grad: 9377.0977  LR: 0.00001887  \n","Epoch: [1][1800/2225] Elapsed 10m 7s (remain 2m 22s) Loss: 0.0106(0.0201) Grad: 3523.5071  LR: 0.00001873  \n","Epoch: [1][1900/2225] Elapsed 10m 40s (remain 1m 49s) Loss: 0.0316(0.0199) Grad: 11716.8906  LR: 0.00001859  \n","Epoch: [1][2000/2225] Elapsed 11m 14s (remain 1m 15s) Loss: 0.0065(0.0196) Grad: 2708.1968  LR: 0.00001845  \n","Epoch: [1][2100/2225] Elapsed 11m 48s (remain 0m 41s) Loss: 0.0011(0.0193) Grad: 1503.2404  LR: 0.00001829  \n","Epoch: [1][2200/2225] Elapsed 12m 22s (remain 0m 8s) Loss: 0.0570(0.0192) Grad: 16807.2617  LR: 0.00001813  \n","Epoch: [1][2224/2225] Elapsed 12m 30s (remain 0m 0s) Loss: 0.0131(0.0191) Grad: 5270.2808  LR: 0.00001809  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 14s) Loss: 0.0244(0.0244) \n","EVAL: [100/158] Elapsed 0m 21s (remain 0m 11s) Loss: 0.0061(0.0136) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0034(0.0127) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.0191  avg_val_loss: 0.0127  time: 784s\n","Epoch 1 - Score: 0.8683\n","Epoch 1 - Save Best Score: 0.8683 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [2][0/2225] Elapsed 0m 0s (remain 29m 52s) Loss: 0.0028(0.0028) Grad: 4620.3530  LR: 0.00001809  \n","Epoch: [2][100/2225] Elapsed 0m 35s (remain 12m 18s) Loss: 0.0232(0.0109) Grad: 23716.5039  LR: 0.00001792  \n","Epoch: [2][200/2225] Elapsed 1m 9s (remain 11m 34s) Loss: 0.0090(0.0100) Grad: 16620.6055  LR: 0.00001774  \n","Epoch: [2][300/2225] Elapsed 1m 42s (remain 10m 58s) Loss: 0.0005(0.0095) Grad: 1962.1649  LR: 0.00001756  \n","Epoch: [2][400/2225] Elapsed 2m 16s (remain 10m 22s) Loss: 0.0165(0.0102) Grad: 27270.0312  LR: 0.00001737  \n","Epoch: [2][500/2225] Elapsed 2m 51s (remain 9m 48s) Loss: 0.0193(0.0102) Grad: 21448.0469  LR: 0.00001718  \n","Epoch: [2][600/2225] Elapsed 3m 25s (remain 9m 15s) Loss: 0.0074(0.0102) Grad: 14251.6377  LR: 0.00001698  \n","Epoch: [2][700/2225] Elapsed 3m 59s (remain 8m 41s) Loss: 0.0095(0.0102) Grad: 9610.5713  LR: 0.00001678  \n","Epoch: [2][800/2225] Elapsed 4m 33s (remain 8m 6s) Loss: 0.0036(0.0102) Grad: 8397.1621  LR: 0.00001657  \n","Epoch: [2][900/2225] Elapsed 5m 7s (remain 7m 32s) Loss: 0.0001(0.0101) Grad: 159.4539  LR: 0.00001635  \n","Epoch: [2][1000/2225] Elapsed 5m 41s (remain 6m 58s) Loss: 0.0205(0.0099) Grad: 16221.0430  LR: 0.00001613  \n","Epoch: [2][1100/2225] Elapsed 6m 15s (remain 6m 23s) Loss: 0.0039(0.0099) Grad: 12647.1152  LR: 0.00001590  \n","Epoch: [2][1200/2225] Elapsed 6m 49s (remain 5m 49s) Loss: 0.0131(0.0097) Grad: 14366.1865  LR: 0.00001567  \n","Epoch: [2][1300/2225] Elapsed 7m 24s (remain 5m 15s) Loss: 0.0005(0.0096) Grad: 1691.3212  LR: 0.00001544  \n","Epoch: [2][1400/2225] Elapsed 7m 58s (remain 4m 41s) Loss: 0.0002(0.0096) Grad: 552.3097  LR: 0.00001520  \n","Epoch: [2][1500/2225] Elapsed 8m 32s (remain 4m 7s) Loss: 0.0062(0.0096) Grad: 13564.2314  LR: 0.00001496  \n","Epoch: [2][1600/2225] Elapsed 9m 6s (remain 3m 33s) Loss: 0.0002(0.0096) Grad: 1967.7605  LR: 0.00001471  \n","Epoch: [2][1700/2225] Elapsed 9m 41s (remain 2m 58s) Loss: 0.0264(0.0096) Grad: 24062.1484  LR: 0.00001446  \n","Epoch: [2][1800/2225] Elapsed 10m 15s (remain 2m 24s) Loss: 0.0009(0.0095) Grad: 16383.9014  LR: 0.00001420  \n","Epoch: [2][1900/2225] Elapsed 10m 49s (remain 1m 50s) Loss: 0.0036(0.0096) Grad: 31275.0312  LR: 0.00001395  \n","Epoch: [2][2000/2225] Elapsed 11m 23s (remain 1m 16s) Loss: 0.0026(0.0096) Grad: 25190.6699  LR: 0.00001369  \n","Epoch: [2][2100/2225] Elapsed 11m 57s (remain 0m 42s) Loss: 0.0251(0.0096) Grad: 90939.3516  LR: 0.00001342  \n","Epoch: [2][2200/2225] Elapsed 12m 31s (remain 0m 8s) Loss: 0.0288(0.0095) Grad: 47200.1289  LR: 0.00001316  \n","Epoch: [2][2224/2225] Elapsed 12m 40s (remain 0m 0s) Loss: 0.0001(0.0095) Grad: 382.0223  LR: 0.00001309  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 17s) Loss: 0.0386(0.0386) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0043(0.0140) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0002(0.0135) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.0095  avg_val_loss: 0.0135  time: 794s\n","Epoch 2 - Score: 0.8864\n","Epoch 2 - Save Best Score: 0.8864 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [3][0/2225] Elapsed 0m 0s (remain 30m 23s) Loss: 0.0018(0.0018) Grad: 5570.6382  LR: 0.00001309  \n","Epoch: [3][100/2225] Elapsed 0m 35s (remain 12m 19s) Loss: 0.0023(0.0066) Grad: 10090.3516  LR: 0.00001282  \n","Epoch: [3][200/2225] Elapsed 1m 9s (remain 11m 36s) Loss: 0.0065(0.0074) Grad: 8657.4814  LR: 0.00001255  \n","Epoch: [3][300/2225] Elapsed 1m 43s (remain 10m 58s) Loss: 0.0027(0.0078) Grad: 7403.1577  LR: 0.00001227  \n","Epoch: [3][400/2225] Elapsed 2m 17s (remain 10m 23s) Loss: 0.0001(0.0073) Grad: 385.7992  LR: 0.00001200  \n","Epoch: [3][500/2225] Elapsed 2m 51s (remain 9m 48s) Loss: 0.0000(0.0073) Grad: 102.3473  LR: 0.00001172  \n","Epoch: [3][600/2225] Elapsed 3m 25s (remain 9m 14s) Loss: 0.0140(0.0074) Grad: 86831.5000  LR: 0.00001144  \n","Epoch: [3][700/2225] Elapsed 3m 59s (remain 8m 40s) Loss: 0.0085(0.0075) Grad: 13465.9502  LR: 0.00001116  \n","Epoch: [3][800/2225] Elapsed 4m 33s (remain 8m 6s) Loss: 0.0157(0.0075) Grad: 32400.7598  LR: 0.00001088  \n","Epoch: [3][900/2225] Elapsed 5m 7s (remain 7m 32s) Loss: 0.0205(0.0075) Grad: 56966.4570  LR: 0.00001060  \n","Epoch: [3][1000/2225] Elapsed 5m 41s (remain 6m 58s) Loss: 0.0039(0.0076) Grad: 22740.4980  LR: 0.00001032  \n","Epoch: [3][1100/2225] Elapsed 6m 16s (remain 6m 24s) Loss: 0.0007(0.0076) Grad: 5085.7090  LR: 0.00001003  \n","Epoch: [3][1200/2225] Elapsed 6m 50s (remain 5m 49s) Loss: 0.0072(0.0076) Grad: 40570.2695  LR: 0.00000975  \n","Epoch: [3][1300/2225] Elapsed 7m 24s (remain 5m 15s) Loss: 0.0015(0.0077) Grad: 11279.6553  LR: 0.00000947  \n","Epoch: [3][1400/2225] Elapsed 7m 58s (remain 4m 41s) Loss: 0.0138(0.0076) Grad: 17128.2461  LR: 0.00000919  \n","Epoch: [3][1500/2225] Elapsed 8m 33s (remain 4m 7s) Loss: 0.0000(0.0076) Grad: 92.0173  LR: 0.00000891  \n","Epoch: [3][1600/2225] Elapsed 9m 7s (remain 3m 33s) Loss: 0.0164(0.0076) Grad: 30581.6387  LR: 0.00000863  \n","Epoch: [3][1700/2225] Elapsed 9m 41s (remain 2m 59s) Loss: 0.0046(0.0077) Grad: 16259.1299  LR: 0.00000835  \n","Epoch: [3][1800/2225] Elapsed 10m 15s (remain 2m 24s) Loss: 0.0037(0.0077) Grad: 18872.1797  LR: 0.00000807  \n","Epoch: [3][1900/2225] Elapsed 10m 49s (remain 1m 50s) Loss: 0.0046(0.0078) Grad: 12419.8398  LR: 0.00000779  \n","Epoch: [3][2000/2225] Elapsed 11m 24s (remain 1m 16s) Loss: 0.0002(0.0078) Grad: 3860.1755  LR: 0.00000752  \n","Epoch: [3][2100/2225] Elapsed 11m 58s (remain 0m 42s) Loss: 0.0091(0.0079) Grad: 33332.7344  LR: 0.00000725  \n","Epoch: [3][2200/2225] Elapsed 12m 32s (remain 0m 8s) Loss: 0.0006(0.0079) Grad: 9815.4209  LR: 0.00000698  \n","Epoch: [3][2224/2225] Elapsed 12m 40s (remain 0m 0s) Loss: 0.0048(0.0079) Grad: 168814.7656  LR: 0.00000691  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 15s) Loss: 0.0328(0.0328) \n","EVAL: [100/158] Elapsed 0m 21s (remain 0m 11s) Loss: 0.0038(0.0134) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0002(0.0131) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.0079  avg_val_loss: 0.0131  time: 795s\n","Epoch 3 - Score: 0.8823\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [4][0/2225] Elapsed 0m 0s (remain 31m 24s) Loss: 0.0013(0.0013) Grad: 4089.5454  LR: 0.00000691  \n","Epoch: [4][100/2225] Elapsed 0m 34s (remain 12m 8s) Loss: 0.0030(0.0054) Grad: 18236.7012  LR: 0.00000664  \n","Epoch: [4][200/2225] Elapsed 1m 8s (remain 11m 29s) Loss: 0.0001(0.0057) Grad: 341.6655  LR: 0.00000638  \n","Epoch: [4][300/2225] Elapsed 1m 42s (remain 10m 54s) Loss: 0.0019(0.0060) Grad: 14064.1016  LR: 0.00000612  \n","Epoch: [4][400/2225] Elapsed 2m 16s (remain 10m 19s) Loss: 0.0028(0.0063) Grad: 22005.1953  LR: 0.00000586  \n","Epoch: [4][500/2225] Elapsed 2m 50s (remain 9m 45s) Loss: 0.0080(0.0067) Grad: 14485.6221  LR: 0.00000560  \n","Epoch: [4][600/2225] Elapsed 3m 23s (remain 9m 11s) Loss: 0.0000(0.0067) Grad: 80.6627  LR: 0.00000535  \n","Epoch: [4][700/2225] Elapsed 3m 57s (remain 8m 36s) Loss: 0.0002(0.0068) Grad: 2200.9634  LR: 0.00000510  \n","Epoch: [4][800/2225] Elapsed 4m 31s (remain 8m 2s) Loss: 0.0001(0.0067) Grad: 251.4105  LR: 0.00000486  \n","Epoch: [4][900/2225] Elapsed 5m 5s (remain 7m 28s) Loss: 0.0010(0.0066) Grad: 4951.4961  LR: 0.00000462  \n","Epoch: [4][1000/2225] Elapsed 5m 38s (remain 6m 54s) Loss: 0.0065(0.0068) Grad: 17773.6191  LR: 0.00000438  \n","Epoch: [4][1100/2225] Elapsed 6m 12s (remain 6m 20s) Loss: 0.0001(0.0067) Grad: 326.2260  LR: 0.00000415  \n","Epoch: [4][1200/2225] Elapsed 6m 46s (remain 5m 46s) Loss: 0.0156(0.0067) Grad: 69522.1953  LR: 0.00000392  \n","Epoch: [4][1300/2225] Elapsed 7m 20s (remain 5m 12s) Loss: 0.0011(0.0065) Grad: 4433.5532  LR: 0.00000370  \n","Epoch: [4][1400/2225] Elapsed 7m 54s (remain 4m 38s) Loss: 0.0001(0.0064) Grad: 1298.0042  LR: 0.00000348  \n","Epoch: [4][1500/2225] Elapsed 8m 27s (remain 4m 4s) Loss: 0.0075(0.0064) Grad: 11557.9814  LR: 0.00000327  \n","Epoch: [4][1600/2225] Elapsed 9m 1s (remain 3m 31s) Loss: 0.0000(0.0064) Grad: 23.3244  LR: 0.00000307  \n","Epoch: [4][1700/2225] Elapsed 9m 35s (remain 2m 57s) Loss: 0.0002(0.0062) Grad: 1313.3650  LR: 0.00000287  \n","Epoch: [4][1800/2225] Elapsed 10m 8s (remain 2m 23s) Loss: 0.0001(0.0062) Grad: 546.9438  LR: 0.00000267  \n","Epoch: [4][1900/2225] Elapsed 10m 42s (remain 1m 49s) Loss: 0.0031(0.0062) Grad: 12761.5400  LR: 0.00000248  \n","Epoch: [4][2000/2225] Elapsed 11m 16s (remain 1m 15s) Loss: 0.0007(0.0062) Grad: 8633.3584  LR: 0.00000230  \n","Epoch: [4][2100/2225] Elapsed 11m 50s (remain 0m 41s) Loss: 0.0000(0.0063) Grad: 160.7931  LR: 0.00000212  \n","Epoch: [4][2200/2225] Elapsed 12m 23s (remain 0m 8s) Loss: 0.0036(0.0063) Grad: 66148.0234  LR: 0.00000195  \n","Epoch: [4][2224/2225] Elapsed 12m 31s (remain 0m 0s) Loss: 0.0000(0.0063) Grad: 83.6217  LR: 0.00000191  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 18s) Loss: 0.0559(0.0559) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0030(0.0162) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0001(0.0158) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.0063  avg_val_loss: 0.0158  time: 786s\n","Epoch 4 - Score: 0.8832\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [5][0/2225] Elapsed 0m 0s (remain 29m 50s) Loss: 0.0001(0.0001) Grad: 355.3850  LR: 0.00000191  \n","Epoch: [5][100/2225] Elapsed 0m 34s (remain 12m 14s) Loss: 0.0000(0.0057) Grad: 122.2188  LR: 0.00000175  \n","Epoch: [5][200/2225] Elapsed 1m 8s (remain 11m 34s) Loss: 0.0002(0.0061) Grad: 989.2277  LR: 0.00000159  \n","Epoch: [5][300/2225] Elapsed 1m 43s (remain 10m 58s) Loss: 0.0002(0.0056) Grad: 1568.8628  LR: 0.00000144  \n","Epoch: [5][400/2225] Elapsed 2m 17s (remain 10m 23s) Loss: 0.0000(0.0057) Grad: 58.2786  LR: 0.00000130  \n","Epoch: [5][500/2225] Elapsed 2m 51s (remain 9m 48s) Loss: 0.0001(0.0055) Grad: 464.5581  LR: 0.00000116  \n","Epoch: [5][600/2225] Elapsed 3m 25s (remain 9m 14s) Loss: 0.0014(0.0055) Grad: 8009.3677  LR: 0.00000103  \n","Epoch: [5][700/2225] Elapsed 3m 59s (remain 8m 40s) Loss: 0.0010(0.0054) Grad: 5728.9941  LR: 0.00000091  \n","Epoch: [5][800/2225] Elapsed 4m 33s (remain 8m 6s) Loss: 0.0197(0.0053) Grad: 39295.1445  LR: 0.00000080  \n","Epoch: [5][900/2225] Elapsed 5m 7s (remain 7m 31s) Loss: 0.0025(0.0052) Grad: 5887.4443  LR: 0.00000069  \n","Epoch: [5][1000/2225] Elapsed 5m 41s (remain 6m 57s) Loss: 0.0000(0.0052) Grad: 222.5555  LR: 0.00000059  \n","Epoch: [5][1100/2225] Elapsed 6m 15s (remain 6m 23s) Loss: 0.0027(0.0051) Grad: 5958.4121  LR: 0.00000050  \n","Epoch: [5][1200/2225] Elapsed 6m 49s (remain 5m 49s) Loss: 0.0036(0.0052) Grad: 21561.2676  LR: 0.00000042  \n","Epoch: [5][1300/2225] Elapsed 7m 23s (remain 5m 15s) Loss: 0.0000(0.0052) Grad: 189.3628  LR: 0.00000034  \n","Epoch: [5][1400/2225] Elapsed 7m 58s (remain 4m 41s) Loss: 0.0000(0.0051) Grad: 135.4910  LR: 0.00000027  \n","Epoch: [5][1500/2225] Elapsed 8m 32s (remain 4m 7s) Loss: 0.0011(0.0050) Grad: 9472.2627  LR: 0.00000021  \n","Epoch: [5][1600/2225] Elapsed 9m 6s (remain 3m 33s) Loss: 0.0137(0.0050) Grad: 62151.9922  LR: 0.00000016  \n","Epoch: [5][1700/2225] Elapsed 9m 40s (remain 2m 58s) Loss: 0.0030(0.0050) Grad: 13080.1445  LR: 0.00000011  \n","Epoch: [5][1800/2225] Elapsed 10m 14s (remain 2m 24s) Loss: 0.0053(0.0050) Grad: 21624.0996  LR: 0.00000007  \n","Epoch: [5][1900/2225] Elapsed 10m 49s (remain 1m 50s) Loss: 0.0037(0.0050) Grad: 32325.3887  LR: 0.00000004  \n","Epoch: [5][2000/2225] Elapsed 11m 23s (remain 1m 16s) Loss: 0.0156(0.0050) Grad: 105516.8438  LR: 0.00000002  \n","Epoch: [5][2100/2225] Elapsed 11m 57s (remain 0m 42s) Loss: 0.0000(0.0051) Grad: 334.5406  LR: 0.00000001  \n","Epoch: [5][2200/2225] Elapsed 12m 31s (remain 0m 8s) Loss: 0.0000(0.0052) Grad: 28.4054  LR: 0.00000000  \n","Epoch: [5][2224/2225] Elapsed 12m 39s (remain 0m 0s) Loss: 0.0003(0.0052) Grad: 4363.6113  LR: 0.00000000  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 15s) Loss: 0.0582(0.0582) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0020(0.0169) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0001(0.0166) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.0052  avg_val_loss: 0.0166  time: 793s\n","Epoch 5 - Score: 0.8839\n","========== fold: 8 result ==========\n","Score: 0.8864\n","========== fold: 9 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2225] Elapsed 0m 0s (remain 21m 28s) Loss: 0.8743(0.8743) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2225] Elapsed 0m 34s (remain 11m 57s) Loss: 0.0283(0.1065) Grad: 4110.9038  LR: 0.00002000  \n","Epoch: [1][200/2225] Elapsed 1m 7s (remain 11m 23s) Loss: 0.0154(0.0683) Grad: 1429.7568  LR: 0.00001998  \n","Epoch: [1][300/2225] Elapsed 1m 41s (remain 10m 49s) Loss: 0.0137(0.0534) Grad: 1070.3966  LR: 0.00001996  \n","Epoch: [1][400/2225] Elapsed 2m 15s (remain 10m 14s) Loss: 0.0868(0.0453) Grad: 5502.8662  LR: 0.00001994  \n","Epoch: [1][500/2225] Elapsed 2m 48s (remain 9m 40s) Loss: 0.0015(0.0399) Grad: 610.7102  LR: 0.00001990  \n","Epoch: [1][600/2225] Elapsed 3m 22s (remain 9m 6s) Loss: 0.0194(0.0359) Grad: 2087.9004  LR: 0.00001986  \n","Epoch: [1][700/2225] Elapsed 3m 55s (remain 8m 32s) Loss: 0.0192(0.0336) Grad: 2498.4866  LR: 0.00001980  \n","Epoch: [1][800/2225] Elapsed 4m 29s (remain 7m 58s) Loss: 0.0046(0.0317) Grad: 1212.6405  LR: 0.00001975  \n","Epoch: [1][900/2225] Elapsed 5m 3s (remain 7m 25s) Loss: 0.0286(0.0301) Grad: 2872.8579  LR: 0.00001968  \n","Epoch: [1][1000/2225] Elapsed 5m 36s (remain 6m 51s) Loss: 0.0178(0.0286) Grad: 2095.4980  LR: 0.00001960  \n","Epoch: [1][1100/2225] Elapsed 6m 10s (remain 6m 18s) Loss: 0.0060(0.0274) Grad: 486.4282  LR: 0.00001952  \n","Epoch: [1][1200/2225] Elapsed 6m 43s (remain 5m 44s) Loss: 0.0041(0.0265) Grad: 546.1528  LR: 0.00001943  \n","Epoch: [1][1300/2225] Elapsed 7m 17s (remain 5m 10s) Loss: 0.0207(0.0257) Grad: 932.7838  LR: 0.00001933  \n","Epoch: [1][1400/2225] Elapsed 7m 51s (remain 4m 37s) Loss: 0.0345(0.0253) Grad: 3802.7786  LR: 0.00001923  \n","Epoch: [1][1500/2225] Elapsed 8m 24s (remain 4m 3s) Loss: 0.0028(0.0247) Grad: 242.9859  LR: 0.00001912  \n","Epoch: [1][1600/2225] Elapsed 8m 58s (remain 3m 29s) Loss: 0.0107(0.0241) Grad: 1534.0212  LR: 0.00001900  \n","Epoch: [1][1700/2225] Elapsed 9m 31s (remain 2m 56s) Loss: 0.0021(0.0234) Grad: 370.0733  LR: 0.00001887  \n","Epoch: [1][1800/2225] Elapsed 10m 5s (remain 2m 22s) Loss: 0.0197(0.0228) Grad: 1595.7036  LR: 0.00001873  \n","Epoch: [1][1900/2225] Elapsed 10m 38s (remain 1m 48s) Loss: 0.0922(0.0224) Grad: 4116.2935  LR: 0.00001859  \n","Epoch: [1][2000/2225] Elapsed 11m 12s (remain 1m 15s) Loss: 0.0124(0.0219) Grad: 944.5341  LR: 0.00001845  \n","Epoch: [1][2100/2225] Elapsed 11m 45s (remain 0m 41s) Loss: 0.0226(0.0214) Grad: 6242.4468  LR: 0.00001829  \n","Epoch: [1][2200/2225] Elapsed 12m 19s (remain 0m 8s) Loss: 0.0073(0.0209) Grad: 1141.3176  LR: 0.00001813  \n","Epoch: [1][2224/2225] Elapsed 12m 27s (remain 0m 0s) Loss: 0.0032(0.0208) Grad: 1064.7615  LR: 0.00001809  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 12s) Loss: 0.0037(0.0037) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0054(0.0150) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0043(0.0134) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.0208  avg_val_loss: 0.0134  time: 781s\n","Epoch 1 - Score: 0.8582\n","Epoch 1 - Save Best Score: 0.8582 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [2][0/2225] Elapsed 0m 0s (remain 29m 40s) Loss: 0.0010(0.0010) Grad: 2124.0000  LR: 0.00001809  \n","Epoch: [2][100/2225] Elapsed 0m 34s (remain 12m 14s) Loss: 0.0068(0.0111) Grad: 11822.5010  LR: 0.00001792  \n","Epoch: [2][200/2225] Elapsed 1m 8s (remain 11m 33s) Loss: 0.0103(0.0101) Grad: 52906.2188  LR: 0.00001774  \n","Epoch: [2][300/2225] Elapsed 1m 42s (remain 10m 56s) Loss: 0.0014(0.0100) Grad: 6562.9683  LR: 0.00001756  \n","Epoch: [2][400/2225] Elapsed 2m 16s (remain 10m 21s) Loss: 0.0066(0.0102) Grad: 25269.3164  LR: 0.00001737  \n","Epoch: [2][500/2225] Elapsed 2m 50s (remain 9m 46s) Loss: 0.0024(0.0101) Grad: 6260.5142  LR: 0.00001718  \n","Epoch: [2][600/2225] Elapsed 3m 24s (remain 9m 12s) Loss: 0.0003(0.0099) Grad: 1446.2776  LR: 0.00001698  \n","Epoch: [2][700/2225] Elapsed 3m 58s (remain 8m 37s) Loss: 0.0069(0.0098) Grad: 25290.2441  LR: 0.00001678  \n","Epoch: [2][800/2225] Elapsed 4m 31s (remain 8m 3s) Loss: 0.0121(0.0097) Grad: 32067.9551  LR: 0.00001657  \n","Epoch: [2][900/2225] Elapsed 5m 5s (remain 7m 29s) Loss: 0.0020(0.0095) Grad: 5030.7183  LR: 0.00001635  \n","Epoch: [2][1000/2225] Elapsed 5m 39s (remain 6m 55s) Loss: 0.0084(0.0095) Grad: 6863.5894  LR: 0.00001613  \n","Epoch: [2][1100/2225] Elapsed 6m 13s (remain 6m 21s) Loss: 0.0132(0.0094) Grad: 12368.4609  LR: 0.00001590  \n","Epoch: [2][1200/2225] Elapsed 6m 47s (remain 5m 47s) Loss: 0.0023(0.0094) Grad: 5052.9644  LR: 0.00001567  \n","Epoch: [2][1300/2225] Elapsed 7m 21s (remain 5m 13s) Loss: 0.0059(0.0094) Grad: 13904.4629  LR: 0.00001544  \n","Epoch: [2][1400/2225] Elapsed 7m 55s (remain 4m 39s) Loss: 0.0146(0.0096) Grad: 33453.5312  LR: 0.00001520  \n","Epoch: [2][1500/2225] Elapsed 8m 29s (remain 4m 5s) Loss: 0.0026(0.0095) Grad: 10136.8340  LR: 0.00001496  \n","Epoch: [2][1600/2225] Elapsed 9m 2s (remain 3m 31s) Loss: 0.0005(0.0094) Grad: 6810.9346  LR: 0.00001471  \n","Epoch: [2][1700/2225] Elapsed 9m 36s (remain 2m 57s) Loss: 0.0453(0.0094) Grad: 50057.3086  LR: 0.00001446  \n","Epoch: [2][1800/2225] Elapsed 10m 10s (remain 2m 23s) Loss: 0.0715(0.0093) Grad: 83957.8750  LR: 0.00001420  \n","Epoch: [2][1900/2225] Elapsed 10m 44s (remain 1m 49s) Loss: 0.0001(0.0092) Grad: 307.1956  LR: 0.00001395  \n","Epoch: [2][2000/2225] Elapsed 11m 18s (remain 1m 15s) Loss: 0.0025(0.0092) Grad: 16579.5566  LR: 0.00001369  \n","Epoch: [2][2100/2225] Elapsed 11m 52s (remain 0m 42s) Loss: 0.0034(0.0092) Grad: 75463.9453  LR: 0.00001342  \n","Epoch: [2][2200/2225] Elapsed 12m 25s (remain 0m 8s) Loss: 0.0002(0.0092) Grad: 1139.1666  LR: 0.00001316  \n","Epoch: [2][2224/2225] Elapsed 12m 34s (remain 0m 0s) Loss: 0.0156(0.0092) Grad: 72289.8516  LR: 0.00001309  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 14s) Loss: 0.0011(0.0011) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0118(0.0161) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0076(0.0146) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.0092  avg_val_loss: 0.0146  time: 788s\n","Epoch 2 - Score: 0.8679\n","Epoch 2 - Save Best Score: 0.8679 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [3][0/2225] Elapsed 0m 0s (remain 29m 28s) Loss: 0.0074(0.0074) Grad: 25005.5234  LR: 0.00001309  \n","Epoch: [3][100/2225] Elapsed 0m 34s (remain 12m 14s) Loss: 0.0002(0.0082) Grad: 533.7615  LR: 0.00001282  \n","Epoch: [3][200/2225] Elapsed 1m 8s (remain 11m 32s) Loss: 0.0011(0.0079) Grad: 4116.1040  LR: 0.00001255  \n","Epoch: [3][300/2225] Elapsed 1m 42s (remain 10m 55s) Loss: 0.0128(0.0082) Grad: 28530.6035  LR: 0.00001227  \n","Epoch: [3][400/2225] Elapsed 2m 16s (remain 10m 20s) Loss: 0.0110(0.0081) Grad: 15543.2725  LR: 0.00001200  \n","Epoch: [3][500/2225] Elapsed 2m 50s (remain 9m 45s) Loss: 0.0001(0.0084) Grad: 534.1058  LR: 0.00001172  \n","Epoch: [3][600/2225] Elapsed 3m 24s (remain 9m 11s) Loss: 0.0001(0.0083) Grad: 475.6935  LR: 0.00001144  \n","Epoch: [3][700/2225] Elapsed 3m 58s (remain 8m 37s) Loss: 0.0015(0.0079) Grad: 5429.7021  LR: 0.00001116  \n","Epoch: [3][800/2225] Elapsed 4m 31s (remain 8m 3s) Loss: 0.0010(0.0077) Grad: 3464.0061  LR: 0.00001088  \n","Epoch: [3][900/2225] Elapsed 5m 6s (remain 7m 29s) Loss: 0.0090(0.0078) Grad: 91004.6797  LR: 0.00001060  \n","Epoch: [3][1000/2225] Elapsed 5m 40s (remain 6m 55s) Loss: 0.0005(0.0076) Grad: 3200.4907  LR: 0.00001032  \n","Epoch: [3][1100/2225] Elapsed 6m 13s (remain 6m 21s) Loss: 0.0001(0.0075) Grad: 348.2986  LR: 0.00001003  \n","Epoch: [3][1200/2225] Elapsed 6m 48s (remain 5m 47s) Loss: 0.0001(0.0075) Grad: 411.1433  LR: 0.00000975  \n","Epoch: [3][1300/2225] Elapsed 7m 22s (remain 5m 14s) Loss: 0.0193(0.0075) Grad: 19450.2051  LR: 0.00000947  \n","Epoch: [3][1400/2225] Elapsed 7m 56s (remain 4m 40s) Loss: 0.0522(0.0076) Grad: 24312.7461  LR: 0.00000919  \n","Epoch: [3][1500/2225] Elapsed 8m 30s (remain 4m 6s) Loss: 0.0297(0.0075) Grad: 33529.8086  LR: 0.00000891  \n","Epoch: [3][1600/2225] Elapsed 9m 4s (remain 3m 32s) Loss: 0.0059(0.0075) Grad: 18290.7324  LR: 0.00000863  \n","Epoch: [3][1700/2225] Elapsed 9m 38s (remain 2m 58s) Loss: 0.0099(0.0077) Grad: 30672.9629  LR: 0.00000835  \n","Epoch: [3][1800/2225] Elapsed 10m 12s (remain 2m 24s) Loss: 0.0000(0.0077) Grad: 37.4569  LR: 0.00000807  \n","Epoch: [3][1900/2225] Elapsed 10m 46s (remain 1m 50s) Loss: 0.0000(0.0077) Grad: 133.7299  LR: 0.00000779  \n","Epoch: [3][2000/2225] Elapsed 11m 19s (remain 1m 16s) Loss: 0.0373(0.0077) Grad: 90983.8906  LR: 0.00000752  \n","Epoch: [3][2100/2225] Elapsed 11m 53s (remain 0m 42s) Loss: 0.0018(0.0077) Grad: 10074.4834  LR: 0.00000725  \n","Epoch: [3][2200/2225] Elapsed 12m 27s (remain 0m 8s) Loss: 0.0075(0.0078) Grad: 20159.1113  LR: 0.00000698  \n","Epoch: [3][2224/2225] Elapsed 12m 36s (remain 0m 0s) Loss: 0.0119(0.0079) Grad: 42898.4102  LR: 0.00000691  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 16s) Loss: 0.0013(0.0013) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0385(0.0168) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0084(0.0150) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.0079  avg_val_loss: 0.0150  time: 790s\n","Epoch 3 - Score: 0.8731\n","Epoch 3 - Save Best Score: 0.8731 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [4][0/2225] Elapsed 0m 0s (remain 30m 3s) Loss: 0.0000(0.0000) Grad: 49.0887  LR: 0.00000691  \n","Epoch: [4][100/2225] Elapsed 0m 35s (remain 12m 18s) Loss: 0.0073(0.0054) Grad: 15618.0586  LR: 0.00000664  \n","Epoch: [4][200/2225] Elapsed 1m 8s (remain 11m 34s) Loss: 0.0054(0.0064) Grad: 12841.3721  LR: 0.00000638  \n","Epoch: [4][300/2225] Elapsed 1m 42s (remain 10m 57s) Loss: 0.0004(0.0068) Grad: 2930.8457  LR: 0.00000612  \n","Epoch: [4][400/2225] Elapsed 2m 16s (remain 10m 22s) Loss: 0.0031(0.0066) Grad: 13050.9180  LR: 0.00000586  \n","Epoch: [4][500/2225] Elapsed 2m 50s (remain 9m 47s) Loss: 0.0003(0.0068) Grad: 1324.2179  LR: 0.00000560  \n","Epoch: [4][600/2225] Elapsed 3m 24s (remain 9m 13s) Loss: 0.0030(0.0067) Grad: 55102.8047  LR: 0.00000535  \n","Epoch: [4][700/2225] Elapsed 3m 58s (remain 8m 38s) Loss: 0.0000(0.0069) Grad: 65.8832  LR: 0.00000510  \n","Epoch: [4][800/2225] Elapsed 4m 32s (remain 8m 4s) Loss: 0.0232(0.0067) Grad: 11576.9414  LR: 0.00000486  \n","Epoch: [4][900/2225] Elapsed 5m 6s (remain 7m 30s) Loss: 0.0018(0.0067) Grad: 8285.2393  LR: 0.00000462  \n","Epoch: [4][1000/2225] Elapsed 5m 40s (remain 6m 55s) Loss: 0.0011(0.0068) Grad: 4468.3481  LR: 0.00000438  \n","Epoch: [4][1100/2225] Elapsed 6m 14s (remain 6m 21s) Loss: 0.0197(0.0067) Grad: 66534.3906  LR: 0.00000415  \n","Epoch: [4][1200/2225] Elapsed 6m 47s (remain 5m 47s) Loss: 0.0034(0.0067) Grad: 29800.5938  LR: 0.00000392  \n","Epoch: [4][1300/2225] Elapsed 7m 21s (remain 5m 13s) Loss: 0.0039(0.0066) Grad: 4698.6377  LR: 0.00000370  \n","Epoch: [4][1400/2225] Elapsed 7m 56s (remain 4m 39s) Loss: 0.0075(0.0066) Grad: 13811.1191  LR: 0.00000348  \n","Epoch: [4][1500/2225] Elapsed 8m 29s (remain 4m 5s) Loss: 0.0085(0.0065) Grad: 15535.4414  LR: 0.00000327  \n","Epoch: [4][1600/2225] Elapsed 9m 3s (remain 3m 31s) Loss: 0.0139(0.0066) Grad: 59345.9062  LR: 0.00000307  \n","Epoch: [4][1700/2225] Elapsed 9m 37s (remain 2m 58s) Loss: 0.0108(0.0066) Grad: 15103.6201  LR: 0.00000287  \n","Epoch: [4][1800/2225] Elapsed 10m 12s (remain 2m 24s) Loss: 0.0005(0.0065) Grad: 3390.7141  LR: 0.00000267  \n","Epoch: [4][1900/2225] Elapsed 10m 46s (remain 1m 50s) Loss: 0.0002(0.0065) Grad: 1704.8804  LR: 0.00000248  \n","Epoch: [4][2000/2225] Elapsed 11m 20s (remain 1m 16s) Loss: 0.0048(0.0065) Grad: 27720.1270  LR: 0.00000230  \n","Epoch: [4][2100/2225] Elapsed 11m 54s (remain 0m 42s) Loss: 0.0001(0.0066) Grad: 511.7911  LR: 0.00000212  \n","Epoch: [4][2200/2225] Elapsed 12m 28s (remain 0m 8s) Loss: 0.0002(0.0066) Grad: 3056.1858  LR: 0.00000195  \n","Epoch: [4][2224/2225] Elapsed 12m 36s (remain 0m 0s) Loss: 0.0013(0.0066) Grad: 8007.2222  LR: 0.00000191  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 15s) Loss: 0.0005(0.0005) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0356(0.0185) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0091(0.0165) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.0066  avg_val_loss: 0.0165  time: 790s\n","Epoch 4 - Score: 0.8756\n","Epoch 4 - Save Best Score: 0.8756 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [5][0/2225] Elapsed 0m 0s (remain 29m 1s) Loss: 0.0173(0.0173) Grad: 66607.2812  LR: 0.00000191  \n","Epoch: [5][100/2225] Elapsed 0m 34s (remain 12m 15s) Loss: 0.0000(0.0055) Grad: 74.2329  LR: 0.00000175  \n","Epoch: [5][200/2225] Elapsed 1m 9s (remain 11m 34s) Loss: 0.0248(0.0064) Grad: 81471.8203  LR: 0.00000159  \n","Epoch: [5][300/2225] Elapsed 1m 42s (remain 10m 57s) Loss: 0.0018(0.0064) Grad: 65800.1875  LR: 0.00000144  \n","Epoch: [5][400/2225] Elapsed 2m 16s (remain 10m 22s) Loss: 0.0000(0.0062) Grad: 43.5291  LR: 0.00000130  \n","Epoch: [5][500/2225] Elapsed 2m 50s (remain 9m 47s) Loss: 0.0004(0.0059) Grad: 4817.4199  LR: 0.00000116  \n","Epoch: [5][600/2225] Elapsed 3m 24s (remain 9m 12s) Loss: 0.0014(0.0058) Grad: 24745.4980  LR: 0.00000103  \n","Epoch: [5][700/2225] Elapsed 3m 58s (remain 8m 38s) Loss: 0.0112(0.0055) Grad: 80182.4219  LR: 0.00000091  \n","Epoch: [5][800/2225] Elapsed 4m 32s (remain 8m 4s) Loss: 0.0002(0.0056) Grad: 1269.6589  LR: 0.00000080  \n","Epoch: [5][900/2225] Elapsed 5m 6s (remain 7m 29s) Loss: 0.0018(0.0054) Grad: 10250.7979  LR: 0.00000069  \n","Epoch: [5][1000/2225] Elapsed 5m 40s (remain 6m 55s) Loss: 0.0082(0.0053) Grad: 11743.6982  LR: 0.00000059  \n","Epoch: [5][1100/2225] Elapsed 6m 14s (remain 6m 22s) Loss: 0.0024(0.0054) Grad: 25411.3730  LR: 0.00000050  \n","Epoch: [5][1200/2225] Elapsed 6m 48s (remain 5m 47s) Loss: 0.0047(0.0054) Grad: 16417.2227  LR: 0.00000042  \n","Epoch: [5][1300/2225] Elapsed 7m 21s (remain 5m 13s) Loss: 0.0266(0.0054) Grad: 129958.1641  LR: 0.00000034  \n","Epoch: [5][1400/2225] Elapsed 7m 55s (remain 4m 39s) Loss: 0.0031(0.0055) Grad: 17452.0234  LR: 0.00000027  \n","Epoch: [5][1500/2225] Elapsed 8m 30s (remain 4m 6s) Loss: 0.0000(0.0055) Grad: 275.3442  LR: 0.00000021  \n","Epoch: [5][1600/2225] Elapsed 9m 3s (remain 3m 31s) Loss: 0.0030(0.0055) Grad: 24647.5352  LR: 0.00000016  \n","Epoch: [5][1700/2225] Elapsed 9m 37s (remain 2m 57s) Loss: 0.0003(0.0056) Grad: 3488.6711  LR: 0.00000011  \n","Epoch: [5][1800/2225] Elapsed 10m 11s (remain 2m 24s) Loss: 0.0001(0.0055) Grad: 403.1408  LR: 0.00000007  \n","Epoch: [5][1900/2225] Elapsed 10m 45s (remain 1m 50s) Loss: 0.0006(0.0055) Grad: 4938.9639  LR: 0.00000004  \n","Epoch: [5][2000/2225] Elapsed 11m 19s (remain 1m 16s) Loss: 0.0001(0.0055) Grad: 1814.1709  LR: 0.00000002  \n","Epoch: [5][2100/2225] Elapsed 11m 53s (remain 0m 42s) Loss: 0.0052(0.0054) Grad: 8359.4775  LR: 0.00000001  \n","Epoch: [5][2200/2225] Elapsed 12m 27s (remain 0m 8s) Loss: 0.0176(0.0054) Grad: 152064.5000  LR: 0.00000000  \n","Epoch: [5][2224/2225] Elapsed 12m 35s (remain 0m 0s) Loss: 0.0016(0.0054) Grad: 29221.6289  LR: 0.00000000  \n","EVAL: [0/158] Elapsed 0m 0s (remain 1m 14s) Loss: 0.0003(0.0003) \n","EVAL: [100/158] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0426(0.0195) \n","EVAL: [157/158] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0093(0.0173) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.0054  avg_val_loss: 0.0173  time: 789s\n","Epoch 5 - Score: 0.8792\n","Epoch 5 - Save Best Score: 0.8792 Model\n","========== fold: 9 result ==========\n","Score: 0.8792\n","========== fold: 10 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2224] Elapsed 0m 0s (remain 22m 9s) Loss: 0.5157(0.5157) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2224] Elapsed 0m 33s (remain 11m 43s) Loss: 0.0290(0.0765) Grad: 3728.5066  LR: 0.00002000  \n","Epoch: [1][200/2224] Elapsed 1m 6s (remain 11m 6s) Loss: 0.0351(0.0530) Grad: 5797.7788  LR: 0.00001998  \n","Epoch: [1][300/2224] Elapsed 1m 38s (remain 10m 31s) Loss: 0.0039(0.0441) Grad: 1976.5680  LR: 0.00001996  \n","Epoch: [1][400/2224] Elapsed 2m 11s (remain 9m 58s) Loss: 0.0123(0.0384) Grad: 1513.4683  LR: 0.00001994  \n","Epoch: [1][500/2224] Elapsed 2m 44s (remain 9m 24s) Loss: 0.0014(0.0349) Grad: 482.3877  LR: 0.00001990  \n","Epoch: [1][600/2224] Elapsed 3m 16s (remain 8m 51s) Loss: 0.0713(0.0319) Grad: 8928.6895  LR: 0.00001986  \n","Epoch: [1][700/2224] Elapsed 3m 49s (remain 8m 19s) Loss: 0.0075(0.0304) Grad: 1480.1373  LR: 0.00001980  \n","Epoch: [1][800/2224] Elapsed 4m 22s (remain 7m 46s) Loss: 0.0085(0.0289) Grad: 2599.7161  LR: 0.00001975  \n","Epoch: [1][900/2224] Elapsed 4m 55s (remain 7m 13s) Loss: 0.0007(0.0274) Grad: 1276.2314  LR: 0.00001968  \n","Epoch: [1][1000/2224] Elapsed 5m 27s (remain 6m 40s) Loss: 0.0170(0.0263) Grad: 19446.3242  LR: 0.00001960  \n","Epoch: [1][1100/2224] Elapsed 6m 0s (remain 6m 7s) Loss: 0.0239(0.0256) Grad: 11527.0498  LR: 0.00001952  \n","Epoch: [1][1200/2224] Elapsed 6m 33s (remain 5m 34s) Loss: 0.0059(0.0246) Grad: 2753.9822  LR: 0.00001943  \n","Epoch: [1][1300/2224] Elapsed 7m 6s (remain 5m 2s) Loss: 0.0559(0.0240) Grad: 5971.9912  LR: 0.00001933  \n","Epoch: [1][1400/2224] Elapsed 7m 38s (remain 4m 29s) Loss: 0.0021(0.0233) Grad: 624.1428  LR: 0.00001923  \n","Epoch: [1][1500/2224] Elapsed 8m 11s (remain 3m 56s) Loss: 0.0338(0.0228) Grad: 4067.4006  LR: 0.00001911  \n","Epoch: [1][1600/2224] Elapsed 8m 44s (remain 3m 23s) Loss: 0.0059(0.0223) Grad: 971.2711  LR: 0.00001899  \n","Epoch: [1][1700/2224] Elapsed 9m 17s (remain 2m 51s) Loss: 0.0151(0.0219) Grad: 1918.8206  LR: 0.00001887  \n","Epoch: [1][1800/2224] Elapsed 9m 49s (remain 2m 18s) Loss: 0.0009(0.0214) Grad: 209.5843  LR: 0.00001873  \n","Epoch: [1][1900/2224] Elapsed 10m 22s (remain 1m 45s) Loss: 0.0037(0.0211) Grad: 1208.2949  LR: 0.00001859  \n","Epoch: [1][2000/2224] Elapsed 10m 55s (remain 1m 13s) Loss: 0.0217(0.0205) Grad: 4740.8271  LR: 0.00001844  \n","Epoch: [1][2100/2224] Elapsed 11m 28s (remain 0m 40s) Loss: 0.0013(0.0203) Grad: 1425.1313  LR: 0.00001829  \n","Epoch: [1][2200/2224] Elapsed 12m 0s (remain 0m 7s) Loss: 0.0040(0.0200) Grad: 1448.8917  LR: 0.00001813  \n","Epoch: [1][2223/2224] Elapsed 12m 8s (remain 0m 0s) Loss: 0.0054(0.0199) Grad: 6736.5781  LR: 0.00001809  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 13s) Loss: 0.0215(0.0215) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0014(0.0152) \n","EVAL: [159/160] Elapsed 0m 31s (remain 0m 0s) Loss: 0.0000(0.0132) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.0199  avg_val_loss: 0.0132  time: 761s\n","Epoch 1 - Score: 0.8703\n","Epoch 1 - Save Best Score: 0.8703 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [2][0/2224] Elapsed 0m 0s (remain 29m 25s) Loss: 0.0009(0.0009) Grad: 2564.5002  LR: 0.00001809  \n","Epoch: [2][100/2224] Elapsed 0m 34s (remain 12m 9s) Loss: 0.0067(0.0088) Grad: 26672.4570  LR: 0.00001792  \n","Epoch: [2][200/2224] Elapsed 1m 8s (remain 11m 26s) Loss: 0.0004(0.0085) Grad: 1072.8889  LR: 0.00001774  \n","Epoch: [2][300/2224] Elapsed 1m 41s (remain 10m 49s) Loss: 0.0172(0.0088) Grad: 36807.2305  LR: 0.00001756  \n","Epoch: [2][400/2224] Elapsed 2m 15s (remain 10m 14s) Loss: 0.0057(0.0084) Grad: 74713.5859  LR: 0.00001737  \n","Epoch: [2][500/2224] Elapsed 2m 48s (remain 9m 40s) Loss: 0.0001(0.0091) Grad: 256.0980  LR: 0.00001718  \n","Epoch: [2][600/2224] Elapsed 3m 22s (remain 9m 6s) Loss: 0.0330(0.0090) Grad: 68252.7656  LR: 0.00001698  \n","Epoch: [2][700/2224] Elapsed 3m 55s (remain 8m 32s) Loss: 0.0350(0.0090) Grad: 51383.3672  LR: 0.00001678  \n","Epoch: [2][800/2224] Elapsed 4m 29s (remain 7m 58s) Loss: 0.0016(0.0089) Grad: 5268.7603  LR: 0.00001657  \n","Epoch: [2][900/2224] Elapsed 5m 3s (remain 7m 25s) Loss: 0.0155(0.0089) Grad: 32335.6582  LR: 0.00001635  \n","Epoch: [2][1000/2224] Elapsed 5m 36s (remain 6m 51s) Loss: 0.0095(0.0090) Grad: 11690.0273  LR: 0.00001613  \n","Epoch: [2][1100/2224] Elapsed 6m 10s (remain 6m 17s) Loss: 0.0223(0.0090) Grad: 40696.3945  LR: 0.00001590  \n","Epoch: [2][1200/2224] Elapsed 6m 43s (remain 5m 44s) Loss: 0.0048(0.0090) Grad: 10255.4248  LR: 0.00001567  \n","Epoch: [2][1300/2224] Elapsed 7m 17s (remain 5m 10s) Loss: 0.0007(0.0092) Grad: 2481.6235  LR: 0.00001544  \n","Epoch: [2][1400/2224] Elapsed 7m 51s (remain 4m 36s) Loss: 0.0017(0.0091) Grad: 4858.7725  LR: 0.00001520  \n","Epoch: [2][1500/2224] Elapsed 8m 24s (remain 4m 3s) Loss: 0.0109(0.0090) Grad: 15773.6621  LR: 0.00001496  \n","Epoch: [2][1600/2224] Elapsed 8m 58s (remain 3m 29s) Loss: 0.0179(0.0090) Grad: 31842.5469  LR: 0.00001471  \n","Epoch: [2][1700/2224] Elapsed 9m 31s (remain 2m 55s) Loss: 0.0048(0.0090) Grad: 9187.4404  LR: 0.00001446  \n","Epoch: [2][1800/2224] Elapsed 10m 5s (remain 2m 22s) Loss: 0.0049(0.0091) Grad: 8637.8486  LR: 0.00001420  \n","Epoch: [2][1900/2224] Elapsed 10m 39s (remain 1m 48s) Loss: 0.0001(0.0091) Grad: 229.3649  LR: 0.00001394  \n","Epoch: [2][2000/2224] Elapsed 11m 12s (remain 1m 14s) Loss: 0.0053(0.0091) Grad: 19479.6914  LR: 0.00001368  \n","Epoch: [2][2100/2224] Elapsed 11m 46s (remain 0m 41s) Loss: 0.0061(0.0090) Grad: 15550.9053  LR: 0.00001342  \n","Epoch: [2][2200/2224] Elapsed 12m 20s (remain 0m 7s) Loss: 0.0539(0.0091) Grad: 47127.7109  LR: 0.00001315  \n","Epoch: [2][2223/2224] Elapsed 12m 27s (remain 0m 0s) Loss: 0.0064(0.0091) Grad: 40969.4336  LR: 0.00001309  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 18s) Loss: 0.0228(0.0228) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0011(0.0154) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0000(0.0136) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.0091  avg_val_loss: 0.0136  time: 782s\n","Epoch 2 - Score: 0.8766\n","Epoch 2 - Save Best Score: 0.8766 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [3][0/2224] Elapsed 0m 0s (remain 28m 40s) Loss: 0.0031(0.0031) Grad: 4290.1377  LR: 0.00001309  \n","Epoch: [3][100/2224] Elapsed 0m 34s (remain 12m 6s) Loss: 0.0046(0.0068) Grad: 6613.9609  LR: 0.00001282  \n","Epoch: [3][200/2224] Elapsed 1m 8s (remain 11m 27s) Loss: 0.0003(0.0066) Grad: 2706.8596  LR: 0.00001255  \n","Epoch: [3][300/2224] Elapsed 1m 41s (remain 10m 51s) Loss: 0.0100(0.0069) Grad: 25398.7969  LR: 0.00001227  \n","Epoch: [3][400/2224] Elapsed 2m 15s (remain 10m 16s) Loss: 0.0013(0.0070) Grad: 4889.2241  LR: 0.00001200  \n","Epoch: [3][500/2224] Elapsed 2m 49s (remain 9m 42s) Loss: 0.0078(0.0068) Grad: 16608.3789  LR: 0.00001172  \n","Epoch: [3][600/2224] Elapsed 3m 22s (remain 9m 8s) Loss: 0.0070(0.0071) Grad: 10381.3750  LR: 0.00001144  \n","Epoch: [3][700/2224] Elapsed 3m 56s (remain 8m 33s) Loss: 0.0211(0.0070) Grad: 19834.1758  LR: 0.00001116  \n","Epoch: [3][800/2224] Elapsed 4m 30s (remain 7m 59s) Loss: 0.0020(0.0074) Grad: 15629.2129  LR: 0.00001088  \n","Epoch: [3][900/2224] Elapsed 5m 3s (remain 7m 25s) Loss: 0.0013(0.0073) Grad: 6022.3911  LR: 0.00001060  \n","Epoch: [3][1000/2224] Elapsed 5m 37s (remain 6m 51s) Loss: 0.0012(0.0074) Grad: 15673.1543  LR: 0.00001031  \n","Epoch: [3][1100/2224] Elapsed 6m 10s (remain 6m 18s) Loss: 0.0048(0.0074) Grad: 4573.3389  LR: 0.00001003  \n","Epoch: [3][1200/2224] Elapsed 6m 44s (remain 5m 44s) Loss: 0.0010(0.0075) Grad: 3205.6021  LR: 0.00000975  \n","Epoch: [3][1300/2224] Elapsed 7m 18s (remain 5m 10s) Loss: 0.0026(0.0076) Grad: 16893.5918  LR: 0.00000947  \n","Epoch: [3][1400/2224] Elapsed 7m 51s (remain 4m 37s) Loss: 0.0150(0.0077) Grad: 51308.0352  LR: 0.00000918  \n","Epoch: [3][1500/2224] Elapsed 8m 25s (remain 4m 3s) Loss: 0.0011(0.0077) Grad: 6254.7305  LR: 0.00000890  \n","Epoch: [3][1600/2224] Elapsed 8m 59s (remain 3m 29s) Loss: 0.0001(0.0077) Grad: 2745.3401  LR: 0.00000862  \n","Epoch: [3][1700/2224] Elapsed 9m 32s (remain 2m 56s) Loss: 0.0115(0.0080) Grad: 33403.8711  LR: 0.00000834  \n","Epoch: [3][1800/2224] Elapsed 10m 6s (remain 2m 22s) Loss: 0.0045(0.0080) Grad: 9468.9199  LR: 0.00000807  \n","Epoch: [3][1900/2224] Elapsed 10m 39s (remain 1m 48s) Loss: 0.0022(0.0079) Grad: 17006.1523  LR: 0.00000779  \n","Epoch: [3][2000/2224] Elapsed 11m 13s (remain 1m 15s) Loss: 0.0105(0.0078) Grad: 25160.7207  LR: 0.00000751  \n","Epoch: [3][2100/2224] Elapsed 11m 46s (remain 0m 41s) Loss: 0.0058(0.0078) Grad: 24640.5723  LR: 0.00000724  \n","Epoch: [3][2200/2224] Elapsed 12m 20s (remain 0m 7s) Loss: 0.0208(0.0078) Grad: 105781.6250  LR: 0.00000697  \n","Epoch: [3][2223/2224] Elapsed 12m 28s (remain 0m 0s) Loss: 0.0045(0.0078) Grad: 152633.1562  LR: 0.00000691  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 14s) Loss: 0.0187(0.0187) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0015(0.0166) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0000(0.0149) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.0078  avg_val_loss: 0.0149  time: 782s\n","Epoch 3 - Score: 0.8756\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [4][0/2224] Elapsed 0m 0s (remain 29m 31s) Loss: 0.0021(0.0021) Grad: 10748.7549  LR: 0.00000691  \n","Epoch: [4][100/2224] Elapsed 0m 34s (remain 12m 6s) Loss: 0.0000(0.0054) Grad: 86.3715  LR: 0.00000664  \n","Epoch: [4][200/2224] Elapsed 1m 8s (remain 11m 27s) Loss: 0.0002(0.0052) Grad: 1685.7137  LR: 0.00000638  \n","Epoch: [4][300/2224] Elapsed 1m 41s (remain 10m 51s) Loss: 0.0119(0.0061) Grad: 6597.4233  LR: 0.00000611  \n","Epoch: [4][400/2224] Elapsed 2m 15s (remain 10m 16s) Loss: 0.0068(0.0057) Grad: 12152.2031  LR: 0.00000585  \n","Epoch: [4][500/2224] Elapsed 2m 49s (remain 9m 41s) Loss: 0.0000(0.0059) Grad: 26.7646  LR: 0.00000560  \n","Epoch: [4][600/2224] Elapsed 3m 22s (remain 9m 7s) Loss: 0.0016(0.0060) Grad: 13617.8984  LR: 0.00000535  \n","Epoch: [4][700/2224] Elapsed 3m 56s (remain 8m 33s) Loss: 0.0109(0.0061) Grad: 23361.3535  LR: 0.00000510  \n","Epoch: [4][800/2224] Elapsed 4m 30s (remain 7m 59s) Loss: 0.0005(0.0061) Grad: 2887.0190  LR: 0.00000485  \n","Epoch: [4][900/2224] Elapsed 5m 3s (remain 7m 26s) Loss: 0.0001(0.0063) Grad: 420.1337  LR: 0.00000461  \n","Epoch: [4][1000/2224] Elapsed 5m 37s (remain 6m 52s) Loss: 0.0138(0.0064) Grad: 24570.8008  LR: 0.00000438  \n","Epoch: [4][1100/2224] Elapsed 6m 10s (remain 6m 18s) Loss: 0.0000(0.0063) Grad: 130.1270  LR: 0.00000415  \n","Epoch: [4][1200/2224] Elapsed 6m 44s (remain 5m 44s) Loss: 0.0003(0.0063) Grad: 1576.3257  LR: 0.00000392  \n","Epoch: [4][1300/2224] Elapsed 7m 17s (remain 5m 10s) Loss: 0.0031(0.0062) Grad: 7525.5088  LR: 0.00000370  \n","Epoch: [4][1400/2224] Elapsed 7m 51s (remain 4m 36s) Loss: 0.0056(0.0063) Grad: 6681.6987  LR: 0.00000348  \n","Epoch: [4][1500/2224] Elapsed 8m 25s (remain 4m 3s) Loss: 0.0000(0.0063) Grad: 51.4529  LR: 0.00000327  \n","Epoch: [4][1600/2224] Elapsed 8m 59s (remain 3m 29s) Loss: 0.0013(0.0063) Grad: 8373.2568  LR: 0.00000306  \n","Epoch: [4][1700/2224] Elapsed 9m 32s (remain 2m 56s) Loss: 0.0005(0.0062) Grad: 3000.9749  LR: 0.00000286  \n","Epoch: [4][1800/2224] Elapsed 10m 6s (remain 2m 22s) Loss: 0.0009(0.0062) Grad: 7701.6772  LR: 0.00000267  \n","Epoch: [4][1900/2224] Elapsed 10m 40s (remain 1m 48s) Loss: 0.0000(0.0061) Grad: 465.1016  LR: 0.00000248  \n","Epoch: [4][2000/2224] Elapsed 11m 13s (remain 1m 15s) Loss: 0.0001(0.0060) Grad: 1604.0293  LR: 0.00000230  \n","Epoch: [4][2100/2224] Elapsed 11m 47s (remain 0m 41s) Loss: 0.0000(0.0061) Grad: 225.2655  LR: 0.00000212  \n","Epoch: [4][2200/2224] Elapsed 12m 20s (remain 0m 7s) Loss: 0.0012(0.0062) Grad: 25538.9648  LR: 0.00000195  \n","Epoch: [4][2223/2224] Elapsed 12m 28s (remain 0m 0s) Loss: 0.0007(0.0062) Grad: 11498.3223  LR: 0.00000191  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 12s) Loss: 0.0289(0.0289) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0004(0.0177) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0000(0.0161) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.0062  avg_val_loss: 0.0161  time: 783s\n","Epoch 4 - Score: 0.8816\n","Epoch 4 - Save Best Score: 0.8816 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [5][0/2224] Elapsed 0m 0s (remain 30m 6s) Loss: 0.0013(0.0013) Grad: 14842.8730  LR: 0.00000191  \n","Epoch: [5][100/2224] Elapsed 0m 35s (remain 12m 21s) Loss: 0.0005(0.0045) Grad: 4375.7920  LR: 0.00000175  \n","Epoch: [5][200/2224] Elapsed 1m 9s (remain 11m 39s) Loss: 0.0008(0.0045) Grad: 5816.7427  LR: 0.00000159  \n","Epoch: [5][300/2224] Elapsed 1m 43s (remain 11m 2s) Loss: 0.0061(0.0043) Grad: 24365.2324  LR: 0.00000144  \n","Epoch: [5][400/2224] Elapsed 2m 17s (remain 10m 25s) Loss: 0.0065(0.0045) Grad: 26813.6641  LR: 0.00000130  \n","Epoch: [5][500/2224] Elapsed 2m 51s (remain 9m 49s) Loss: 0.0065(0.0048) Grad: 51901.4141  LR: 0.00000116  \n","Epoch: [5][600/2224] Elapsed 3m 25s (remain 9m 15s) Loss: 0.0042(0.0047) Grad: 75395.4375  LR: 0.00000103  \n","Epoch: [5][700/2224] Elapsed 3m 59s (remain 8m 41s) Loss: 0.0001(0.0048) Grad: 748.5139  LR: 0.00000091  \n","Epoch: [5][800/2224] Elapsed 4m 33s (remain 8m 6s) Loss: 0.0013(0.0049) Grad: 16145.9922  LR: 0.00000080  \n","Epoch: [5][900/2224] Elapsed 5m 7s (remain 7m 32s) Loss: 0.0013(0.0049) Grad: 6265.3594  LR: 0.00000069  \n","Epoch: [5][1000/2224] Elapsed 5m 41s (remain 6m 57s) Loss: 0.0006(0.0050) Grad: 4170.5732  LR: 0.00000059  \n","Epoch: [5][1100/2224] Elapsed 6m 15s (remain 6m 23s) Loss: 0.0001(0.0049) Grad: 993.4478  LR: 0.00000050  \n","Epoch: [5][1200/2224] Elapsed 6m 50s (remain 5m 49s) Loss: 0.0004(0.0050) Grad: 2742.8320  LR: 0.00000041  \n","Epoch: [5][1300/2224] Elapsed 7m 24s (remain 5m 15s) Loss: 0.0000(0.0050) Grad: 166.5839  LR: 0.00000034  \n","Epoch: [5][1400/2224] Elapsed 7m 58s (remain 4m 41s) Loss: 0.0000(0.0051) Grad: 79.1857  LR: 0.00000027  \n","Epoch: [5][1500/2224] Elapsed 8m 32s (remain 4m 7s) Loss: 0.0227(0.0050) Grad: 24646.4062  LR: 0.00000021  \n","Epoch: [5][1600/2224] Elapsed 9m 6s (remain 3m 32s) Loss: 0.0052(0.0051) Grad: 32844.9336  LR: 0.00000015  \n","Epoch: [5][1700/2224] Elapsed 9m 40s (remain 2m 58s) Loss: 0.0003(0.0050) Grad: 4219.5044  LR: 0.00000011  \n","Epoch: [5][1800/2224] Elapsed 10m 15s (remain 2m 24s) Loss: 0.0002(0.0050) Grad: 1281.8508  LR: 0.00000007  \n","Epoch: [5][1900/2224] Elapsed 10m 49s (remain 1m 50s) Loss: 0.0145(0.0049) Grad: 29068.8906  LR: 0.00000004  \n","Epoch: [5][2000/2224] Elapsed 11m 23s (remain 1m 16s) Loss: 0.0016(0.0049) Grad: 20257.2188  LR: 0.00000002  \n","Epoch: [5][2100/2224] Elapsed 11m 57s (remain 0m 42s) Loss: 0.0242(0.0049) Grad: 311234.6250  LR: 0.00000001  \n","Epoch: [5][2200/2224] Elapsed 12m 31s (remain 0m 7s) Loss: 0.0013(0.0049) Grad: 14613.9580  LR: 0.00000000  \n","Epoch: [5][2223/2224] Elapsed 12m 39s (remain 0m 0s) Loss: 0.0031(0.0049) Grad: 32890.2148  LR: 0.00000000  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 21s) Loss: 0.0324(0.0324) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 12s) Loss: 0.0003(0.0195) \n","EVAL: [159/160] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0000(0.0179) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.0049  avg_val_loss: 0.0179  time: 794s\n","Epoch 5 - Score: 0.8785\n","========== fold: 10 result ==========\n","Score: 0.8816\n","========== fold: 11 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2224] Elapsed 0m 0s (remain 20m 53s) Loss: 1.1460(1.1460) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2224] Elapsed 0m 32s (remain 11m 29s) Loss: 0.0174(0.1288) Grad: 2295.0457  LR: 0.00002000  \n","Epoch: [1][200/2224] Elapsed 1m 5s (remain 10m 57s) Loss: 0.0280(0.0801) Grad: 1602.2192  LR: 0.00001998  \n","Epoch: [1][300/2224] Elapsed 1m 37s (remain 10m 24s) Loss: 0.0348(0.0622) Grad: 1542.0105  LR: 0.00001996  \n","Epoch: [1][400/2224] Elapsed 2m 10s (remain 9m 51s) Loss: 0.0130(0.0528) Grad: 1513.2983  LR: 0.00001994  \n","Epoch: [1][500/2224] Elapsed 2m 42s (remain 9m 19s) Loss: 0.0475(0.0460) Grad: 5662.7871  LR: 0.00001990  \n","Epoch: [1][600/2224] Elapsed 3m 15s (remain 8m 46s) Loss: 0.0090(0.0422) Grad: 685.6855  LR: 0.00001986  \n","Epoch: [1][700/2224] Elapsed 3m 47s (remain 8m 13s) Loss: 0.0028(0.0391) Grad: 460.9738  LR: 0.00001980  \n","Epoch: [1][800/2224] Elapsed 4m 19s (remain 7m 41s) Loss: 0.0660(0.0366) Grad: 8033.1733  LR: 0.00001975  \n","Epoch: [1][900/2224] Elapsed 4m 51s (remain 7m 8s) Loss: 0.0167(0.0344) Grad: 2304.2021  LR: 0.00001968  \n","Epoch: [1][1000/2224] Elapsed 5m 24s (remain 6m 35s) Loss: 0.0085(0.0326) Grad: 598.8116  LR: 0.00001960  \n","Epoch: [1][1100/2224] Elapsed 5m 56s (remain 6m 3s) Loss: 0.0241(0.0310) Grad: 4185.7847  LR: 0.00001952  \n","Epoch: [1][1200/2224] Elapsed 6m 28s (remain 5m 31s) Loss: 0.0052(0.0299) Grad: 566.8807  LR: 0.00001943  \n","Epoch: [1][1300/2224] Elapsed 7m 1s (remain 4m 58s) Loss: 0.0077(0.0287) Grad: 886.8669  LR: 0.00001933  \n","Epoch: [1][1400/2224] Elapsed 7m 33s (remain 4m 26s) Loss: 0.0103(0.0280) Grad: 887.8240  LR: 0.00001923  \n","Epoch: [1][1500/2224] Elapsed 8m 5s (remain 3m 53s) Loss: 0.0122(0.0273) Grad: 548.1641  LR: 0.00001911  \n","Epoch: [1][1600/2224] Elapsed 8m 37s (remain 3m 21s) Loss: 0.0068(0.0265) Grad: 637.3044  LR: 0.00001899  \n","Epoch: [1][1700/2224] Elapsed 9m 10s (remain 2m 49s) Loss: 0.0041(0.0258) Grad: 382.1091  LR: 0.00001887  \n","Epoch: [1][1800/2224] Elapsed 9m 42s (remain 2m 16s) Loss: 0.0019(0.0251) Grad: 374.9453  LR: 0.00001873  \n","Epoch: [1][1900/2224] Elapsed 10m 14s (remain 1m 44s) Loss: 0.0197(0.0247) Grad: 2849.8706  LR: 0.00001859  \n","Epoch: [1][2000/2224] Elapsed 10m 47s (remain 1m 12s) Loss: 0.0438(0.0241) Grad: 2694.0344  LR: 0.00001844  \n","Epoch: [1][2100/2224] Elapsed 11m 19s (remain 0m 39s) Loss: 0.0004(0.0235) Grad: 87.8480  LR: 0.00001829  \n","Epoch: [1][2200/2224] Elapsed 11m 51s (remain 0m 7s) Loss: 0.0378(0.0232) Grad: 8655.9180  LR: 0.00001813  \n","Epoch: [1][2223/2224] Elapsed 11m 59s (remain 0m 0s) Loss: 0.0130(0.0231) Grad: 1090.3112  LR: 0.00001809  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 13s) Loss: 0.0263(0.0263) \n","EVAL: [100/160] Elapsed 0m 19s (remain 0m 11s) Loss: 0.0005(0.0185) \n","EVAL: [159/160] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0007(0.0149) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.0231  avg_val_loss: 0.0149  time: 751s\n","Epoch 1 - Score: 0.8581\n","Epoch 1 - Save Best Score: 0.8581 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [2][0/2224] Elapsed 0m 0s (remain 28m 37s) Loss: 0.0008(0.0008) Grad: 2018.8033  LR: 0.00001809  \n","Epoch: [2][100/2224] Elapsed 0m 33s (remain 11m 42s) Loss: 0.0105(0.0077) Grad: 17977.7832  LR: 0.00001792  \n","Epoch: [2][200/2224] Elapsed 1m 5s (remain 11m 2s) Loss: 0.0033(0.0088) Grad: 20427.7285  LR: 0.00001774  \n","Epoch: [2][300/2224] Elapsed 1m 38s (remain 10m 27s) Loss: 0.0022(0.0087) Grad: 7835.7397  LR: 0.00001756  \n","Epoch: [2][400/2224] Elapsed 2m 10s (remain 9m 53s) Loss: 0.0038(0.0086) Grad: 4368.4912  LR: 0.00001737  \n","Epoch: [2][500/2224] Elapsed 2m 42s (remain 9m 19s) Loss: 0.0057(0.0090) Grad: 13718.8545  LR: 0.00001718  \n","Epoch: [2][600/2224] Elapsed 3m 15s (remain 8m 46s) Loss: 0.0020(0.0089) Grad: 6064.4697  LR: 0.00001698  \n","Epoch: [2][700/2224] Elapsed 3m 47s (remain 8m 14s) Loss: 0.0001(0.0090) Grad: 893.1508  LR: 0.00001678  \n","Epoch: [2][800/2224] Elapsed 4m 19s (remain 7m 41s) Loss: 0.0082(0.0092) Grad: 12334.0859  LR: 0.00001657  \n","Epoch: [2][900/2224] Elapsed 4m 52s (remain 7m 8s) Loss: 0.0002(0.0095) Grad: 513.7116  LR: 0.00001635  \n","Epoch: [2][1000/2224] Elapsed 5m 24s (remain 6m 36s) Loss: 0.0121(0.0094) Grad: 37354.2109  LR: 0.00001613  \n","Epoch: [2][1100/2224] Elapsed 5m 57s (remain 6m 4s) Loss: 0.0023(0.0095) Grad: 7910.2778  LR: 0.00001590  \n","Epoch: [2][1200/2224] Elapsed 6m 29s (remain 5m 31s) Loss: 0.0463(0.0094) Grad: 57747.1875  LR: 0.00001567  \n","Epoch: [2][1300/2224] Elapsed 7m 1s (remain 4m 59s) Loss: 0.0198(0.0094) Grad: 18775.3613  LR: 0.00001544  \n","Epoch: [2][1400/2224] Elapsed 7m 34s (remain 4m 26s) Loss: 0.0451(0.0094) Grad: 253608.9375  LR: 0.00001520  \n","Epoch: [2][1500/2224] Elapsed 8m 6s (remain 3m 54s) Loss: 0.0023(0.0093) Grad: 5418.4067  LR: 0.00001496  \n","Epoch: [2][1600/2224] Elapsed 8m 39s (remain 3m 22s) Loss: 0.0248(0.0093) Grad: 119038.4844  LR: 0.00001471  \n","Epoch: [2][1700/2224] Elapsed 9m 11s (remain 2m 49s) Loss: 0.0001(0.0093) Grad: 234.7769  LR: 0.00001446  \n","Epoch: [2][1800/2224] Elapsed 9m 43s (remain 2m 17s) Loss: 0.0242(0.0093) Grad: 38438.9453  LR: 0.00001420  \n","Epoch: [2][1900/2224] Elapsed 10m 16s (remain 1m 44s) Loss: 0.0146(0.0092) Grad: 15696.3262  LR: 0.00001394  \n","Epoch: [2][2000/2224] Elapsed 10m 48s (remain 1m 12s) Loss: 0.0043(0.0092) Grad: 15499.7852  LR: 0.00001368  \n","Epoch: [2][2100/2224] Elapsed 11m 21s (remain 0m 39s) Loss: 0.0176(0.0093) Grad: 124453.2656  LR: 0.00001342  \n","Epoch: [2][2200/2224] Elapsed 11m 53s (remain 0m 7s) Loss: 0.0051(0.0093) Grad: 9832.4766  LR: 0.00001315  \n","Epoch: [2][2223/2224] Elapsed 12m 0s (remain 0m 0s) Loss: 0.0069(0.0094) Grad: 45446.7227  LR: 0.00001309  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 15s) Loss: 0.0307(0.0307) \n","EVAL: [100/160] Elapsed 0m 19s (remain 0m 11s) Loss: 0.0001(0.0194) \n","EVAL: [159/160] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0001(0.0151) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.0094  avg_val_loss: 0.0151  time: 753s\n","Epoch 2 - Score: 0.8750\n","Epoch 2 - Save Best Score: 0.8750 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [3][0/2224] Elapsed 0m 0s (remain 30m 36s) Loss: 0.0089(0.0089) Grad: 12690.1777  LR: 0.00001309  \n","Epoch: [3][100/2224] Elapsed 0m 33s (remain 11m 51s) Loss: 0.0578(0.0067) Grad: 69856.3047  LR: 0.00001282  \n","Epoch: [3][200/2224] Elapsed 1m 6s (remain 11m 11s) Loss: 0.0067(0.0076) Grad: 16736.5488  LR: 0.00001255  \n","Epoch: [3][300/2224] Elapsed 1m 39s (remain 10m 35s) Loss: 0.0050(0.0078) Grad: 29928.0156  LR: 0.00001227  \n","Epoch: [3][400/2224] Elapsed 2m 12s (remain 10m 1s) Loss: 0.0072(0.0080) Grad: 68095.0469  LR: 0.00001200  \n","Epoch: [3][500/2224] Elapsed 2m 45s (remain 9m 27s) Loss: 0.0031(0.0081) Grad: 7880.9624  LR: 0.00001172  \n","Epoch: [3][600/2224] Elapsed 3m 17s (remain 8m 54s) Loss: 0.0074(0.0081) Grad: 25824.1348  LR: 0.00001144  \n","Epoch: [3][700/2224] Elapsed 3m 50s (remain 8m 21s) Loss: 0.0005(0.0081) Grad: 5584.3335  LR: 0.00001116  \n","Epoch: [3][800/2224] Elapsed 4m 23s (remain 7m 47s) Loss: 0.0237(0.0082) Grad: 29223.4551  LR: 0.00001088  \n","Epoch: [3][900/2224] Elapsed 4m 56s (remain 7m 15s) Loss: 0.0093(0.0081) Grad: 15703.0537  LR: 0.00001060  \n","Epoch: [3][1000/2224] Elapsed 5m 29s (remain 6m 42s) Loss: 0.0080(0.0082) Grad: 14843.4141  LR: 0.00001031  \n","Epoch: [3][1100/2224] Elapsed 6m 1s (remain 6m 9s) Loss: 0.0036(0.0082) Grad: 23613.3379  LR: 0.00001003  \n","Epoch: [3][1200/2224] Elapsed 6m 34s (remain 5m 36s) Loss: 0.0050(0.0082) Grad: 7993.1973  LR: 0.00000975  \n","Epoch: [3][1300/2224] Elapsed 7m 7s (remain 5m 3s) Loss: 0.0269(0.0082) Grad: 41956.0664  LR: 0.00000947  \n","Epoch: [3][1400/2224] Elapsed 7m 40s (remain 4m 30s) Loss: 0.0309(0.0080) Grad: 38623.0938  LR: 0.00000918  \n","Epoch: [3][1500/2224] Elapsed 8m 13s (remain 3m 57s) Loss: 0.0183(0.0079) Grad: 20731.5781  LR: 0.00000890  \n","Epoch: [3][1600/2224] Elapsed 8m 46s (remain 3m 24s) Loss: 0.0319(0.0080) Grad: 133157.8750  LR: 0.00000862  \n","Epoch: [3][1700/2224] Elapsed 9m 18s (remain 2m 51s) Loss: 0.0023(0.0080) Grad: 7544.1987  LR: 0.00000834  \n","Epoch: [3][1800/2224] Elapsed 9m 51s (remain 2m 18s) Loss: 0.0062(0.0080) Grad: 9929.8340  LR: 0.00000807  \n","Epoch: [3][1900/2224] Elapsed 10m 24s (remain 1m 46s) Loss: 0.0003(0.0079) Grad: 1701.8217  LR: 0.00000779  \n","Epoch: [3][2000/2224] Elapsed 10m 57s (remain 1m 13s) Loss: 0.0148(0.0080) Grad: 16042.4365  LR: 0.00000751  \n","Epoch: [3][2100/2224] Elapsed 11m 29s (remain 0m 40s) Loss: 0.0258(0.0081) Grad: 102487.3125  LR: 0.00000724  \n","Epoch: [3][2200/2224] Elapsed 12m 2s (remain 0m 7s) Loss: 0.0049(0.0081) Grad: 13367.9033  LR: 0.00000697  \n","Epoch: [3][2223/2224] Elapsed 12m 10s (remain 0m 0s) Loss: 0.0048(0.0080) Grad: 85103.9688  LR: 0.00000691  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 14s) Loss: 0.0255(0.0255) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0001(0.0207) \n","EVAL: [159/160] Elapsed 0m 31s (remain 0m 0s) Loss: 0.0001(0.0159) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.0080  avg_val_loss: 0.0159  time: 763s\n","Epoch 3 - Score: 0.8789\n","Epoch 3 - Save Best Score: 0.8789 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [4][0/2224] Elapsed 0m 0s (remain 30m 31s) Loss: 0.0111(0.0111) Grad: 21624.9863  LR: 0.00000691  \n","Epoch: [4][100/2224] Elapsed 0m 33s (remain 11m 44s) Loss: 0.0081(0.0076) Grad: 34922.5938  LR: 0.00000664  \n","Epoch: [4][200/2224] Elapsed 1m 5s (remain 11m 3s) Loss: 0.0117(0.0065) Grad: 158632.4531  LR: 0.00000638  \n","Epoch: [4][300/2224] Elapsed 1m 38s (remain 10m 27s) Loss: 0.0019(0.0064) Grad: 23050.0352  LR: 0.00000611  \n","Epoch: [4][400/2224] Elapsed 2m 10s (remain 9m 54s) Loss: 0.0019(0.0062) Grad: 12136.6484  LR: 0.00000585  \n","Epoch: [4][500/2224] Elapsed 2m 43s (remain 9m 20s) Loss: 0.0036(0.0066) Grad: 5932.6411  LR: 0.00000560  \n","Epoch: [4][600/2224] Elapsed 3m 15s (remain 8m 48s) Loss: 0.0044(0.0064) Grad: 170442.5469  LR: 0.00000535  \n","Epoch: [4][700/2224] Elapsed 3m 47s (remain 8m 15s) Loss: 0.0033(0.0065) Grad: 9879.7217  LR: 0.00000510  \n","Epoch: [4][800/2224] Elapsed 4m 20s (remain 7m 42s) Loss: 0.0017(0.0064) Grad: 9243.4580  LR: 0.00000485  \n","Epoch: [4][900/2224] Elapsed 4m 52s (remain 7m 9s) Loss: 0.0033(0.0064) Grad: 8787.1699  LR: 0.00000461  \n","Epoch: [4][1000/2224] Elapsed 5m 25s (remain 6m 37s) Loss: 0.0073(0.0064) Grad: 17046.3848  LR: 0.00000438  \n","Epoch: [4][1100/2224] Elapsed 5m 57s (remain 6m 4s) Loss: 0.0108(0.0066) Grad: 27952.6992  LR: 0.00000415  \n","Epoch: [4][1200/2224] Elapsed 6m 30s (remain 5m 32s) Loss: 0.0002(0.0068) Grad: 563.5787  LR: 0.00000392  \n","Epoch: [4][1300/2224] Elapsed 7m 2s (remain 4m 59s) Loss: 0.0002(0.0067) Grad: 1826.6554  LR: 0.00000370  \n","Epoch: [4][1400/2224] Elapsed 7m 35s (remain 4m 27s) Loss: 0.0003(0.0067) Grad: 3512.9363  LR: 0.00000348  \n","Epoch: [4][1500/2224] Elapsed 8m 7s (remain 3m 54s) Loss: 0.0001(0.0066) Grad: 333.9998  LR: 0.00000327  \n","Epoch: [4][1600/2224] Elapsed 8m 40s (remain 3m 22s) Loss: 0.0000(0.0067) Grad: 37.8830  LR: 0.00000306  \n","Epoch: [4][1700/2224] Elapsed 9m 12s (remain 2m 49s) Loss: 0.0001(0.0068) Grad: 457.8648  LR: 0.00000286  \n","Epoch: [4][1800/2224] Elapsed 9m 44s (remain 2m 17s) Loss: 0.0002(0.0068) Grad: 694.0231  LR: 0.00000267  \n","Epoch: [4][1900/2224] Elapsed 10m 17s (remain 1m 44s) Loss: 0.0070(0.0067) Grad: 20784.5820  LR: 0.00000248  \n","Epoch: [4][2000/2224] Elapsed 10m 49s (remain 1m 12s) Loss: 0.0011(0.0067) Grad: 16307.4277  LR: 0.00000230  \n","Epoch: [4][2100/2224] Elapsed 11m 22s (remain 0m 39s) Loss: 0.0001(0.0068) Grad: 1218.2540  LR: 0.00000212  \n","Epoch: [4][2200/2224] Elapsed 11m 54s (remain 0m 7s) Loss: 0.0038(0.0067) Grad: 45476.5273  LR: 0.00000195  \n","Epoch: [4][2223/2224] Elapsed 12m 2s (remain 0m 0s) Loss: 0.0048(0.0067) Grad: 54626.4219  LR: 0.00000191  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 13s) Loss: 0.0351(0.0351) \n","EVAL: [100/160] Elapsed 0m 19s (remain 0m 11s) Loss: 0.0000(0.0215) \n","EVAL: [159/160] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0001(0.0167) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.0067  avg_val_loss: 0.0167  time: 754s\n","Epoch 4 - Score: 0.8829\n","Epoch 4 - Save Best Score: 0.8829 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [5][0/2224] Elapsed 0m 0s (remain 31m 4s) Loss: 0.0044(0.0044) Grad: 9443.3633  LR: 0.00000191  \n","Epoch: [5][100/2224] Elapsed 0m 33s (remain 11m 52s) Loss: 0.0018(0.0042) Grad: 11325.2754  LR: 0.00000175  \n","Epoch: [5][200/2224] Elapsed 1m 6s (remain 11m 11s) Loss: 0.0003(0.0046) Grad: 2321.7937  LR: 0.00000159  \n","Epoch: [5][300/2224] Elapsed 1m 39s (remain 10m 35s) Loss: 0.0000(0.0049) Grad: 75.7893  LR: 0.00000144  \n","Epoch: [5][400/2224] Elapsed 2m 12s (remain 10m 1s) Loss: 0.0119(0.0051) Grad: 20028.9922  LR: 0.00000130  \n","Epoch: [5][500/2224] Elapsed 2m 45s (remain 9m 28s) Loss: 0.0230(0.0056) Grad: 28710.3867  LR: 0.00000116  \n","Epoch: [5][600/2224] Elapsed 3m 18s (remain 8m 55s) Loss: 0.0001(0.0056) Grad: 2130.5208  LR: 0.00000103  \n","Epoch: [5][700/2224] Elapsed 3m 51s (remain 8m 21s) Loss: 0.0027(0.0055) Grad: 8295.5938  LR: 0.00000091  \n","Epoch: [5][800/2224] Elapsed 4m 23s (remain 7m 48s) Loss: 0.0002(0.0055) Grad: 2091.4954  LR: 0.00000080  \n","Epoch: [5][900/2224] Elapsed 4m 56s (remain 7m 15s) Loss: 0.0011(0.0055) Grad: 7232.3950  LR: 0.00000069  \n","Epoch: [5][1000/2224] Elapsed 5m 29s (remain 6m 42s) Loss: 0.0046(0.0055) Grad: 19826.4453  LR: 0.00000059  \n","Epoch: [5][1100/2224] Elapsed 6m 2s (remain 6m 9s) Loss: 0.0007(0.0055) Grad: 3562.8337  LR: 0.00000050  \n","Epoch: [5][1200/2224] Elapsed 6m 35s (remain 5m 36s) Loss: 0.0078(0.0057) Grad: 51918.6523  LR: 0.00000041  \n","Epoch: [5][1300/2224] Elapsed 7m 7s (remain 5m 3s) Loss: 0.0019(0.0058) Grad: 13815.6865  LR: 0.00000034  \n","Epoch: [5][1400/2224] Elapsed 7m 40s (remain 4m 30s) Loss: 0.0029(0.0057) Grad: 11114.8125  LR: 0.00000027  \n","Epoch: [5][1500/2224] Elapsed 8m 13s (remain 3m 57s) Loss: 0.0054(0.0058) Grad: 9786.1230  LR: 0.00000021  \n","Epoch: [5][1600/2224] Elapsed 8m 46s (remain 3m 24s) Loss: 0.0002(0.0058) Grad: 1600.2382  LR: 0.00000015  \n","Epoch: [5][1700/2224] Elapsed 9m 19s (remain 2m 52s) Loss: 0.0000(0.0058) Grad: 119.8443  LR: 0.00000011  \n","Epoch: [5][1800/2224] Elapsed 9m 52s (remain 2m 19s) Loss: 0.0291(0.0057) Grad: 52837.3164  LR: 0.00000007  \n","Epoch: [5][1900/2224] Elapsed 10m 25s (remain 1m 46s) Loss: 0.0006(0.0058) Grad: 3125.1184  LR: 0.00000004  \n","Epoch: [5][2000/2224] Elapsed 10m 57s (remain 1m 13s) Loss: 0.0035(0.0057) Grad: 25038.2832  LR: 0.00000002  \n","Epoch: [5][2100/2224] Elapsed 11m 30s (remain 0m 40s) Loss: 0.0104(0.0057) Grad: 50526.9258  LR: 0.00000001  \n","Epoch: [5][2200/2224] Elapsed 12m 3s (remain 0m 7s) Loss: 0.0000(0.0057) Grad: 37.5459  LR: 0.00000000  \n","Epoch: [5][2223/2224] Elapsed 12m 10s (remain 0m 0s) Loss: 0.0000(0.0057) Grad: 106.6190  LR: 0.00000000  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 14s) Loss: 0.0409(0.0409) \n","EVAL: [100/160] Elapsed 0m 20s (remain 0m 11s) Loss: 0.0000(0.0224) \n","EVAL: [159/160] Elapsed 0m 31s (remain 0m 0s) Loss: 0.0001(0.0174) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.0057  avg_val_loss: 0.0174  time: 763s\n","Epoch 5 - Score: 0.8840\n","Epoch 5 - Save Best Score: 0.8840 Model\n","========== fold: 11 result ==========\n","Score: 0.8840\n","========== fold: 12 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2224] Elapsed 0m 0s (remain 23m 4s) Loss: 0.6175(0.6175) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2224] Elapsed 0m 33s (remain 11m 36s) Loss: 0.0314(0.0936) Grad: 4064.3442  LR: 0.00002000  \n","Epoch: [1][200/2224] Elapsed 1m 5s (remain 11m 0s) Loss: 0.0081(0.0628) Grad: 1694.1458  LR: 0.00001998  \n","Epoch: [1][300/2224] Elapsed 1m 38s (remain 10m 26s) Loss: 0.0278(0.0501) Grad: 2830.4585  LR: 0.00001996  \n","Epoch: [1][400/2224] Elapsed 2m 10s (remain 9m 52s) Loss: 0.0169(0.0438) Grad: 1875.0344  LR: 0.00001994  \n","Epoch: [1][500/2224] Elapsed 2m 42s (remain 9m 19s) Loss: 0.0257(0.0390) Grad: 2047.9187  LR: 0.00001990  \n","Epoch: [1][600/2224] Elapsed 3m 14s (remain 8m 46s) Loss: 0.0306(0.0358) Grad: 2606.3975  LR: 0.00001986  \n","Epoch: [1][700/2224] Elapsed 3m 47s (remain 8m 13s) Loss: 0.0059(0.0330) Grad: 1134.1270  LR: 0.00001980  \n","Epoch: [1][800/2224] Elapsed 4m 19s (remain 7m 41s) Loss: 0.0459(0.0311) Grad: 3726.1968  LR: 0.00001975  \n","Epoch: [1][900/2224] Elapsed 4m 51s (remain 7m 8s) Loss: 0.0021(0.0293) Grad: 270.0529  LR: 0.00001968  \n","Epoch: [1][1000/2224] Elapsed 5m 24s (remain 6m 36s) Loss: 0.0148(0.0280) Grad: 959.9708  LR: 0.00001960  \n","Epoch: [1][1100/2224] Elapsed 5m 56s (remain 6m 3s) Loss: 0.0217(0.0270) Grad: 5181.8506  LR: 0.00001952  \n","Epoch: [1][1200/2224] Elapsed 6m 29s (remain 5m 31s) Loss: 0.0074(0.0260) Grad: 649.8415  LR: 0.00001943  \n","Epoch: [1][1300/2224] Elapsed 7m 1s (remain 4m 59s) Loss: 0.0347(0.0252) Grad: 10452.4424  LR: 0.00001933  \n","Epoch: [1][1400/2224] Elapsed 7m 34s (remain 4m 26s) Loss: 0.0081(0.0245) Grad: 669.7719  LR: 0.00001923  \n","Epoch: [1][1500/2224] Elapsed 8m 6s (remain 3m 54s) Loss: 0.0187(0.0239) Grad: 945.7177  LR: 0.00001911  \n","Epoch: [1][1600/2224] Elapsed 8m 38s (remain 3m 21s) Loss: 0.0328(0.0234) Grad: 3454.7759  LR: 0.00001899  \n","Epoch: [1][1700/2224] Elapsed 9m 10s (remain 2m 49s) Loss: 0.0113(0.0229) Grad: 4256.7710  LR: 0.00001887  \n","Epoch: [1][1800/2224] Elapsed 9m 43s (remain 2m 16s) Loss: 0.0086(0.0225) Grad: 838.0711  LR: 0.00001873  \n","Epoch: [1][1900/2224] Elapsed 10m 15s (remain 1m 44s) Loss: 0.0041(0.0221) Grad: 611.1243  LR: 0.00001859  \n","Epoch: [1][2000/2224] Elapsed 10m 48s (remain 1m 12s) Loss: 0.0025(0.0216) Grad: 273.7315  LR: 0.00001844  \n","Epoch: [1][2100/2224] Elapsed 11m 20s (remain 0m 39s) Loss: 0.0220(0.0214) Grad: 4796.3745  LR: 0.00001829  \n","Epoch: [1][2200/2224] Elapsed 11m 52s (remain 0m 7s) Loss: 0.0070(0.0208) Grad: 1725.1935  LR: 0.00001813  \n","Epoch: [1][2223/2224] Elapsed 12m 0s (remain 0m 0s) Loss: 0.0011(0.0208) Grad: 283.8344  LR: 0.00001809  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 17s) Loss: 0.0059(0.0059) \n","EVAL: [100/160] Elapsed 0m 19s (remain 0m 11s) Loss: 0.0019(0.0118) \n","EVAL: [159/160] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0006(0.0111) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.0208  avg_val_loss: 0.0111  time: 752s\n","Epoch 1 - Score: 0.8843\n","Epoch 1 - Save Best Score: 0.8843 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [2][0/2224] Elapsed 0m 0s (remain 31m 5s) Loss: 0.0122(0.0122) Grad: 16148.9639  LR: 0.00001809  \n","Epoch: [2][100/2224] Elapsed 0m 33s (remain 11m 45s) Loss: 0.0028(0.0080) Grad: 5223.7759  LR: 0.00001792  \n","Epoch: [2][200/2224] Elapsed 1m 6s (remain 11m 4s) Loss: 0.0016(0.0086) Grad: 3632.3599  LR: 0.00001774  \n","Epoch: [2][300/2224] Elapsed 1m 38s (remain 10m 29s) Loss: 0.0092(0.0082) Grad: 15797.1719  LR: 0.00001756  \n","Epoch: [2][400/2224] Elapsed 2m 10s (remain 9m 55s) Loss: 0.0012(0.0085) Grad: 3176.2456  LR: 0.00001737  \n","Epoch: [2][500/2224] Elapsed 2m 43s (remain 9m 21s) Loss: 0.0005(0.0087) Grad: 3582.8120  LR: 0.00001718  \n","Epoch: [2][600/2224] Elapsed 3m 15s (remain 8m 48s) Loss: 0.0137(0.0087) Grad: 57826.7617  LR: 0.00001698  \n","Epoch: [2][700/2224] Elapsed 3m 48s (remain 8m 15s) Loss: 0.0004(0.0090) Grad: 1098.9423  LR: 0.00001678  \n","Epoch: [2][800/2224] Elapsed 4m 20s (remain 7m 43s) Loss: 0.0087(0.0094) Grad: 43758.8438  LR: 0.00001657  \n","Epoch: [2][900/2224] Elapsed 4m 53s (remain 7m 10s) Loss: 0.0026(0.0095) Grad: 5513.9751  LR: 0.00001635  \n","Epoch: [2][1000/2224] Elapsed 5m 25s (remain 6m 38s) Loss: 0.0046(0.0093) Grad: 32380.2637  LR: 0.00001613  \n","Epoch: [2][1100/2224] Elapsed 5m 58s (remain 6m 5s) Loss: 0.0113(0.0093) Grad: 20022.7754  LR: 0.00001590  \n","Epoch: [2][1200/2224] Elapsed 6m 30s (remain 5m 32s) Loss: 0.0007(0.0093) Grad: 3260.9084  LR: 0.00001567  \n","Epoch: [2][1300/2224] Elapsed 7m 2s (remain 4m 59s) Loss: 0.0106(0.0093) Grad: 23313.2676  LR: 0.00001544  \n","Epoch: [2][1400/2224] Elapsed 7m 35s (remain 4m 27s) Loss: 0.0017(0.0094) Grad: 4381.8857  LR: 0.00001520  \n","Epoch: [2][1500/2224] Elapsed 8m 7s (remain 3m 54s) Loss: 0.0069(0.0094) Grad: 21818.2051  LR: 0.00001496  \n","Epoch: [2][1600/2224] Elapsed 8m 40s (remain 3m 22s) Loss: 0.0068(0.0093) Grad: 9225.8779  LR: 0.00001471  \n","Epoch: [2][1700/2224] Elapsed 9m 12s (remain 2m 49s) Loss: 0.0016(0.0092) Grad: 14595.8857  LR: 0.00001446  \n","Epoch: [2][1800/2224] Elapsed 9m 45s (remain 2m 17s) Loss: 0.0086(0.0092) Grad: 17630.5469  LR: 0.00001420  \n","Epoch: [2][1900/2224] Elapsed 10m 17s (remain 1m 44s) Loss: 0.0043(0.0092) Grad: 10999.6201  LR: 0.00001394  \n","Epoch: [2][2000/2224] Elapsed 10m 49s (remain 1m 12s) Loss: 0.0001(0.0093) Grad: 495.8910  LR: 0.00001368  \n","Epoch: [2][2100/2224] Elapsed 11m 22s (remain 0m 39s) Loss: 0.0128(0.0092) Grad: 26234.1426  LR: 0.00001342  \n","Epoch: [2][2200/2224] Elapsed 11m 54s (remain 0m 7s) Loss: 0.0021(0.0091) Grad: 11718.8828  LR: 0.00001315  \n","Epoch: [2][2223/2224] Elapsed 12m 2s (remain 0m 0s) Loss: 0.0003(0.0091) Grad: 2957.7375  LR: 0.00001309  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 16s) Loss: 0.0074(0.0074) \n","EVAL: [100/160] Elapsed 0m 19s (remain 0m 11s) Loss: 0.0002(0.0137) \n","EVAL: [159/160] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0001(0.0124) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.0091  avg_val_loss: 0.0124  time: 754s\n","Epoch 2 - Score: 0.8923\n","Epoch 2 - Save Best Score: 0.8923 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [3][0/2224] Elapsed 0m 0s (remain 30m 15s) Loss: 0.0087(0.0087) Grad: 35622.9258  LR: 0.00001309  \n","Epoch: [3][100/2224] Elapsed 0m 33s (remain 11m 40s) Loss: 0.0054(0.0085) Grad: 8998.2871  LR: 0.00001282  \n","Epoch: [3][200/2224] Elapsed 1m 5s (remain 11m 1s) Loss: 0.0038(0.0076) Grad: 20267.0918  LR: 0.00001255  \n","Epoch: [3][300/2224] Elapsed 1m 38s (remain 10m 26s) Loss: 0.0106(0.0078) Grad: 16955.0664  LR: 0.00001227  \n","Epoch: [3][400/2224] Elapsed 2m 10s (remain 9m 52s) Loss: 0.0121(0.0078) Grad: 34958.8359  LR: 0.00001200  \n","Epoch: [3][500/2224] Elapsed 2m 42s (remain 9m 19s) Loss: 0.0208(0.0078) Grad: 41066.8320  LR: 0.00001172  \n","Epoch: [3][600/2224] Elapsed 3m 15s (remain 8m 46s) Loss: 0.0129(0.0076) Grad: 17905.2246  LR: 0.00001144  \n","Epoch: [3][700/2224] Elapsed 3m 47s (remain 8m 14s) Loss: 0.0174(0.0075) Grad: 29229.8691  LR: 0.00001116  \n","Epoch: [3][800/2224] Elapsed 4m 19s (remain 7m 41s) Loss: 0.0016(0.0074) Grad: 6687.3799  LR: 0.00001088  \n","Epoch: [3][900/2224] Elapsed 4m 52s (remain 7m 9s) Loss: 0.0014(0.0075) Grad: 5229.0801  LR: 0.00001060  \n","Epoch: [3][1000/2224] Elapsed 5m 24s (remain 6m 36s) Loss: 0.0071(0.0077) Grad: 14504.9561  LR: 0.00001031  \n","Epoch: [3][1100/2224] Elapsed 5m 57s (remain 6m 4s) Loss: 0.0964(0.0076) Grad: 85597.1094  LR: 0.00001003  \n","Epoch: [3][1200/2224] Elapsed 6m 29s (remain 5m 31s) Loss: 0.0010(0.0078) Grad: 8435.5420  LR: 0.00000975  \n","Epoch: [3][1300/2224] Elapsed 7m 2s (remain 4m 59s) Loss: 0.0026(0.0079) Grad: 9542.7432  LR: 0.00000947  \n","Epoch: [3][1400/2224] Elapsed 7m 34s (remain 4m 27s) Loss: 0.0010(0.0079) Grad: 6722.9507  LR: 0.00000918  \n","Epoch: [3][1500/2224] Elapsed 8m 7s (remain 3m 54s) Loss: 0.0022(0.0078) Grad: 11299.6523  LR: 0.00000890  \n","Epoch: [3][1600/2224] Elapsed 8m 39s (remain 3m 22s) Loss: 0.0257(0.0079) Grad: 42298.4883  LR: 0.00000862  \n","Epoch: [3][1700/2224] Elapsed 9m 12s (remain 2m 49s) Loss: 0.0022(0.0079) Grad: 9693.4922  LR: 0.00000834  \n","Epoch: [3][1800/2224] Elapsed 9m 44s (remain 2m 17s) Loss: 0.0080(0.0078) Grad: 38279.8711  LR: 0.00000807  \n","Epoch: [3][1900/2224] Elapsed 10m 17s (remain 1m 44s) Loss: 0.0086(0.0079) Grad: 15387.8584  LR: 0.00000779  \n","Epoch: [3][2000/2224] Elapsed 10m 49s (remain 1m 12s) Loss: 0.0075(0.0078) Grad: 26999.6543  LR: 0.00000751  \n","Epoch: [3][2100/2224] Elapsed 11m 21s (remain 0m 39s) Loss: 0.0000(0.0078) Grad: 331.9007  LR: 0.00000724  \n","Epoch: [3][2200/2224] Elapsed 11m 53s (remain 0m 7s) Loss: 0.0185(0.0078) Grad: 42281.7070  LR: 0.00000697  \n","Epoch: [3][2223/2224] Elapsed 12m 1s (remain 0m 0s) Loss: 0.0002(0.0077) Grad: 1510.8301  LR: 0.00000691  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 15s) Loss: 0.0108(0.0108) \n","EVAL: [100/160] Elapsed 0m 19s (remain 0m 11s) Loss: 0.0011(0.0146) \n","EVAL: [159/160] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0000(0.0132) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.0077  avg_val_loss: 0.0132  time: 753s\n","Epoch 3 - Score: 0.8942\n","Epoch 3 - Save Best Score: 0.8942 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [4][0/2224] Elapsed 0m 0s (remain 30m 28s) Loss: 0.0002(0.0002) Grad: 672.7072  LR: 0.00000691  \n","Epoch: [4][100/2224] Elapsed 0m 33s (remain 11m 43s) Loss: 0.0060(0.0046) Grad: 9095.0264  LR: 0.00000664  \n","Epoch: [4][200/2224] Elapsed 1m 5s (remain 11m 3s) Loss: 0.0002(0.0061) Grad: 648.3679  LR: 0.00000638  \n","Epoch: [4][300/2224] Elapsed 1m 38s (remain 10m 28s) Loss: 0.0001(0.0064) Grad: 437.2094  LR: 0.00000611  \n","Epoch: [4][400/2224] Elapsed 2m 10s (remain 9m 53s) Loss: 0.0005(0.0065) Grad: 3117.2551  LR: 0.00000585  \n","Epoch: [4][500/2224] Elapsed 2m 42s (remain 9m 20s) Loss: 0.0005(0.0066) Grad: 3283.3230  LR: 0.00000560  \n","Epoch: [4][600/2224] Elapsed 3m 15s (remain 8m 47s) Loss: 0.0025(0.0066) Grad: 16851.2539  LR: 0.00000535  \n","Epoch: [4][700/2224] Elapsed 3m 47s (remain 8m 14s) Loss: 0.0068(0.0068) Grad: 12514.7051  LR: 0.00000510  \n","Epoch: [4][800/2224] Elapsed 4m 19s (remain 7m 41s) Loss: 0.0077(0.0068) Grad: 19494.2715  LR: 0.00000485  \n","Epoch: [4][900/2224] Elapsed 4m 52s (remain 7m 9s) Loss: 0.0000(0.0069) Grad: 79.5127  LR: 0.00000461  \n","Epoch: [4][1000/2224] Elapsed 5m 24s (remain 6m 36s) Loss: 0.0022(0.0070) Grad: 10714.1543  LR: 0.00000438  \n","Epoch: [4][1100/2224] Elapsed 5m 57s (remain 6m 4s) Loss: 0.0058(0.0068) Grad: 19707.7715  LR: 0.00000415  \n","Epoch: [4][1200/2224] Elapsed 6m 29s (remain 5m 31s) Loss: 0.0002(0.0067) Grad: 1373.8468  LR: 0.00000392  \n","Epoch: [4][1300/2224] Elapsed 7m 1s (remain 4m 59s) Loss: 0.0001(0.0068) Grad: 152.9022  LR: 0.00000370  \n","Epoch: [4][1400/2224] Elapsed 7m 34s (remain 4m 26s) Loss: 0.0117(0.0069) Grad: 18265.1641  LR: 0.00000348  \n","Epoch: [4][1500/2224] Elapsed 8m 6s (remain 3m 54s) Loss: 0.0020(0.0068) Grad: 23420.4648  LR: 0.00000327  \n","Epoch: [4][1600/2224] Elapsed 8m 39s (remain 3m 22s) Loss: 0.0064(0.0068) Grad: 9921.6357  LR: 0.00000306  \n","Epoch: [4][1700/2224] Elapsed 9m 11s (remain 2m 49s) Loss: 0.0004(0.0067) Grad: 1643.1486  LR: 0.00000286  \n","Epoch: [4][1800/2224] Elapsed 9m 44s (remain 2m 17s) Loss: 0.0096(0.0066) Grad: 24153.3574  LR: 0.00000267  \n","Epoch: [4][1900/2224] Elapsed 10m 16s (remain 1m 44s) Loss: 0.0001(0.0066) Grad: 337.9368  LR: 0.00000248  \n","Epoch: [4][2000/2224] Elapsed 10m 49s (remain 1m 12s) Loss: 0.0013(0.0066) Grad: 12571.6904  LR: 0.00000230  \n","Epoch: [4][2100/2224] Elapsed 11m 21s (remain 0m 39s) Loss: 0.0008(0.0065) Grad: 12394.1533  LR: 0.00000212  \n","Epoch: [4][2200/2224] Elapsed 11m 53s (remain 0m 7s) Loss: 0.0303(0.0065) Grad: 65625.2578  LR: 0.00000195  \n","Epoch: [4][2223/2224] Elapsed 12m 1s (remain 0m 0s) Loss: 0.0001(0.0065) Grad: 11470.9121  LR: 0.00000191  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 13s) Loss: 0.0116(0.0116) \n","EVAL: [100/160] Elapsed 0m 19s (remain 0m 11s) Loss: 0.0004(0.0156) \n","EVAL: [159/160] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0000(0.0141) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.0065  avg_val_loss: 0.0141  time: 753s\n","Epoch 4 - Score: 0.8982\n","Epoch 4 - Save Best Score: 0.8982 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [5][0/2224] Elapsed 0m 0s (remain 30m 50s) Loss: 0.0027(0.0027) Grad: 19458.3789  LR: 0.00000191  \n","Epoch: [5][100/2224] Elapsed 0m 33s (remain 11m 43s) Loss: 0.0001(0.0049) Grad: 1226.1537  LR: 0.00000175  \n","Epoch: [5][200/2224] Elapsed 1m 5s (remain 11m 3s) Loss: 0.0005(0.0046) Grad: 3442.2039  LR: 0.00000159  \n","Epoch: [5][300/2224] Elapsed 1m 38s (remain 10m 27s) Loss: 0.0009(0.0043) Grad: 7695.5991  LR: 0.00000144  \n","Epoch: [5][400/2224] Elapsed 2m 10s (remain 9m 53s) Loss: 0.0005(0.0046) Grad: 6408.8774  LR: 0.00000130  \n","Epoch: [5][500/2224] Elapsed 2m 43s (remain 9m 20s) Loss: 0.0012(0.0046) Grad: 20247.8809  LR: 0.00000116  \n","Epoch: [5][600/2224] Elapsed 3m 15s (remain 8m 47s) Loss: 0.0045(0.0048) Grad: 23028.5664  LR: 0.00000103  \n","Epoch: [5][700/2224] Elapsed 3m 47s (remain 8m 14s) Loss: 0.0042(0.0050) Grad: 11104.8965  LR: 0.00000091  \n","Epoch: [5][800/2224] Elapsed 4m 20s (remain 7m 41s) Loss: 0.0001(0.0053) Grad: 250.8520  LR: 0.00000080  \n","Epoch: [5][900/2224] Elapsed 4m 52s (remain 7m 9s) Loss: 0.0154(0.0055) Grad: 38285.6328  LR: 0.00000069  \n","Epoch: [5][1000/2224] Elapsed 5m 24s (remain 6m 36s) Loss: 0.0000(0.0057) Grad: 277.3111  LR: 0.00000059  \n","Epoch: [5][1100/2224] Elapsed 5m 57s (remain 6m 4s) Loss: 0.0048(0.0057) Grad: 14969.4268  LR: 0.00000050  \n","Epoch: [5][1200/2224] Elapsed 6m 29s (remain 5m 31s) Loss: 0.0001(0.0058) Grad: 590.6437  LR: 0.00000041  \n","Epoch: [5][1300/2224] Elapsed 7m 2s (remain 4m 59s) Loss: 0.0011(0.0058) Grad: 4728.1284  LR: 0.00000034  \n","Epoch: [5][1400/2224] Elapsed 7m 34s (remain 4m 26s) Loss: 0.0000(0.0057) Grad: 45.2987  LR: 0.00000027  \n","Epoch: [5][1500/2224] Elapsed 8m 6s (remain 3m 54s) Loss: 0.0053(0.0057) Grad: 26332.3203  LR: 0.00000021  \n","Epoch: [5][1600/2224] Elapsed 8m 39s (remain 3m 22s) Loss: 0.0026(0.0057) Grad: 4758.5073  LR: 0.00000015  \n","Epoch: [5][1700/2224] Elapsed 9m 11s (remain 2m 49s) Loss: 0.0005(0.0056) Grad: 3407.4924  LR: 0.00000011  \n","Epoch: [5][1800/2224] Elapsed 9m 43s (remain 2m 17s) Loss: 0.0111(0.0055) Grad: 24607.4141  LR: 0.00000007  \n","Epoch: [5][1900/2224] Elapsed 10m 15s (remain 1m 44s) Loss: 0.0002(0.0056) Grad: 746.4709  LR: 0.00000004  \n","Epoch: [5][2000/2224] Elapsed 10m 48s (remain 1m 12s) Loss: 0.0029(0.0056) Grad: 18664.4863  LR: 0.00000002  \n","Epoch: [5][2100/2224] Elapsed 11m 20s (remain 0m 39s) Loss: 0.0006(0.0055) Grad: 5888.7510  LR: 0.00000001  \n","Epoch: [5][2200/2224] Elapsed 11m 53s (remain 0m 7s) Loss: 0.0069(0.0055) Grad: 44095.9961  LR: 0.00000000  \n","Epoch: [5][2223/2224] Elapsed 12m 0s (remain 0m 0s) Loss: 0.0000(0.0055) Grad: 259.7734  LR: 0.00000000  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 12s) Loss: 0.0115(0.0115) \n","EVAL: [100/160] Elapsed 0m 19s (remain 0m 11s) Loss: 0.0002(0.0156) \n","EVAL: [159/160] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0000(0.0141) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.0055  avg_val_loss: 0.0141  time: 752s\n","Epoch 5 - Score: 0.8985\n","Epoch 5 - Save Best Score: 0.8985 Model\n","========== fold: 12 result ==========\n","Score: 0.8985\n","========== fold: 13 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2224] Elapsed 0m 0s (remain 22m 43s) Loss: 1.2152(1.2152) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2224] Elapsed 0m 33s (remain 11m 37s) Loss: 0.0215(0.1328) Grad: 1510.9850  LR: 0.00002000  \n","Epoch: [1][200/2224] Elapsed 1m 5s (remain 11m 0s) Loss: 0.0300(0.0806) Grad: 3860.9302  LR: 0.00001998  \n","Epoch: [1][300/2224] Elapsed 1m 38s (remain 10m 26s) Loss: 0.0350(0.0634) Grad: 2598.8816  LR: 0.00001996  \n","Epoch: [1][400/2224] Elapsed 2m 10s (remain 9m 52s) Loss: 0.0016(0.0527) Grad: 250.3292  LR: 0.00001994  \n","Epoch: [1][500/2224] Elapsed 2m 42s (remain 9m 20s) Loss: 0.0375(0.0464) Grad: 2178.2556  LR: 0.00001990  \n","Epoch: [1][600/2224] Elapsed 3m 15s (remain 8m 48s) Loss: 0.0032(0.0418) Grad: 698.2263  LR: 0.00001986  \n","Epoch: [1][700/2224] Elapsed 3m 47s (remain 8m 15s) Loss: 0.0242(0.0386) Grad: 2296.9636  LR: 0.00001980  \n","Epoch: [1][800/2224] Elapsed 4m 20s (remain 7m 42s) Loss: 0.0097(0.0360) Grad: 1232.7939  LR: 0.00001975  \n","Epoch: [1][900/2224] Elapsed 4m 52s (remain 7m 9s) Loss: 0.0401(0.0339) Grad: 1508.0323  LR: 0.00001968  \n","Epoch: [1][1000/2224] Elapsed 5m 24s (remain 6m 36s) Loss: 0.0077(0.0321) Grad: 641.3434  LR: 0.00001960  \n","Epoch: [1][1100/2224] Elapsed 5m 57s (remain 6m 4s) Loss: 0.0282(0.0308) Grad: 2553.9861  LR: 0.00001952  \n","Epoch: [1][1200/2224] Elapsed 6m 29s (remain 5m 31s) Loss: 0.0432(0.0294) Grad: 5579.1846  LR: 0.00001943  \n","Epoch: [1][1300/2224] Elapsed 7m 2s (remain 4m 59s) Loss: 0.0079(0.0284) Grad: 522.6957  LR: 0.00001933  \n","Epoch: [1][1400/2224] Elapsed 7m 34s (remain 4m 26s) Loss: 0.0184(0.0274) Grad: 1412.8447  LR: 0.00001923  \n","Epoch: [1][1500/2224] Elapsed 8m 6s (remain 3m 54s) Loss: 0.0078(0.0264) Grad: 448.1876  LR: 0.00001911  \n","Epoch: [1][1600/2224] Elapsed 8m 38s (remain 3m 21s) Loss: 0.0139(0.0257) Grad: 580.2125  LR: 0.00001899  \n","Epoch: [1][1700/2224] Elapsed 9m 11s (remain 2m 49s) Loss: 0.0075(0.0251) Grad: 2256.2207  LR: 0.00001887  \n","Epoch: [1][1800/2224] Elapsed 9m 43s (remain 2m 17s) Loss: 0.0127(0.0246) Grad: 1716.1361  LR: 0.00001873  \n","Epoch: [1][1900/2224] Elapsed 10m 16s (remain 1m 44s) Loss: 0.0081(0.0239) Grad: 3752.7515  LR: 0.00001859  \n","Epoch: [1][2000/2224] Elapsed 10m 48s (remain 1m 12s) Loss: 0.0314(0.0233) Grad: 1435.8450  LR: 0.00001844  \n","Epoch: [1][2100/2224] Elapsed 11m 20s (remain 0m 39s) Loss: 0.0195(0.0228) Grad: 1806.1234  LR: 0.00001829  \n","Epoch: [1][2200/2224] Elapsed 11m 52s (remain 0m 7s) Loss: 0.0037(0.0224) Grad: 3355.0046  LR: 0.00001813  \n","Epoch: [1][2223/2224] Elapsed 12m 0s (remain 0m 0s) Loss: 0.0097(0.0222) Grad: 1682.3260  LR: 0.00001809  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 18s) Loss: 0.0220(0.0220) \n","EVAL: [100/160] Elapsed 0m 19s (remain 0m 11s) Loss: 0.0218(0.0161) \n","EVAL: [159/160] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0001(0.0142) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.0222  avg_val_loss: 0.0142  time: 752s\n","Epoch 1 - Score: 0.8573\n","Epoch 1 - Save Best Score: 0.8573 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [2][0/2224] Elapsed 0m 0s (remain 29m 40s) Loss: 0.0029(0.0029) Grad: 14755.1934  LR: 0.00001809  \n","Epoch: [2][100/2224] Elapsed 0m 33s (remain 11m 41s) Loss: 0.0049(0.0088) Grad: 7098.0518  LR: 0.00001792  \n","Epoch: [2][200/2224] Elapsed 1m 5s (remain 11m 3s) Loss: 0.0092(0.0088) Grad: 17171.1211  LR: 0.00001774  \n","Epoch: [2][300/2224] Elapsed 1m 38s (remain 10m 27s) Loss: 0.0009(0.0086) Grad: 4388.0273  LR: 0.00001756  \n","Epoch: [2][400/2224] Elapsed 2m 10s (remain 9m 53s) Loss: 0.0138(0.0089) Grad: 23663.3789  LR: 0.00001737  \n","Epoch: [2][500/2224] Elapsed 2m 43s (remain 9m 21s) Loss: 0.0001(0.0089) Grad: 199.5214  LR: 0.00001718  \n","Epoch: [2][600/2224] Elapsed 3m 15s (remain 8m 47s) Loss: 0.0017(0.0092) Grad: 10447.3438  LR: 0.00001698  \n","Epoch: [2][700/2224] Elapsed 3m 47s (remain 8m 15s) Loss: 0.0042(0.0094) Grad: 5304.7173  LR: 0.00001678  \n","Epoch: [2][800/2224] Elapsed 4m 20s (remain 7m 42s) Loss: 0.0032(0.0096) Grad: 5830.4663  LR: 0.00001657  \n","Epoch: [2][900/2224] Elapsed 4m 52s (remain 7m 9s) Loss: 0.0057(0.0097) Grad: 10331.2871  LR: 0.00001635  \n","Epoch: [2][1000/2224] Elapsed 5m 24s (remain 6m 37s) Loss: 0.0110(0.0096) Grad: 17495.5605  LR: 0.00001613  \n","Epoch: [2][1100/2224] Elapsed 5m 57s (remain 6m 4s) Loss: 0.0209(0.0095) Grad: 32340.0391  LR: 0.00001590  \n","Epoch: [2][1200/2224] Elapsed 6m 29s (remain 5m 31s) Loss: 0.0136(0.0096) Grad: 13993.6201  LR: 0.00001567  \n","Epoch: [2][1300/2224] Elapsed 7m 1s (remain 4m 59s) Loss: 0.0001(0.0094) Grad: 668.7036  LR: 0.00001544  \n","Epoch: [2][1400/2224] Elapsed 7m 34s (remain 4m 26s) Loss: 0.0060(0.0094) Grad: 14343.5293  LR: 0.00001520  \n","Epoch: [2][1500/2224] Elapsed 8m 6s (remain 3m 54s) Loss: 0.0105(0.0094) Grad: 21078.2773  LR: 0.00001496  \n","Epoch: [2][1600/2224] Elapsed 8m 39s (remain 3m 22s) Loss: 0.0108(0.0093) Grad: 11515.1855  LR: 0.00001471  \n","Epoch: [2][1700/2224] Elapsed 9m 11s (remain 2m 49s) Loss: 0.0000(0.0093) Grad: 237.7286  LR: 0.00001446  \n","Epoch: [2][1800/2224] Elapsed 9m 43s (remain 2m 17s) Loss: 0.0089(0.0093) Grad: 24214.5020  LR: 0.00001420  \n","Epoch: [2][1900/2224] Elapsed 10m 16s (remain 1m 44s) Loss: 0.0011(0.0093) Grad: 5023.8867  LR: 0.00001394  \n","Epoch: [2][2000/2224] Elapsed 10m 48s (remain 1m 12s) Loss: 0.0038(0.0092) Grad: 39261.4219  LR: 0.00001368  \n","Epoch: [2][2100/2224] Elapsed 11m 20s (remain 0m 39s) Loss: 0.0273(0.0092) Grad: 124440.1562  LR: 0.00001342  \n","Epoch: [2][2200/2224] Elapsed 11m 53s (remain 0m 7s) Loss: 0.1078(0.0093) Grad: 148626.0625  LR: 0.00001315  \n","Epoch: [2][2223/2224] Elapsed 12m 0s (remain 0m 0s) Loss: 0.0105(0.0093) Grad: 43595.9570  LR: 0.00001309  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 14s) Loss: 0.0296(0.0296) \n","EVAL: [100/160] Elapsed 0m 19s (remain 0m 11s) Loss: 0.0148(0.0157) \n","EVAL: [159/160] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0000(0.0133) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.0093  avg_val_loss: 0.0133  time: 753s\n","Epoch 2 - Score: 0.8783\n","Epoch 2 - Save Best Score: 0.8783 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [3][0/2224] Elapsed 0m 0s (remain 32m 20s) Loss: 0.0230(0.0230) Grad: 28215.5020  LR: 0.00001309  \n","Epoch: [3][100/2224] Elapsed 0m 33s (remain 11m 44s) Loss: 0.0117(0.0077) Grad: 42553.8555  LR: 0.00001282  \n","Epoch: [3][200/2224] Elapsed 1m 6s (remain 11m 4s) Loss: 0.0072(0.0076) Grad: 15660.7676  LR: 0.00001255  \n","Epoch: [3][300/2224] Elapsed 1m 38s (remain 10m 29s) Loss: 0.0007(0.0080) Grad: 4664.3169  LR: 0.00001227  \n","Epoch: [3][400/2224] Elapsed 2m 10s (remain 9m 54s) Loss: 0.0061(0.0079) Grad: 19756.8496  LR: 0.00001200  \n","Epoch: [3][500/2224] Elapsed 2m 43s (remain 9m 21s) Loss: 0.0090(0.0079) Grad: 10586.3721  LR: 0.00001172  \n","Epoch: [3][600/2224] Elapsed 3m 15s (remain 8m 48s) Loss: 0.0100(0.0077) Grad: 20021.8652  LR: 0.00001144  \n","Epoch: [3][700/2224] Elapsed 3m 48s (remain 8m 15s) Loss: 0.0054(0.0077) Grad: 17951.4668  LR: 0.00001116  \n","Epoch: [3][800/2224] Elapsed 4m 20s (remain 7m 42s) Loss: 0.0014(0.0078) Grad: 3885.7307  LR: 0.00001088  \n","Epoch: [3][900/2224] Elapsed 4m 52s (remain 7m 9s) Loss: 0.0089(0.0077) Grad: 28931.9648  LR: 0.00001060  \n","Epoch: [3][1000/2224] Elapsed 5m 25s (remain 6m 37s) Loss: 0.0012(0.0079) Grad: 6881.1919  LR: 0.00001031  \n","Epoch: [3][1100/2224] Elapsed 5m 57s (remain 6m 4s) Loss: 0.0002(0.0079) Grad: 961.1331  LR: 0.00001003  \n","Epoch: [3][1200/2224] Elapsed 6m 29s (remain 5m 32s) Loss: 0.0077(0.0079) Grad: 18832.3789  LR: 0.00000975  \n","Epoch: [3][1300/2224] Elapsed 7m 2s (remain 4m 59s) Loss: 0.0003(0.0079) Grad: 2236.8000  LR: 0.00000947  \n","Epoch: [3][1400/2224] Elapsed 7m 34s (remain 4m 27s) Loss: 0.0003(0.0078) Grad: 2325.4216  LR: 0.00000918  \n","Epoch: [3][1500/2224] Elapsed 8m 7s (remain 3m 54s) Loss: 0.0020(0.0078) Grad: 31299.7344  LR: 0.00000890  \n","Epoch: [3][1600/2224] Elapsed 8m 39s (remain 3m 22s) Loss: 0.0022(0.0080) Grad: 8806.8701  LR: 0.00000862  \n","Epoch: [3][1700/2224] Elapsed 9m 11s (remain 2m 49s) Loss: 0.0093(0.0080) Grad: 15868.0264  LR: 0.00000834  \n","Epoch: [3][1800/2224] Elapsed 9m 44s (remain 2m 17s) Loss: 0.0302(0.0079) Grad: 43053.3750  LR: 0.00000807  \n","Epoch: [3][1900/2224] Elapsed 10m 17s (remain 1m 44s) Loss: 0.0337(0.0081) Grad: 14236.0879  LR: 0.00000779  \n","Epoch: [3][2000/2224] Elapsed 10m 49s (remain 1m 12s) Loss: 0.0017(0.0080) Grad: 24198.7266  LR: 0.00000751  \n","Epoch: [3][2100/2224] Elapsed 11m 21s (remain 0m 39s) Loss: 0.0063(0.0082) Grad: 27246.0898  LR: 0.00000724  \n","Epoch: [3][2200/2224] Elapsed 11m 54s (remain 0m 7s) Loss: 0.0150(0.0082) Grad: 64508.6484  LR: 0.00000697  \n","Epoch: [3][2223/2224] Elapsed 12m 1s (remain 0m 0s) Loss: 0.0001(0.0082) Grad: 1012.3617  LR: 0.00000691  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 17s) Loss: 0.0312(0.0312) \n","EVAL: [100/160] Elapsed 0m 19s (remain 0m 11s) Loss: 0.0126(0.0152) \n","EVAL: [159/160] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0000(0.0129) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.0082  avg_val_loss: 0.0129  time: 753s\n","Epoch 3 - Score: 0.8830\n","Epoch 3 - Save Best Score: 0.8830 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [4][0/2224] Elapsed 0m 0s (remain 30m 47s) Loss: 0.0033(0.0033) Grad: 6369.1187  LR: 0.00000691  \n","Epoch: [4][100/2224] Elapsed 0m 33s (remain 11m 42s) Loss: 0.0205(0.0051) Grad: 26966.4121  LR: 0.00000664  \n","Epoch: [4][200/2224] Elapsed 1m 5s (remain 11m 1s) Loss: 0.0101(0.0057) Grad: 52695.7578  LR: 0.00000638  \n","Epoch: [4][300/2224] Elapsed 1m 38s (remain 10m 26s) Loss: 0.0054(0.0065) Grad: 26958.0527  LR: 0.00000611  \n","Epoch: [4][400/2224] Elapsed 2m 10s (remain 9m 53s) Loss: 0.0096(0.0069) Grad: 36402.2695  LR: 0.00000585  \n","Epoch: [4][500/2224] Elapsed 2m 42s (remain 9m 20s) Loss: 0.0355(0.0069) Grad: 139285.9531  LR: 0.00000560  \n","Epoch: [4][600/2224] Elapsed 3m 15s (remain 8m 47s) Loss: 0.0015(0.0072) Grad: 5026.1128  LR: 0.00000535  \n","Epoch: [4][700/2224] Elapsed 3m 47s (remain 8m 14s) Loss: 0.0136(0.0070) Grad: 73653.9062  LR: 0.00000510  \n","Epoch: [4][800/2224] Elapsed 4m 20s (remain 7m 42s) Loss: 0.0012(0.0070) Grad: 4152.5601  LR: 0.00000485  \n","Epoch: [4][900/2224] Elapsed 4m 52s (remain 7m 9s) Loss: 0.0001(0.0069) Grad: 1064.4133  LR: 0.00000461  \n","Epoch: [4][1000/2224] Elapsed 5m 24s (remain 6m 37s) Loss: 0.0006(0.0069) Grad: 10436.7139  LR: 0.00000438  \n","Epoch: [4][1100/2224] Elapsed 5m 57s (remain 6m 4s) Loss: 0.0013(0.0070) Grad: 5336.1450  LR: 0.00000415  \n","Epoch: [4][1200/2224] Elapsed 6m 29s (remain 5m 32s) Loss: 0.0004(0.0068) Grad: 2326.9814  LR: 0.00000392  \n","Epoch: [4][1300/2224] Elapsed 7m 2s (remain 4m 59s) Loss: 0.0034(0.0069) Grad: 20462.6699  LR: 0.00000370  \n","Epoch: [4][1400/2224] Elapsed 7m 34s (remain 4m 27s) Loss: 0.0003(0.0068) Grad: 1998.7219  LR: 0.00000348  \n","Epoch: [4][1500/2224] Elapsed 8m 7s (remain 3m 54s) Loss: 0.0021(0.0068) Grad: 16609.2637  LR: 0.00000327  \n","Epoch: [4][1600/2224] Elapsed 8m 39s (remain 3m 22s) Loss: 0.0001(0.0068) Grad: 149.0556  LR: 0.00000306  \n","Epoch: [4][1700/2224] Elapsed 9m 12s (remain 2m 49s) Loss: 0.0070(0.0068) Grad: 10497.0029  LR: 0.00000286  \n","Epoch: [4][1800/2224] Elapsed 9m 44s (remain 2m 17s) Loss: 0.0001(0.0068) Grad: 1529.8173  LR: 0.00000267  \n","Epoch: [4][1900/2224] Elapsed 10m 17s (remain 1m 44s) Loss: 0.0136(0.0068) Grad: 24978.5312  LR: 0.00000248  \n","Epoch: [4][2000/2224] Elapsed 10m 49s (remain 1m 12s) Loss: 0.0026(0.0068) Grad: 13984.2695  LR: 0.00000230  \n","Epoch: [4][2100/2224] Elapsed 11m 21s (remain 0m 39s) Loss: 0.0032(0.0067) Grad: 34447.6602  LR: 0.00000212  \n","Epoch: [4][2200/2224] Elapsed 11m 54s (remain 0m 7s) Loss: 0.0006(0.0067) Grad: 9036.8604  LR: 0.00000195  \n","Epoch: [4][2223/2224] Elapsed 12m 1s (remain 0m 0s) Loss: 0.0015(0.0067) Grad: 19207.3965  LR: 0.00000191  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 15s) Loss: 0.0182(0.0182) \n","EVAL: [100/160] Elapsed 0m 19s (remain 0m 11s) Loss: 0.0139(0.0175) \n","EVAL: [159/160] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0000(0.0153) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.0067  avg_val_loss: 0.0153  time: 753s\n","Epoch 4 - Score: 0.8821\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [5][0/2224] Elapsed 0m 0s (remain 30m 16s) Loss: 0.0000(0.0000) Grad: 114.3722  LR: 0.00000191  \n","Epoch: [5][100/2224] Elapsed 0m 33s (remain 11m 42s) Loss: 0.0135(0.0044) Grad: 19351.5078  LR: 0.00000175  \n","Epoch: [5][200/2224] Elapsed 1m 5s (remain 11m 3s) Loss: 0.0003(0.0048) Grad: 4376.0532  LR: 0.00000159  \n","Epoch: [5][300/2224] Elapsed 1m 38s (remain 10m 28s) Loss: 0.0007(0.0052) Grad: 10124.2588  LR: 0.00000144  \n","Epoch: [5][400/2224] Elapsed 2m 10s (remain 9m 54s) Loss: 0.0102(0.0053) Grad: 19191.3887  LR: 0.00000130  \n","Epoch: [5][500/2224] Elapsed 2m 43s (remain 9m 20s) Loss: 0.0043(0.0052) Grad: 16679.2109  LR: 0.00000116  \n","Epoch: [5][600/2224] Elapsed 3m 15s (remain 8m 47s) Loss: 0.0000(0.0049) Grad: 35.2018  LR: 0.00000103  \n","Epoch: [5][700/2224] Elapsed 3m 47s (remain 8m 15s) Loss: 0.0017(0.0052) Grad: 11676.1611  LR: 0.00000091  \n","Epoch: [5][800/2224] Elapsed 4m 20s (remain 7m 42s) Loss: 0.0000(0.0053) Grad: 99.4317  LR: 0.00000080  \n","Epoch: [5][900/2224] Elapsed 4m 52s (remain 7m 9s) Loss: 0.0099(0.0052) Grad: 23348.8105  LR: 0.00000069  \n","Epoch: [5][1000/2224] Elapsed 5m 25s (remain 6m 37s) Loss: 0.0045(0.0054) Grad: 26250.8535  LR: 0.00000059  \n","Epoch: [5][1100/2224] Elapsed 5m 57s (remain 6m 4s) Loss: 0.0017(0.0053) Grad: 13982.2246  LR: 0.00000050  \n","Epoch: [5][1200/2224] Elapsed 6m 29s (remain 5m 32s) Loss: 0.0009(0.0055) Grad: 4515.8232  LR: 0.00000041  \n","Epoch: [5][1300/2224] Elapsed 7m 2s (remain 4m 59s) Loss: 0.0281(0.0055) Grad: 53405.4844  LR: 0.00000034  \n","Epoch: [5][1400/2224] Elapsed 7m 34s (remain 4m 27s) Loss: 0.0011(0.0056) Grad: 5513.5796  LR: 0.00000027  \n","Epoch: [5][1500/2224] Elapsed 8m 7s (remain 3m 54s) Loss: 0.0008(0.0056) Grad: 4746.7349  LR: 0.00000021  \n","Epoch: [5][1600/2224] Elapsed 8m 39s (remain 3m 22s) Loss: 0.0340(0.0057) Grad: 32179.3086  LR: 0.00000015  \n","Epoch: [5][1700/2224] Elapsed 9m 12s (remain 2m 49s) Loss: 0.0000(0.0057) Grad: 118.6289  LR: 0.00000011  \n","Epoch: [5][1800/2224] Elapsed 9m 44s (remain 2m 17s) Loss: 0.0120(0.0057) Grad: 44442.0391  LR: 0.00000007  \n","Epoch: [5][1900/2224] Elapsed 10m 17s (remain 1m 44s) Loss: 0.0033(0.0056) Grad: 8379.4775  LR: 0.00000004  \n","Epoch: [5][2000/2224] Elapsed 10m 49s (remain 1m 12s) Loss: 0.0070(0.0056) Grad: 21389.8457  LR: 0.00000002  \n","Epoch: [5][2100/2224] Elapsed 11m 21s (remain 0m 39s) Loss: 0.0001(0.0056) Grad: 1413.0731  LR: 0.00000001  \n","Epoch: [5][2200/2224] Elapsed 11m 54s (remain 0m 7s) Loss: 0.0000(0.0056) Grad: 179.3178  LR: 0.00000000  \n","Epoch: [5][2223/2224] Elapsed 12m 1s (remain 0m 0s) Loss: 0.0004(0.0056) Grad: 3647.7544  LR: 0.00000000  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 20s) Loss: 0.0238(0.0238) \n","EVAL: [100/160] Elapsed 0m 19s (remain 0m 11s) Loss: 0.0166(0.0185) \n","EVAL: [159/160] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0000(0.0163) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.0056  avg_val_loss: 0.0163  time: 754s\n","Epoch 5 - Score: 0.8819\n","========== fold: 13 result ==========\n","Score: 0.8830\n","========== fold: 14 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2224] Elapsed 0m 0s (remain 21m 43s) Loss: 0.3825(0.3825) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2224] Elapsed 0m 32s (remain 11m 32s) Loss: 0.0266(0.0610) Grad: 2810.1838  LR: 0.00002000  \n","Epoch: [1][200/2224] Elapsed 1m 5s (remain 10m 57s) Loss: 0.0331(0.0445) Grad: 5015.8584  LR: 0.00001998  \n","Epoch: [1][300/2224] Elapsed 1m 37s (remain 10m 24s) Loss: 0.0200(0.0386) Grad: 6041.0322  LR: 0.00001996  \n","Epoch: [1][400/2224] Elapsed 2m 10s (remain 9m 51s) Loss: 0.0156(0.0348) Grad: 3585.6636  LR: 0.00001994  \n","Epoch: [1][500/2224] Elapsed 2m 42s (remain 9m 19s) Loss: 0.0359(0.0318) Grad: 12662.3525  LR: 0.00001990  \n","Epoch: [1][600/2224] Elapsed 3m 14s (remain 8m 46s) Loss: 0.0053(0.0294) Grad: 597.7941  LR: 0.00001986  \n","Epoch: [1][700/2224] Elapsed 3m 47s (remain 8m 13s) Loss: 0.0064(0.0285) Grad: 3369.5562  LR: 0.00001980  \n","Epoch: [1][800/2224] Elapsed 4m 19s (remain 7m 41s) Loss: 0.0092(0.0272) Grad: 7167.3149  LR: 0.00001975  \n","Epoch: [1][900/2224] Elapsed 4m 52s (remain 7m 8s) Loss: 0.0021(0.0259) Grad: 2082.1086  LR: 0.00001968  \n","Epoch: [1][1000/2224] Elapsed 5m 24s (remain 6m 36s) Loss: 0.0118(0.0246) Grad: 3105.0413  LR: 0.00001960  \n","Epoch: [1][1100/2224] Elapsed 5m 56s (remain 6m 3s) Loss: 0.0314(0.0243) Grad: 3571.9753  LR: 0.00001952  \n","Epoch: [1][1200/2224] Elapsed 6m 28s (remain 5m 31s) Loss: 0.0190(0.0236) Grad: 5055.0693  LR: 0.00001943  \n","Epoch: [1][1300/2224] Elapsed 7m 1s (remain 4m 59s) Loss: 0.0082(0.0229) Grad: 4443.4082  LR: 0.00001933  \n","Epoch: [1][1400/2224] Elapsed 7m 34s (remain 4m 26s) Loss: 0.0051(0.0223) Grad: 2406.5466  LR: 0.00001923  \n","Epoch: [1][1500/2224] Elapsed 8m 6s (remain 3m 54s) Loss: 0.0057(0.0220) Grad: 965.4711  LR: 0.00001911  \n","Epoch: [1][1600/2224] Elapsed 8m 38s (remain 3m 21s) Loss: 0.0080(0.0215) Grad: 4879.0791  LR: 0.00001899  \n","Epoch: [1][1700/2224] Elapsed 9m 11s (remain 2m 49s) Loss: 0.0009(0.0211) Grad: 299.9603  LR: 0.00001887  \n","Epoch: [1][1800/2224] Elapsed 9m 43s (remain 2m 17s) Loss: 0.0021(0.0208) Grad: 899.7094  LR: 0.00001873  \n","Epoch: [1][1900/2224] Elapsed 10m 15s (remain 1m 44s) Loss: 0.0053(0.0204) Grad: 899.3954  LR: 0.00001859  \n","Epoch: [1][2000/2224] Elapsed 10m 48s (remain 1m 12s) Loss: 0.0155(0.0202) Grad: 6300.3618  LR: 0.00001844  \n","Epoch: [1][2100/2224] Elapsed 11m 20s (remain 0m 39s) Loss: 0.0059(0.0198) Grad: 3280.2695  LR: 0.00001829  \n","Epoch: [1][2200/2224] Elapsed 11m 53s (remain 0m 7s) Loss: 0.0047(0.0194) Grad: 1893.8182  LR: 0.00001813  \n","Epoch: [1][2223/2224] Elapsed 12m 0s (remain 0m 0s) Loss: 0.0037(0.0194) Grad: 2807.6831  LR: 0.00001809  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 17s) Loss: 0.0627(0.0627) \n","EVAL: [100/160] Elapsed 0m 19s (remain 0m 11s) Loss: 0.0056(0.0141) \n","EVAL: [159/160] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0306(0.0119) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.0194  avg_val_loss: 0.0119  time: 753s\n","Epoch 1 - Score: 0.8820\n","Epoch 1 - Save Best Score: 0.8820 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [2][0/2224] Elapsed 0m 0s (remain 30m 57s) Loss: 0.0002(0.0002) Grad: 911.6697  LR: 0.00001809  \n","Epoch: [2][100/2224] Elapsed 0m 33s (remain 11m 46s) Loss: 0.0028(0.0079) Grad: 6269.3086  LR: 0.00001792  \n","Epoch: [2][200/2224] Elapsed 1m 6s (remain 11m 4s) Loss: 0.0084(0.0097) Grad: 7140.8037  LR: 0.00001774  \n","Epoch: [2][300/2224] Elapsed 1m 38s (remain 10m 29s) Loss: 0.0382(0.0092) Grad: 95927.0625  LR: 0.00001756  \n","Epoch: [2][400/2224] Elapsed 2m 10s (remain 9m 54s) Loss: 0.0067(0.0093) Grad: 10702.0391  LR: 0.00001737  \n","Epoch: [2][500/2224] Elapsed 2m 43s (remain 9m 21s) Loss: 0.0054(0.0095) Grad: 6200.0239  LR: 0.00001718  \n","Epoch: [2][600/2224] Elapsed 3m 15s (remain 8m 48s) Loss: 0.0002(0.0094) Grad: 972.5128  LR: 0.00001698  \n","Epoch: [2][700/2224] Elapsed 3m 48s (remain 8m 15s) Loss: 0.0343(0.0096) Grad: 64459.0391  LR: 0.00001678  \n","Epoch: [2][800/2224] Elapsed 4m 20s (remain 7m 42s) Loss: 0.0016(0.0094) Grad: 6659.9863  LR: 0.00001657  \n","Epoch: [2][900/2224] Elapsed 4m 52s (remain 7m 10s) Loss: 0.0056(0.0094) Grad: 8568.8301  LR: 0.00001635  \n","Epoch: [2][1000/2224] Elapsed 5m 25s (remain 6m 37s) Loss: 0.0044(0.0095) Grad: 10991.7158  LR: 0.00001613  \n","Epoch: [2][1100/2224] Elapsed 5m 58s (remain 6m 5s) Loss: 0.0001(0.0093) Grad: 200.7806  LR: 0.00001590  \n","Epoch: [2][1200/2224] Elapsed 6m 30s (remain 5m 32s) Loss: 0.0207(0.0093) Grad: 27454.1973  LR: 0.00001567  \n","Epoch: [2][1300/2224] Elapsed 7m 2s (remain 4m 59s) Loss: 0.0047(0.0094) Grad: 52786.5430  LR: 0.00001544  \n","Epoch: [2][1400/2224] Elapsed 7m 35s (remain 4m 27s) Loss: 0.0552(0.0094) Grad: 69021.9688  LR: 0.00001520  \n","Epoch: [2][1500/2224] Elapsed 8m 7s (remain 3m 54s) Loss: 0.0026(0.0094) Grad: 5109.3970  LR: 0.00001496  \n","Epoch: [2][1600/2224] Elapsed 8m 39s (remain 3m 22s) Loss: 0.0061(0.0095) Grad: 17137.1406  LR: 0.00001471  \n","Epoch: [2][1700/2224] Elapsed 9m 12s (remain 2m 49s) Loss: 0.0015(0.0095) Grad: 6705.6787  LR: 0.00001446  \n","Epoch: [2][1800/2224] Elapsed 9m 44s (remain 2m 17s) Loss: 0.0019(0.0095) Grad: 7808.9985  LR: 0.00001420  \n","Epoch: [2][1900/2224] Elapsed 10m 17s (remain 1m 44s) Loss: 0.0000(0.0095) Grad: 113.2586  LR: 0.00001394  \n","Epoch: [2][2000/2224] Elapsed 10m 49s (remain 1m 12s) Loss: 0.0004(0.0094) Grad: 2825.8374  LR: 0.00001368  \n","Epoch: [2][2100/2224] Elapsed 11m 21s (remain 0m 39s) Loss: 0.0043(0.0095) Grad: 21844.5859  LR: 0.00001342  \n","Epoch: [2][2200/2224] Elapsed 11m 54s (remain 0m 7s) Loss: 0.0146(0.0095) Grad: 29877.4102  LR: 0.00001315  \n","Epoch: [2][2223/2224] Elapsed 12m 1s (remain 0m 0s) Loss: 0.0066(0.0095) Grad: 11775.0449  LR: 0.00001309  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 16s) Loss: 0.0057(0.0057) \n","EVAL: [100/160] Elapsed 0m 19s (remain 0m 11s) Loss: 0.0041(0.0129) \n","EVAL: [159/160] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0345(0.0112) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.0095  avg_val_loss: 0.0112  time: 754s\n","Epoch 2 - Score: 0.8907\n","Epoch 2 - Save Best Score: 0.8907 Model\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [3][0/2224] Elapsed 0m 0s (remain 31m 25s) Loss: 0.0537(0.0537) Grad: 49428.4102  LR: 0.00001309  \n","Epoch: [3][100/2224] Elapsed 0m 33s (remain 11m 44s) Loss: 0.0122(0.0070) Grad: 30498.0098  LR: 0.00001282  \n","Epoch: [3][200/2224] Elapsed 1m 6s (remain 11m 4s) Loss: 0.0216(0.0070) Grad: 45566.0820  LR: 0.00001255  \n","Epoch: [3][300/2224] Elapsed 1m 38s (remain 10m 28s) Loss: 0.0001(0.0068) Grad: 511.1197  LR: 0.00001227  \n","Epoch: [3][400/2224] Elapsed 2m 10s (remain 9m 54s) Loss: 0.0025(0.0067) Grad: 4226.2520  LR: 0.00001200  \n","Epoch: [3][500/2224] Elapsed 2m 43s (remain 9m 20s) Loss: 0.0000(0.0070) Grad: 84.7068  LR: 0.00001172  \n","Epoch: [3][600/2224] Elapsed 3m 15s (remain 8m 47s) Loss: 0.0003(0.0075) Grad: 3125.7847  LR: 0.00001144  \n","Epoch: [3][700/2224] Elapsed 3m 47s (remain 8m 14s) Loss: 0.0035(0.0077) Grad: 12387.2988  LR: 0.00001116  \n","Epoch: [3][800/2224] Elapsed 4m 20s (remain 7m 42s) Loss: 0.0055(0.0077) Grad: 13968.6719  LR: 0.00001088  \n","Epoch: [3][900/2224] Elapsed 4m 52s (remain 7m 9s) Loss: 0.0046(0.0074) Grad: 21048.4219  LR: 0.00001060  \n","Epoch: [3][1000/2224] Elapsed 5m 25s (remain 6m 37s) Loss: 0.0077(0.0075) Grad: 13065.6113  LR: 0.00001031  \n","Epoch: [3][1100/2224] Elapsed 5m 57s (remain 6m 4s) Loss: 0.0001(0.0075) Grad: 329.9749  LR: 0.00001003  \n","Epoch: [3][1200/2224] Elapsed 6m 30s (remain 5m 32s) Loss: 0.0019(0.0076) Grad: 8407.9404  LR: 0.00000975  \n","Epoch: [3][1300/2224] Elapsed 7m 2s (remain 4m 59s) Loss: 0.0058(0.0076) Grad: 4882.4297  LR: 0.00000947  \n","Epoch: [3][1400/2224] Elapsed 7m 35s (remain 4m 27s) Loss: 0.0023(0.0076) Grad: 10449.6328  LR: 0.00000918  \n","Epoch: [3][1500/2224] Elapsed 8m 7s (remain 3m 54s) Loss: 0.0374(0.0076) Grad: 50885.7500  LR: 0.00000890  \n","Epoch: [3][1600/2224] Elapsed 8m 40s (remain 3m 22s) Loss: 0.0043(0.0076) Grad: 15543.8203  LR: 0.00000862  \n","Epoch: [3][1700/2224] Elapsed 9m 12s (remain 2m 49s) Loss: 0.0032(0.0077) Grad: 8433.5986  LR: 0.00000834  \n","Epoch: [3][1800/2224] Elapsed 9m 45s (remain 2m 17s) Loss: 0.0066(0.0076) Grad: 19772.9746  LR: 0.00000807  \n","Epoch: [3][1900/2224] Elapsed 10m 17s (remain 1m 44s) Loss: 0.0045(0.0076) Grad: 10198.6816  LR: 0.00000779  \n","Epoch: [3][2000/2224] Elapsed 10m 50s (remain 1m 12s) Loss: 0.0136(0.0076) Grad: 31109.0137  LR: 0.00000751  \n","Epoch: [3][2100/2224] Elapsed 11m 22s (remain 0m 39s) Loss: 0.0087(0.0076) Grad: 39648.5508  LR: 0.00000724  \n","Epoch: [3][2200/2224] Elapsed 11m 54s (remain 0m 7s) Loss: 0.0004(0.0077) Grad: 7280.2002  LR: 0.00000697  \n","Epoch: [3][2223/2224] Elapsed 12m 2s (remain 0m 0s) Loss: 0.0000(0.0077) Grad: 226.0641  LR: 0.00000691  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 19s) Loss: 0.0033(0.0033) \n","EVAL: [100/160] Elapsed 0m 19s (remain 0m 11s) Loss: 0.0033(0.0149) \n","EVAL: [159/160] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0394(0.0132) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.0077  avg_val_loss: 0.0132  time: 754s\n","Epoch 3 - Score: 0.8837\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [4][0/2224] Elapsed 0m 0s (remain 31m 4s) Loss: 0.0000(0.0000) Grad: 115.4247  LR: 0.00000691  \n","Epoch: [4][100/2224] Elapsed 0m 33s (remain 11m 41s) Loss: 0.0026(0.0056) Grad: 21954.8770  LR: 0.00000664  \n","Epoch: [4][200/2224] Elapsed 1m 5s (remain 11m 3s) Loss: 0.0002(0.0060) Grad: 1704.7546  LR: 0.00000638  \n","Epoch: [4][300/2224] Elapsed 1m 38s (remain 10m 28s) Loss: 0.0002(0.0057) Grad: 586.8969  LR: 0.00000611  \n","Epoch: [4][400/2224] Elapsed 2m 10s (remain 9m 54s) Loss: 0.0114(0.0054) Grad: 15587.0947  LR: 0.00000585  \n","Epoch: [4][500/2224] Elapsed 2m 43s (remain 9m 21s) Loss: 0.0001(0.0058) Grad: 1219.4458  LR: 0.00000560  \n","Epoch: [4][600/2224] Elapsed 3m 15s (remain 8m 48s) Loss: 0.0154(0.0061) Grad: 14062.3457  LR: 0.00000535  \n","Epoch: [4][700/2224] Elapsed 3m 48s (remain 8m 15s) Loss: 0.0004(0.0063) Grad: 1536.2158  LR: 0.00000510  \n","Epoch: [4][800/2224] Elapsed 4m 20s (remain 7m 43s) Loss: 0.0009(0.0063) Grad: 7904.5869  LR: 0.00000485  \n","Epoch: [4][900/2224] Elapsed 4m 53s (remain 7m 10s) Loss: 0.0013(0.0063) Grad: 8479.2480  LR: 0.00000461  \n","Epoch: [4][1000/2224] Elapsed 5m 25s (remain 6m 37s) Loss: 0.0001(0.0063) Grad: 282.6805  LR: 0.00000438  \n","Epoch: [4][1100/2224] Elapsed 5m 57s (remain 6m 5s) Loss: 0.0107(0.0063) Grad: 43817.3203  LR: 0.00000415  \n","Epoch: [4][1200/2224] Elapsed 6m 30s (remain 5m 32s) Loss: 0.0000(0.0064) Grad: 38.1289  LR: 0.00000392  \n","Epoch: [4][1300/2224] Elapsed 7m 2s (remain 4m 59s) Loss: 0.0008(0.0064) Grad: 2953.8850  LR: 0.00000370  \n","Epoch: [4][1400/2224] Elapsed 7m 35s (remain 4m 27s) Loss: 0.0348(0.0063) Grad: 74583.6484  LR: 0.00000348  \n","Epoch: [4][1500/2224] Elapsed 8m 7s (remain 3m 55s) Loss: 0.0024(0.0063) Grad: 19336.9453  LR: 0.00000327  \n","Epoch: [4][1600/2224] Elapsed 8m 40s (remain 3m 22s) Loss: 0.0116(0.0062) Grad: 5658.8906  LR: 0.00000306  \n","Epoch: [4][1700/2224] Elapsed 9m 12s (remain 2m 49s) Loss: 0.0019(0.0062) Grad: 22195.3848  LR: 0.00000286  \n","Epoch: [4][1800/2224] Elapsed 9m 44s (remain 2m 17s) Loss: 0.0206(0.0061) Grad: 118703.2500  LR: 0.00000267  \n","Epoch: [4][1900/2224] Elapsed 10m 17s (remain 1m 44s) Loss: 0.0046(0.0062) Grad: 5266.3760  LR: 0.00000248  \n","Epoch: [4][2000/2224] Elapsed 10m 49s (remain 1m 12s) Loss: 0.0029(0.0062) Grad: 38374.5781  LR: 0.00000230  \n","Epoch: [4][2100/2224] Elapsed 11m 22s (remain 0m 39s) Loss: 0.0026(0.0062) Grad: 22705.6445  LR: 0.00000212  \n","Epoch: [4][2200/2224] Elapsed 11m 54s (remain 0m 7s) Loss: 0.0102(0.0062) Grad: 101871.1172  LR: 0.00000195  \n","Epoch: [4][2223/2224] Elapsed 12m 1s (remain 0m 0s) Loss: 0.0092(0.0062) Grad: 43068.3711  LR: 0.00000191  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 18s) Loss: 0.0032(0.0032) \n","EVAL: [100/160] Elapsed 0m 19s (remain 0m 11s) Loss: 0.0042(0.0162) \n","EVAL: [159/160] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0392(0.0143) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.0062  avg_val_loss: 0.0143  time: 754s\n","Epoch 4 - Score: 0.8870\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [5][0/2224] Elapsed 0m 0s (remain 31m 3s) Loss: 0.0007(0.0007) Grad: 5201.3887  LR: 0.00000191  \n","Epoch: [5][100/2224] Elapsed 0m 33s (remain 11m 39s) Loss: 0.0227(0.0054) Grad: 30454.0703  LR: 0.00000175  \n","Epoch: [5][200/2224] Elapsed 1m 5s (remain 11m 1s) Loss: 0.0001(0.0049) Grad: 634.5536  LR: 0.00000159  \n","Epoch: [5][300/2224] Elapsed 1m 38s (remain 10m 26s) Loss: 0.0045(0.0049) Grad: 13825.3828  LR: 0.00000144  \n","Epoch: [5][400/2224] Elapsed 2m 10s (remain 9m 52s) Loss: 0.0064(0.0044) Grad: 8060.3784  LR: 0.00000130  \n","Epoch: [5][500/2224] Elapsed 2m 42s (remain 9m 20s) Loss: 0.0017(0.0044) Grad: 18293.2832  LR: 0.00000116  \n","Epoch: [5][600/2224] Elapsed 3m 15s (remain 8m 47s) Loss: 0.0027(0.0048) Grad: 23046.1133  LR: 0.00000103  \n","Epoch: [5][700/2224] Elapsed 3m 47s (remain 8m 14s) Loss: 0.0092(0.0048) Grad: 13536.3223  LR: 0.00000091  \n","Epoch: [5][800/2224] Elapsed 4m 20s (remain 7m 41s) Loss: 0.0039(0.0046) Grad: 33195.9570  LR: 0.00000080  \n","Epoch: [5][900/2224] Elapsed 4m 52s (remain 7m 9s) Loss: 0.0000(0.0047) Grad: 101.5806  LR: 0.00000069  \n","Epoch: [5][1000/2224] Elapsed 5m 24s (remain 6m 36s) Loss: 0.0020(0.0047) Grad: 11780.6543  LR: 0.00000059  \n","Epoch: [5][1100/2224] Elapsed 5m 57s (remain 6m 4s) Loss: 0.0267(0.0046) Grad: 120672.0469  LR: 0.00000050  \n","Epoch: [5][1200/2224] Elapsed 6m 29s (remain 5m 32s) Loss: 0.0113(0.0047) Grad: 51119.5352  LR: 0.00000041  \n","Epoch: [5][1300/2224] Elapsed 7m 2s (remain 4m 59s) Loss: 0.0074(0.0048) Grad: 28546.6367  LR: 0.00000034  \n","Epoch: [5][1400/2224] Elapsed 7m 34s (remain 4m 27s) Loss: 0.0279(0.0049) Grad: 78338.0234  LR: 0.00000027  \n","Epoch: [5][1500/2224] Elapsed 8m 7s (remain 3m 54s) Loss: 0.0590(0.0049) Grad: 50916.3008  LR: 0.00000021  \n","Epoch: [5][1600/2224] Elapsed 8m 39s (remain 3m 22s) Loss: 0.0445(0.0049) Grad: 31134.7246  LR: 0.00000015  \n","Epoch: [5][1700/2224] Elapsed 9m 12s (remain 2m 49s) Loss: 0.0030(0.0051) Grad: 8673.7988  LR: 0.00000011  \n","Epoch: [5][1800/2224] Elapsed 9m 44s (remain 2m 17s) Loss: 0.0001(0.0050) Grad: 462.9604  LR: 0.00000007  \n","Epoch: [5][1900/2224] Elapsed 10m 17s (remain 1m 44s) Loss: 0.0141(0.0050) Grad: 68892.1562  LR: 0.00000004  \n","Epoch: [5][2000/2224] Elapsed 10m 49s (remain 1m 12s) Loss: 0.0151(0.0050) Grad: 94131.4297  LR: 0.00000002  \n","Epoch: [5][2100/2224] Elapsed 11m 22s (remain 0m 39s) Loss: 0.0006(0.0051) Grad: 9139.8262  LR: 0.00000001  \n","Epoch: [5][2200/2224] Elapsed 11m 54s (remain 0m 7s) Loss: 0.0102(0.0051) Grad: 41117.0430  LR: 0.00000000  \n","Epoch: [5][2223/2224] Elapsed 12m 2s (remain 0m 0s) Loss: 0.0107(0.0051) Grad: 26267.6055  LR: 0.00000000  \n","EVAL: [0/160] Elapsed 0m 0s (remain 1m 17s) Loss: 0.0034(0.0034) \n","EVAL: [100/160] Elapsed 0m 19s (remain 0m 11s) Loss: 0.0028(0.0174) \n","EVAL: [159/160] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0396(0.0154) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.0051  avg_val_loss: 0.0154  time: 754s\n","Epoch 5 - Score: 0.8850\n","========== fold: 14 result ==========\n","Score: 0.8907\n","========== CV ==========\n","Score: 0.8847\n"]}],"source":["if CFG.train:\n","    oof_df = pd.DataFrame()\n","    for fold in range(CFG.n_fold):\n","        if fold in CFG.trn_fold:\n","            _oof_df = train_loop(train, fold)\n","            oof_df = pd.concat([oof_df, _oof_df])\n","            LOGGER.info(f\"========== fold: {fold} result ==========\")\n","            get_result(_oof_df)\n","    oof_df = oof_df.reset_index(drop=True)\n","    LOGGER.info(f\"========== CV ==========\")\n","    get_result(oof_df)\n","    oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rTZw09JT0N_N"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"referenced_widgets":["80329cad93844d01b40a09e76d2231af"]},"id":"4rR3p2rHRYpN","outputId":"02c18e6c-5f5c-4105-cecd-5a786c05d0a3"},"outputs":[{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"80329cad93844d01b40a09e76d2231af","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>[fold0] avg_train_loss</td><td>█▃▂▂▁</td></tr><tr><td>[fold0] avg_val_loss</td><td>▁▂▅▆█</td></tr><tr><td>[fold0] epoch</td><td>▁▃▅▆█</td></tr><tr><td>[fold0] loss</td><td>▃▇▂▃▃▃▂█▁▁▃▁▁▅▄▂▁▁▁▂▂▁▁▁▁▃▁▁▁▁▂▃▂▂▂▁▂▂█▂</td></tr><tr><td>[fold0] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold0] score</td><td>▁▅█▇▆</td></tr><tr><td>[fold10] avg_train_loss</td><td>█▃▂▂▁</td></tr><tr><td>[fold10] avg_val_loss</td><td>▁▂▄▅█</td></tr><tr><td>[fold10] epoch</td><td>▁▃▅▆█</td></tr><tr><td>[fold10] loss</td><td>▅█▃▁▂▁▁▂▁▁▃▂▂▁▂▃▁▅▃▆▂▄▁▃▁▂▃▁▁▁▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>[fold10] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold10] score</td><td>▁▅▄█▆</td></tr><tr><td>[fold11] avg_train_loss</td><td>█▂▂▁▁</td></tr><tr><td>[fold11] avg_val_loss</td><td>▁▂▄▆█</td></tr><tr><td>[fold11] epoch</td><td>▁▃▅▆█</td></tr><tr><td>[fold11] loss</td><td>▃▂▃█▄▄▁▂▁▁▁▃▄▁▁▅▁▃▂▁▁▁▁▁▂▁▁▂▁▁▁▁▃▁▂▁▁▁▁▁</td></tr><tr><td>[fold11] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold11] score</td><td>▁▆▇██</td></tr><tr><td>[fold12] avg_train_loss</td><td>█▃▂▁▁</td></tr><tr><td>[fold12] avg_val_loss</td><td>▁▄▆██</td></tr><tr><td>[fold12] epoch</td><td>▁▃▅▆█</td></tr><tr><td>[fold12] loss</td><td>█▂▃▁▂▂▅▂▁▁▃▁▄▁▂▂▂▁▁▁▆▄▂▂▁▃▁▁▁▂▂▁▂▁▁▄▁▂█▁</td></tr><tr><td>[fold12] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold12] score</td><td>▁▅▆██</td></tr><tr><td>[fold13] avg_train_loss</td><td>█▃▂▁▁</td></tr><tr><td>[fold13] avg_val_loss</td><td>▄▂▁▆█</td></tr><tr><td>[fold13] epoch</td><td>▁▃▅▆█</td></tr><tr><td>[fold13] loss</td><td>█▂▄▂▁▄▆▂▃▂▂▃▁▁▁▃▁▁▂▁▁▁▂▃▇▁▃▂▁▄▁▂▁▁▁▁▁▁▁▂</td></tr><tr><td>[fold13] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold13] score</td><td>▁▇███</td></tr><tr><td>[fold14] avg_train_loss</td><td>█▃▂▂▁</td></tr><tr><td>[fold14] avg_val_loss</td><td>▂▁▄▆█</td></tr><tr><td>[fold14] epoch</td><td>▁▃▅▆█</td></tr><tr><td>[fold14] loss</td><td>█▃▄▃▂▂▆▂▁▃▁▂▁▁▁▂▁▁▁▂▁▁▁▂▁▁▁▂▃▁▂▁▁▁▁▁▁▃▁▁</td></tr><tr><td>[fold14] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold14] score</td><td>▁█▂▅▃</td></tr><tr><td>[fold1] avg_train_loss</td><td>█▃▂▂▁</td></tr><tr><td>[fold1] avg_val_loss</td><td>▂▂▁▅█</td></tr><tr><td>[fold1] epoch</td><td>▁▃▅▆█</td></tr><tr><td>[fold1] loss</td><td>▃█▄▁▂▂▃▁▂▁▂▅▁▁▁▃▁▄▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▂▂▁▁▂</td></tr><tr><td>[fold1] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold1] score</td><td>▁▃▇██</td></tr><tr><td>[fold2] avg_train_loss</td><td>█▃▂▂▁</td></tr><tr><td>[fold2] avg_val_loss</td><td>▁▃▅▇█</td></tr><tr><td>[fold2] epoch</td><td>▁▃▅▆█</td></tr><tr><td>[fold2] loss</td><td>█▇█▃▂▁▂▂▁▂▂▃▁▂▁▃▁▇▁▁▁▁▁▁▂▁▁▁▂▃▄▁▁▃▂▂▂▂▁▁</td></tr><tr><td>[fold2] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold2] score</td><td>▁▄▇▇█</td></tr><tr><td>[fold3] avg_train_loss</td><td>█▃▂▁▁</td></tr><tr><td>[fold3] avg_val_loss</td><td>▃▁▄▇█</td></tr><tr><td>[fold3] epoch</td><td>▁▃▅▆█</td></tr><tr><td>[fold3] loss</td><td>█▃▂▂▇▂▁▂▁▂▁▁▁▁▁▂▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁</td></tr><tr><td>[fold3] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold3] score</td><td>▁██▆▇</td></tr><tr><td>[fold4] avg_train_loss</td><td>█▃▂▂▁</td></tr><tr><td>[fold4] avg_val_loss</td><td>▁▂▅▅█</td></tr><tr><td>[fold4] epoch</td><td>▁▃▅▆█</td></tr><tr><td>[fold4] loss</td><td>▃▅▃▄▃▇▁▂▃▃█▆▃▁▁▂▁▁▁▂▁▁▂▁▁▁▄▁▂▃▅▁▁▁▁▁▁▂▁▁</td></tr><tr><td>[fold4] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold4] score</td><td>▁▄▅██</td></tr><tr><td>[fold5] avg_train_loss</td><td>█▃▂▂▁</td></tr><tr><td>[fold5] avg_val_loss</td><td>▁▄▂▇█</td></tr><tr><td>[fold5] epoch</td><td>▁▃▅▆█</td></tr><tr><td>[fold5] loss</td><td>▆▄▁▂▆▆▆▂▂▁▂▄▁▃▁▁▂▁▁▁▁▂█▁▂▁▆▁▇▄▁▁▁▁▂▃▁▂▅▁</td></tr><tr><td>[fold5] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold5] score</td><td>▁▃▇▆█</td></tr><tr><td>[fold6] avg_train_loss</td><td>█▂▂▁▁</td></tr><tr><td>[fold6] avg_val_loss</td><td>▁▁▃▇█</td></tr><tr><td>[fold6] epoch</td><td>▁▃▅▆█</td></tr><tr><td>[fold6] loss</td><td>█▂▄▂▂▂▂▅▇▁▂▃▁▂▂▂▂▄▁▆▂▂▃▄▄▂▂▂▂▂▅▃▁▁▂▂▁▁▁▂</td></tr><tr><td>[fold6] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold6] score</td><td>▁▆█▆█</td></tr><tr><td>[fold7] avg_train_loss</td><td>█▃▂▂▁</td></tr><tr><td>[fold7] avg_val_loss</td><td>▁▃▄▆█</td></tr><tr><td>[fold7] epoch</td><td>▁▃▅▆█</td></tr><tr><td>[fold7] loss</td><td>▁▂▂█▂▁▃▁▂▁▁▁▂▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>[fold7] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold7] score</td><td>▁█▅██</td></tr><tr><td>[fold8] avg_train_loss</td><td>█▃▂▂▁</td></tr><tr><td>[fold8] avg_val_loss</td><td>▁▂▂▇█</td></tr><tr><td>[fold8] epoch</td><td>▁▃▅▆█</td></tr><tr><td>[fold8] loss</td><td>▄▅▄▁▁▁▂▁▁▁▂▂▁▆▂▁▁█▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁▄▂▁▃▂▁</td></tr><tr><td>[fold8] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold8] score</td><td>▁█▆▇▇</td></tr><tr><td>[fold9] avg_train_loss</td><td>█▃▂▂▁</td></tr><tr><td>[fold9] avg_val_loss</td><td>▁▃▄▇█</td></tr><tr><td>[fold9] epoch</td><td>▁▃▅▆█</td></tr><tr><td>[fold9] loss</td><td>▃▇▂▂▆▂▄▂▃▂▁█▄▂▁▁▃▁▂▁▁▁▁▄▂▁▂▁▁▁▂▂▂▁▂▁▁▁▁▁</td></tr><tr><td>[fold9] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold9] score</td><td>▁▄▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>[fold0] avg_train_loss</td><td>0.00575</td></tr><tr><td>[fold0] avg_val_loss</td><td>0.01482</td></tr><tr><td>[fold0] epoch</td><td>5</td></tr><tr><td>[fold0] loss</td><td>0.00768</td></tr><tr><td>[fold0] lr</td><td>0.0</td></tr><tr><td>[fold0] score</td><td>0.88806</td></tr><tr><td>[fold10] avg_train_loss</td><td>0.00486</td></tr><tr><td>[fold10] avg_val_loss</td><td>0.01791</td></tr><tr><td>[fold10] epoch</td><td>5</td></tr><tr><td>[fold10] loss</td><td>0.00309</td></tr><tr><td>[fold10] lr</td><td>0.0</td></tr><tr><td>[fold10] score</td><td>0.87854</td></tr><tr><td>[fold11] avg_train_loss</td><td>0.00565</td></tr><tr><td>[fold11] avg_val_loss</td><td>0.01743</td></tr><tr><td>[fold11] epoch</td><td>5</td></tr><tr><td>[fold11] loss</td><td>2e-05</td></tr><tr><td>[fold11] lr</td><td>0.0</td></tr><tr><td>[fold11] score</td><td>0.884</td></tr><tr><td>[fold12] avg_train_loss</td><td>0.00551</td></tr><tr><td>[fold12] avg_val_loss</td><td>0.01413</td></tr><tr><td>[fold12] epoch</td><td>5</td></tr><tr><td>[fold12] loss</td><td>2e-05</td></tr><tr><td>[fold12] lr</td><td>0.0</td></tr><tr><td>[fold12] score</td><td>0.89849</td></tr><tr><td>[fold13] avg_train_loss</td><td>0.00558</td></tr><tr><td>[fold13] avg_val_loss</td><td>0.01629</td></tr><tr><td>[fold13] epoch</td><td>5</td></tr><tr><td>[fold13] loss</td><td>0.00036</td></tr><tr><td>[fold13] lr</td><td>0.0</td></tr><tr><td>[fold13] score</td><td>0.88186</td></tr><tr><td>[fold14] avg_train_loss</td><td>0.00512</td></tr><tr><td>[fold14] avg_val_loss</td><td>0.01545</td></tr><tr><td>[fold14] epoch</td><td>5</td></tr><tr><td>[fold14] loss</td><td>0.0107</td></tr><tr><td>[fold14] lr</td><td>0.0</td></tr><tr><td>[fold14] score</td><td>0.88499</td></tr><tr><td>[fold1] avg_train_loss</td><td>0.00497</td></tr><tr><td>[fold1] avg_val_loss</td><td>0.01661</td></tr><tr><td>[fold1] epoch</td><td>5</td></tr><tr><td>[fold1] loss</td><td>9e-05</td></tr><tr><td>[fold1] lr</td><td>0.0</td></tr><tr><td>[fold1] score</td><td>0.87714</td></tr><tr><td>[fold2] avg_train_loss</td><td>0.00538</td></tr><tr><td>[fold2] avg_val_loss</td><td>0.01488</td></tr><tr><td>[fold2] epoch</td><td>5</td></tr><tr><td>[fold2] loss</td><td>0.00395</td></tr><tr><td>[fold2] lr</td><td>0.0</td></tr><tr><td>[fold2] score</td><td>0.88458</td></tr><tr><td>[fold3] avg_train_loss</td><td>0.00523</td></tr><tr><td>[fold3] avg_val_loss</td><td>0.01765</td></tr><tr><td>[fold3] epoch</td><td>5</td></tr><tr><td>[fold3] loss</td><td>0.00291</td></tr><tr><td>[fold3] lr</td><td>0.0</td></tr><tr><td>[fold3] score</td><td>0.87259</td></tr><tr><td>[fold4] avg_train_loss</td><td>0.00529</td></tr><tr><td>[fold4] avg_val_loss</td><td>0.01427</td></tr><tr><td>[fold4] epoch</td><td>5</td></tr><tr><td>[fold4] loss</td><td>0.00672</td></tr><tr><td>[fold4] lr</td><td>0.0</td></tr><tr><td>[fold4] score</td><td>0.88955</td></tr><tr><td>[fold5] avg_train_loss</td><td>0.00491</td></tr><tr><td>[fold5] avg_val_loss</td><td>0.01475</td></tr><tr><td>[fold5] epoch</td><td>5</td></tr><tr><td>[fold5] loss</td><td>0.01198</td></tr><tr><td>[fold5] lr</td><td>0.0</td></tr><tr><td>[fold5] score</td><td>0.89691</td></tr><tr><td>[fold6] avg_train_loss</td><td>0.00563</td></tr><tr><td>[fold6] avg_val_loss</td><td>0.01657</td></tr><tr><td>[fold6] epoch</td><td>5</td></tr><tr><td>[fold6] loss</td><td>2e-05</td></tr><tr><td>[fold6] lr</td><td>0.0</td></tr><tr><td>[fold6] score</td><td>0.88214</td></tr><tr><td>[fold7] avg_train_loss</td><td>0.00507</td></tr><tr><td>[fold7] avg_val_loss</td><td>0.01902</td></tr><tr><td>[fold7] epoch</td><td>5</td></tr><tr><td>[fold7] loss</td><td>0.03428</td></tr><tr><td>[fold7] lr</td><td>0.0</td></tr><tr><td>[fold7] score</td><td>0.86656</td></tr><tr><td>[fold8] avg_train_loss</td><td>0.00515</td></tr><tr><td>[fold8] avg_val_loss</td><td>0.0166</td></tr><tr><td>[fold8] epoch</td><td>5</td></tr><tr><td>[fold8] loss</td><td>0.00028</td></tr><tr><td>[fold8] lr</td><td>0.0</td></tr><tr><td>[fold8] score</td><td>0.88394</td></tr><tr><td>[fold9] avg_train_loss</td><td>0.00541</td></tr><tr><td>[fold9] avg_val_loss</td><td>0.01733</td></tr><tr><td>[fold9] epoch</td><td>5</td></tr><tr><td>[fold9] loss</td><td>0.00163</td></tr><tr><td>[fold9] lr</td><td>0.0</td></tr><tr><td>[fold9] score</td><td>0.87918</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">microsoft/deberta-v3-large</strong>: <a href=\"https://wandb.ai/bluehills/NBME-Public/runs/2i25zy57\" target=\"_blank\">https://wandb.ai/bluehills/NBME-Public/runs/2i25zy57</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20220501_060408-2i25zy57/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["if CFG.wandb:\n","    wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kkDcAs2CRZUz"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"NBME training.ipynb","provenance":[],"authorship_tag":"ABX9TyP95Aylrk33LjKmcDdYt9Rg"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"006aaa62f7394e4aa4c92d1ff3880b6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ba422c37787c4677a6a0c39f8c0a8e2d","IPY_MODEL_3193635daa9b4193a18f99934a109dc7","IPY_MODEL_f4537c3202f74bf38a6b6aaf55628a7e"],"layout":"IPY_MODEL_84d6f88223c548faac308f07f25eac93"}},"1ede3793442c4ee5bb835a4343a034c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_71c881072b844b11aa657a879d7e3333","IPY_MODEL_7787e9b4402a4d7ca2d8618aba9a154b","IPY_MODEL_f015d1f3131347959de9c487c3a87b71"],"layout":"IPY_MODEL_a1c50dc370084739b8d372a2ddcf5aab"}},"20c0cd7e5aa84ff586106d298e4a9973":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2466cef331c7466a8fd807296ba671c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f72841178af424e9ef577d29ccdc4fe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3193635daa9b4193a18f99934a109dc7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca09bdced93c444185d08496d55d27d9","max":42146,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bc11d959533e4a559a1fc5c62a8777d2","value":42146}},"4be9ceaa5df443d190178ec17f39ce3c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c6246008ea8487bb3025f44926318e5","max":143,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c129096d2da247ed8648f342a80dbdec","value":143}},"4f3234cedea545c2822d29d91e82da85":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5e95736ad6f34a3d95cdac9a8def775c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f3243d2ec9b40879c10942910dc9b05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d04cb97ac6d49e4b5cb2cd1e0f38329","placeholder":"​","style":"IPY_MODEL_f8701c82716d4839857648540928538c","value":"Downloading: 100%"}},"607b464e4de24b1aa1888bc31ad9d65e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5f3243d2ec9b40879c10942910dc9b05","IPY_MODEL_fbfc71adab044ac6968225f7d1ddb76b","IPY_MODEL_8c2bf321200c4b49ad6c9700682d5db4"],"layout":"IPY_MODEL_d42022d94be04cebb80fe80b96db6383"}},"638984858f8e4af3b6b0c3eedc16f706":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d04cb97ac6d49e4b5cb2cd1e0f38329":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71c881072b844b11aa657a879d7e3333":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e95736ad6f34a3d95cdac9a8def775c","placeholder":"​","style":"IPY_MODEL_2466cef331c7466a8fd807296ba671c4","value":"Downloading: 100%"}},"7787e9b4402a4d7ca2d8618aba9a154b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_638984858f8e4af3b6b0c3eedc16f706","max":873673253,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a20d17e491014c2587b902ee2f4adc55","value":873673253}},"8121a961c716442f9cae45e0032a827a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de433da4e9dd4ceab10d738560bb0388","placeholder":"​","style":"IPY_MODEL_cc014a6e03ee479d8dbd838b6288f03f","value":"100%"}},"84d6f88223c548faac308f07f25eac93":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"867259f20ad8430e9399362566c029e0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"870dc3300e73417eab6f04a7f4d7998b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c2bf321200c4b49ad6c9700682d5db4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c66956ab94ec4a98bda53fdb4c47cb41","placeholder":"​","style":"IPY_MODEL_ec3837bbf7c24500ae42321fa01650b1","value":" 580/580 [00:00&lt;00:00, 25.5kB/s]"}},"9c4901a451f346c8b61e5ead5e95ded9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c6246008ea8487bb3025f44926318e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d3d377dd68546409792f1b62ef0a915":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8121a961c716442f9cae45e0032a827a","IPY_MODEL_4be9ceaa5df443d190178ec17f39ce3c","IPY_MODEL_f404c24e5b5749b2b43febeec330645a"],"layout":"IPY_MODEL_2f72841178af424e9ef577d29ccdc4fe"}},"a1c50dc370084739b8d372a2ddcf5aab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a20d17e491014c2587b902ee2f4adc55":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a5f4b80673bd4e3ab2eb1a8a822a7286":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"afc78b53ad7d474b89f99df14970eda0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba422c37787c4677a6a0c39f8c0a8e2d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_870dc3300e73417eab6f04a7f4d7998b","placeholder":"​","style":"IPY_MODEL_20c0cd7e5aa84ff586106d298e4a9973","value":"100%"}},"bc11d959533e4a559a1fc5c62a8777d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"be0937b1a0b1422295a7774b3a8baec7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c129096d2da247ed8648f342a80dbdec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c66956ab94ec4a98bda53fdb4c47cb41":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca09bdced93c444185d08496d55d27d9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb1063c1ca2e47278516010d79d421d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc014a6e03ee479d8dbd838b6288f03f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1292607f5ef4284bc751aa52bf002b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d42022d94be04cebb80fe80b96db6383":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de433da4e9dd4ceab10d738560bb0388":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec3837bbf7c24500ae42321fa01650b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f015d1f3131347959de9c487c3a87b71":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_afc78b53ad7d474b89f99df14970eda0","placeholder":"​","style":"IPY_MODEL_9c4901a451f346c8b61e5ead5e95ded9","value":" 833M/833M [00:13&lt;00:00, 56.5MB/s]"}},"f404c24e5b5749b2b43febeec330645a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be0937b1a0b1422295a7774b3a8baec7","placeholder":"​","style":"IPY_MODEL_d1292607f5ef4284bc751aa52bf002b7","value":" 143/143 [00:00&lt;00:00, 3604.74it/s]"}},"f4537c3202f74bf38a6b6aaf55628a7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb1063c1ca2e47278516010d79d421d3","placeholder":"​","style":"IPY_MODEL_a5f4b80673bd4e3ab2eb1a8a822a7286","value":" 42146/42146 [00:21&lt;00:00, 2023.95it/s]"}},"f8701c82716d4839857648540928538c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fbfc71adab044ac6968225f7d1ddb76b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_867259f20ad8430e9399362566c029e0","max":580,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4f3234cedea545c2822d29d91e82da85","value":580}}}}},"nbformat":4,"nbformat_minor":0}