{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_article(file_name):\n",
    "    with codecs.open(file_name, \"r\", encoding='utf-8') as file:  # handles accentuated characters\n",
    "        filedata = file.readlines()\n",
    "    \n",
    "    article = \" \".join(filedata).split(\". \")    # split the text by sentences using \". \"\n",
    "    \n",
    "    sentences = []\n",
    "    for sentence in article:             # iterate thru sentences, printing each and generate list of wards for each sentence\n",
    "        # print(sentence)\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))    # replace any non character by \" \"\n",
    "    #sentences.pop()   ##### systematically eliminate last sentence of the text from the returned sentences??\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sentence_1, sentence_2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []     # create an empty list to avoid error below\n",
    " \n",
    "    sentence_1 = [w.lower() for w in sentence_1]\n",
    "    sentence_2 = [w.lower() for w in sentence_2]\n",
    "\n",
    "    all_words = list(set(sentence_1 + sentence_2))  # create total vocabulary of unique words for the two sentences compared\n",
    "\n",
    "    vector1 = [0] * len(all_words)                  # prepare one-hot vectors for each sentence over all vocab\n",
    "    vector2 = [0] * len(all_words)\n",
    "\n",
    "    # build the vector for the first sentence\n",
    "    for w in sentence_1:\n",
    "        if w in stopwords:\n",
    "            continue \n",
    "        vector1[all_words.index(w)] += 1           # list.index(element) returns the index of the given element in the list\n",
    "\n",
    "    # build the vector for the second sentence\n",
    "    for w in sentence_2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    "\n",
    "    return 1 - cosine_distance(vector1, vector2)   # Cosine = 0 for similar sentences => returns 1 if perfectly similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))  # create a square matrix with dim the num of sentences\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences (diagonal of the square matrix)\n",
    "                continue\n",
    "            # similarity of each sentence to all other sentences in the text is measured and logged in the matrix\n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(file_name, top_n=5, show=False):\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "    \n",
    "    # Step 1 - Read text and tokenize\n",
    "    sentences =  read_article(file_name)\n",
    "    print(\"number of sentences in text : \", len(sentences))\n",
    "    \n",
    "    # Step 2 - Generate Similary Matrix across sentences\n",
    "    sentence_similarity_matrix = build_similarity_matrix(sentences, stop_words)\n",
    "    \n",
    "    # Step 3 - Rank sentences in similarity matrix. let’s convert the similarity matrix into a graph. \n",
    "    # The nodes of this graph will represent the sentences and the edges will represent the similarity scores between\n",
    "    # the sentences. On this graph, we will apply the PageRank algorithm to arrive at the sentence rankings.\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "    \n",
    "    # Step 4 - Sort the rank and pick top sentences extract the top N sentences based on their rankings for summary generation\n",
    "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "    if show :\n",
    "        print(\"Indexes of top ranked_sentence order are \", ranked_sentence)\n",
    "    # extract the top N sentences based on their rankings for summary generation\n",
    "    if len(ranked_sentence) < top_n:\n",
    "        top_n = len(ranked_sentence)\n",
    "    for i in range(top_n):\n",
    "        summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "    \n",
    "    # Step 5 - Output the summarize text\n",
    "    print(\"Summarize Text: \\n\", \". \".join(summarize_text)+'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.6359, 0.5088],\n",
       "        [0.6359, 1.0000, 0.6196],\n",
       "        [0.5088, 0.6196, 1.0000]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L12-v2\")\n",
    "\n",
    "embeddings = model.encode([\n",
    "    'I like the trains',\n",
    "    'trains are better than cars',\n",
    "    'airplaines are better than trains'\n",
    "])\n",
    "\n",
    "util.dot_score(embeddings, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp\\D2GIVW5LLF\n",
      "temp\\D2GIVW5LLF temp\\D2GIVW5LLF\\system temp\\D2GIVW5LLF\\model\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Zumo\\\\AppData\\\\Roaming\\\\pyrouge\\\\settings.ini'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\Users\\Zumo\\Documents\\Github\\Feedback-Prize\\summarization.ipynb Cella 10\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Users/Zumo/Documents/Github/Feedback-Prize/summarization.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m exp21 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoday it was sunny and hot. The skay was blue. All was right\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Users/Zumo/Documents/Github/Feedback-Prize/summarization.ipynb#X31sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m exp22 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoday it was sunny and hot. All was right\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Users/Zumo/Documents/Github/Feedback-Prize/summarization.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m evaluate_rouge(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Users/Zumo/Documents/Github/Feedback-Prize/summarization.ipynb#X31sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     [sum1\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m. \u001b[39;49m\u001b[39m\"\u001b[39;49m), sum2\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m. \u001b[39;49m\u001b[39m\"\u001b[39;49m)],\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Users/Zumo/Documents/Github/Feedback-Prize/summarization.ipynb#X31sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     [[exp11\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m. \u001b[39;49m\u001b[39m\"\u001b[39;49m), exp12\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m. \u001b[39;49m\u001b[39m\"\u001b[39;49m)], [exp21\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m. \u001b[39;49m\u001b[39m\"\u001b[39;49m), exp22\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m. \u001b[39;49m\u001b[39m\"\u001b[39;49m)]],\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Users/Zumo/Documents/Github/Feedback-Prize/summarization.ipynb#X31sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n",
      "File \u001b[1;32mf:\\Users\\Zumo\\Documents\\Github\\Feedback-Prize\\summary\\rouge.py:37\u001b[0m, in \u001b[0;36mevaluate_rouge\u001b[1;34m(summaries, references, remove_temp, rouge_args)\u001b[0m\n\u001b[0;32m     34\u001b[0m         f\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(summary))\n\u001b[0;32m     36\u001b[0m args_str \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mmap\u001b[39m(\u001b[39mstr\u001b[39m, rouge_args))\n\u001b[1;32m---> 37\u001b[0m rouge \u001b[39m=\u001b[39m Rouge155(rouge_args\u001b[39m=\u001b[39;49margs_str)\n\u001b[0;32m     38\u001b[0m rouge\u001b[39m.\u001b[39msystem_dir \u001b[39m=\u001b[39m system_dir  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m     39\u001b[0m rouge\u001b[39m.\u001b[39mmodel_dir \u001b[39m=\u001b[39m model_dir  \u001b[39m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Zumo\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\pyrouge\\Rouge155.py:88\u001b[0m, in \u001b[0;36mRouge155.__init__\u001b[1;34m(self, rouge_dir, rouge_args)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config_file \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_settings_file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_config_path()\n\u001b[1;32m---> 88\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__set_rouge_dir(rouge_dir)\n\u001b[0;32m     89\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__clean_rouge_args(rouge_args)\n\u001b[0;32m     90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_system_filename_pattern \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Zumo\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\pyrouge\\Rouge155.py:402\u001b[0m, in \u001b[0;36mRouge155.__set_rouge_dir\u001b[1;34m(self, home_dir)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[39mVerfify presence of ROUGE-1.5.5.pl and data folder, and set\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[39mthose paths.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \n\u001b[0;32m    400\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m home_dir:\n\u001b[1;32m--> 402\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_home_dir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_rouge_home_dir_from_settings()\n\u001b[0;32m    403\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_home_dir \u001b[39m=\u001b[39m home_dir\n",
      "File \u001b[1;32mc:\\Users\\Zumo\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\pyrouge\\Rouge155.py:416\u001b[0m, in \u001b[0;36mRouge155.__get_rouge_home_dir_from_settings\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__get_rouge_home_dir_from_settings\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    415\u001b[0m     config \u001b[39m=\u001b[39m ConfigParser()\n\u001b[1;32m--> 416\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings_file) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    417\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39mread_file\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    418\u001b[0m             config\u001b[39m.\u001b[39mread_file(f)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Zumo\\\\AppData\\\\Roaming\\\\pyrouge\\\\settings.ini'"
     ]
    }
   ],
   "source": [
    "from summary.rouge import evaluate_rouge\n",
    "\n",
    "sum1 = \"today it was sunny and hot. The skay was blue. All was right\"\n",
    "exp11 = \"today it was sunny and hot. The skay was blue. All was right\"\n",
    "exp12 = \"today it was sunny and hot. All was right\"\n",
    "\n",
    "sum2 = \"today it was cold and snowy. The skay was blue. All was right\"\n",
    "exp21 = \"today it was sunny and hot. The skay was blue. All was right\"\n",
    "exp22 = \"today it was sunny and hot. All was right\"\n",
    "\n",
    "evaluate_rouge(\n",
    "    [sum1.split(\". \"), sum2.split(\". \")],\n",
    "    [[exp11.split(\". \"), exp12.split(\". \")], [exp21.split(\". \"), exp22.split(\". \")]],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L12-v2\")\n",
    "\n",
    "def read_article_bert(file_name):\n",
    "    with codecs.open(file_name, \"r\", encoding='utf-8') as file:\n",
    "        filedata = file.readlines()\n",
    "    \n",
    "    article = \" \".join(filedata).split(\". \")\n",
    "    return [sentence.replace(\"[^a-zA-Z]\", \" \") for sentence in article]\n",
    "\n",
    "def generate_summary_bert(file_name, top_n=5, show=False):\n",
    "    summarize_text = []\n",
    "    \n",
    "    # Step 1 - Read text and tokenize\n",
    "    sentences =  read_article_bert(file_name)\n",
    "    print(\"number of sentences in text : \", len(sentences))\n",
    "    \n",
    "    # Step 2 - Generate Similary Matrix across sentences\n",
    "    encodings = model.encode(sentences, convert_to_tensor=True)\n",
    "    sentence_similarity_matrix = util.dot_score(encodings, encodings) # type: ignore\n",
    "    sentence_similarity_matrix = np.asarray(sentence_similarity_matrix)\n",
    "    \n",
    "    # Step 3 - Rank sentences in similarity matrix. let’s convert the similarity matrix into a graph. \n",
    "    # The nodes of this graph will represent the sentences and the edges will represent the similarity scores between\n",
    "    # the sentences. On this graph, we will apply the PageRank algorithm to arrive at the sentence rankings.\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "    \n",
    "    # Step 4 - Sort the rank and pick top sentences extract the top N sentences based on their rankings for summary generation\n",
    "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "    if show :\n",
    "        print(\"Indexes of top ranked_sentence order are \", ranked_sentence)\n",
    "    # extract the top N sentences based on their rankings for summary generation\n",
    "    if len(ranked_sentence) < top_n:\n",
    "        top_n = len(ranked_sentence)\n",
    "        \n",
    "        \n",
    "    summarize_text = [ranked_sentence[i][1] for i in range(top_n)]\n",
    "\n",
    "    \n",
    "    # Step 5 - Output the summarize text\n",
    "    print(\"Summarize Text: \\n\", \". \".join(summarize_text)+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences in text :  11\n",
      "Summarize Text: \n",
      " There is no good reason that NASA would hide life on Mars from the rest of the world.\n",
      " \n",
      " So, NASA is not hiding life on Mars from us, and they are not trying to trick us into thinking that the \"face\" on mars is just a mesa, because it actually is. Some people belive that the so called \"face\" on mars was created by life on mars. NASA hiding life would be illogical, because if they found life on Mars, they would make a lot of money, and we all know that the people at NASA aren't illogical people..\n"
     ]
    }
   ],
   "source": [
    "# let's begin\n",
    "generate_summary(\"input/feedback-prize-2021/train/0000D23A521A.txt\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences in text :  11\n",
      "Summarize Text: \n",
      " There is no good reason that NASA would hide life on Mars from the rest of the world.\n",
      " \n",
      " So, NASA is not hiding life on Mars from us, and they are not trying to trick us into thinking that the \"face\" on mars is just a mesa, because it actually is. Some people belive that the so called \"face\" on mars was created by life on mars. This \"face\" on mars only looks like a face because humans tend to see faces wherever we look, humans are obviously extremely social, which is why our brain is designed to recognize faces.\n",
      " \n",
      " Many conspiracy theorists believe that NASA is hiding life on Mars from the rest of the world.\n"
     ]
    }
   ],
   "source": [
    "# let's begin\n",
    "generate_summary_bert(\"input/feedback-prize-2021/train/0000D23A521A.txt\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences in text :  11\n"
     ]
    }
   ],
   "source": [
    "# Step 1 - Read text and tokenize\n",
    "sentences =  read_article_bert(\"input/feedback-prize-2021/train/0000D23A521A.txt\")\n",
    "print(\"number of sentences in text : \", len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 11)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2 - Generate Similary Matrix across sentences\n",
    "encodings = model.encode(sentences, convert_to_tensor=True)\n",
    "sentence_similarity_matrix = util.dot_score(encodings, encodings) # type: ignore\n",
    "sentence_similarity_matrix = np.asarray(sentence_similarity_matrix)\n",
    "\n",
    "sentence_similarity_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Rank sentences in similarity matrix. let’s convert the similarity matrix into a graph. \n",
    "# The nodes of this graph will represent the sentences and the edges will represent the similarity scores between\n",
    "# the sentences. On this graph, we will apply the PageRank algorithm to arrive at the sentence rankings.\n",
    "sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "scores = nx.pagerank(sentence_similarity_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 3\n",
    "\n",
    "# Step 4 - Sort the rank and pick top sentences extract the top N sentences based on their rankings for summary generation\n",
    "ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "# print(\"Indexes of top ranked_sentence order are \", ranked_sentence)\n",
    "# extract the top N sentences based on their rankings for summary generation\n",
    "if len(ranked_sentence) < top_n:\n",
    "    top_n = len(ranked_sentence)\n",
    "\n",
    "summarize_text = [ranked_sentence[i][1] for i in range(top_n)]\n",
    "\n",
    "# Step 5 - Output the summarize text\n",
    "print(\"Summarize Text: \\n\", \". \".join(summarize_text)+'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PacSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summary.pacsum import PacSumExtractorWithBert\n",
    "from summary.rouge import evaluate_rouge\n",
    "\n",
    "def extract_summary(extractor: PacSumExtractorWithBert, data_iterator):\n",
    "\n",
    "    summaries = []\n",
    "    references = []\n",
    "\n",
    "    for item in data_iterator:\n",
    "        article, abstract, inputs = item\n",
    "        \n",
    "        if len(article) <= self.extract_num:\n",
    "            summaries.append(article)\n",
    "            references.append([abstract])\n",
    "            continue\n",
    "        \n",
    "        summary = extractor.extract_summary(article)\n",
    "        summaries.append(summary)\n",
    "        references.append([abstract])\n",
    "\n",
    "    result = evaluate_rouge(summaries, references, remove_temp=True, rouge_args=[])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c004a322e9c82f28ba2e77aacdbb1b6ccb0b2b4ae6a31db23bc8a8c53511ac4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
